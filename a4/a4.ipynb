{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "a4.ipynb",
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leinaxd/Tps/blob/master/a4/a4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "x__t1uIZd139"
      },
      "source": [
        "# GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAfnePXhF3J2",
        "cellView": "form"
      },
      "source": [
        "#@markdown SSH\n",
        "\n",
        "#@markdown Ingrese directorio con contraseña privada\n",
        "directorio = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!ssh-keygen -h dsd\n",
        "# !ssh-keygen -t ed25519 -C \"leinaxd@gmail.com\"\n",
        "# %cd /root/.ssh/\n",
        "# !ls -la\n",
        "# !cat id_ed25519.pub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PehpfW10d14A",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8bc7f7-4a3d-43ca-8794-9d95ddbda615"
      },
      "source": [
        "#@markdown Cargar\n",
        "%cd ~\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/leinaxd/Tps/\n",
        "\n",
        "%cd Tps/a4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/content\n",
            "Cloning into 'Tps'...\n",
            "remote: Enumerating objects: 662, done.\u001b[K\n",
            "remote: Counting objects: 100% (662/662), done.\u001b[K\n",
            "remote: Compressing objects: 100% (489/489), done.\u001b[K\n",
            "remote: Total 662 (delta 366), reused 403 (delta 160), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (662/662), 77.47 MiB | 25.28 MiB/s, done.\n",
            "Resolving deltas: 100% (366/366), done.\n",
            "Checking out files: 100% (216/216), done.\n",
            "/content/Tps/a4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqNCm1kNt547",
        "cellView": "form"
      },
      "source": [
        "AUTH = \"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\" #@param {type:\"string\"}\n",
        "!git config --global user.email \"leinaxd@gmail.com\"\n",
        "!git config --global user.name \"leinaxd\"\n",
        "!git remote set-url origin https://$AUTH@github.com/leinaxd/Tps.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "E2DN-kWZd14C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "d72fe9b5-3d9b-479f-f3b7-4ad152eb50e2"
      },
      "source": [
        "commit = \"a4 1f: nmt step\" #@param {type:\"string\"}\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"$commit\"\n",
        "!git push\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is ahead of 'origin/master' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Counting objects: 9, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 12.71 KiB | 6.36 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/leinaxd/Tps.git\n",
            "   a77d23b..9c1074f  master -> master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "cellView": "form",
        "id": "Mc8E2_i6d14D"
      },
      "source": [
        "#@markdown pull / update\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Gn5O6OqEhB"
      },
      "source": [
        "# Instalar librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKAYyMXjqHOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b44d997-1167-4f91-8458-30ce6a1eafa0"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install sacrebleu"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.4 portalocker-2.3.2 sacrebleu-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Z3vHnNuLd14E"
      },
      "source": [
        "# CS224N Assignment 4: Neural Machine Translation with RNNs (45 + 30 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gNPr6wj8d14E"
      },
      "source": [
        "Esta tarea se divide en dos partes: *Neural Machine Translation with RNNs* y *Analyzing NMT systems*. El primero es básicamente codificación y el segundo son preguntas escritas. Si te atoras en la primera sección, puedes trabajar en la segunda ya que las secciones son independientes.\n",
        "La notación e implementación de los *sistemas NMT* es tramposa. Recomendamos la lectura de Zhang, 2020. https:arxic.org/abs/2010.04791 para entender mejor la tarea de traducción *Cherokee-Inglés* que sirvió de inspiración para este proyecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEpOpuMDfBcR"
      },
      "source": [
        "## 1. Neural Machine Translation with RNNs (45 points)\n",
        "\n",
        "En *Machine Translation* el objetivo es convertir una oración desde un lenguaje *source*(Cherokee) hacia un lenguaje *target* (Inglés).<br>\n",
        "En este proyecto implementaremos un modelo neuronal **seq2seq** con **Atención**. En esta sección, describiremos el **procedimiento de entrenamiento** para el sistema **NMT** propuesto, el cual utiliza un **Encoder** **LSTM** **Bidireccional** y un **Decoder** **LSTM** **Unidireccional**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ajj9oU_goY0"
      },
      "source": [
        "### Descripción del modelo (Procedimiento de aprendizaje)\n",
        "![modelo](https://github.com/leinaxd/Tps/raw/master/a4/imgs/arquitecturaNMT.jpg)\n",
        "Figura 1. Modelo seq2seq con Atención multiplicativa, mostrada en la tercera fase del decoder. Los estados ocultos $h_i^{enc}$ y los estados de las celdas $c_i^{enc}$ se definen a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "IKnv473nXIOC"
      },
      "source": [
        "Dada una oración en un lenguaje de origen, buscaremos los embeddings a nivel *subword* a partir de la matriz de *embeddings* obteniendo $x_1...x_m \\hspace{10mm} (x_i\\in \\mathbb{R}^{e\\times 1})$, donde $m$ es la longitud de la oración de entrada y $e$ la dimensión del embedding.\n",
        "Alimentamos estos embeddings al encoder bidireccional, obteniendo estados ocultos y estados de celda tanto para el *forwards* como *backwards* del **LSTM**. Las versiones *Forwards* y *Backwards* se concatenan para obtener el estado oculto $\\mathbf{h_i^{enc}}$ y estado de celda $\\mathbf{c_i^{enc}}$:\n",
        "\n",
        "$\\begin{align}\n",
        "\\mathbf{h_i^{enc}} &= [\\overleftarrow{\\mathbf{h_i^{enc}}}; \\overrightarrow{\\mathbf{h_i^{enc}}}] \\hspace{10mm} &&con \\hspace{5mm} \\mathbf{h_i^{enc}}\\in \\mathbb{R}^{2h\\times 1},\\hspace{5mm} \\overleftarrow{\\mathbf{h_i^{enc}}},\\overrightarrow{\\mathbf{h_i^{enc}}}\\in \\mathbb{R}^{h\\times 1} \\hspace{5mm} 1\\le i\\le m\\\\\n",
        "\\mathbf{c_i^{enc}} &= [\\overleftarrow{\\mathbf{c_i^{enc}}}; \\overrightarrow{\\mathbf{c_i^{enc}}}] \\hspace{10mm} &&con \\hspace{5mm} \\mathbf{c_i^{enc}}\\in \\mathbb{R}^{2h\\times 1},\\hspace{5mm} \\overleftarrow{\\mathbf{c_i^{enc}}},\\overrightarrow{\\mathbf{c_i^{enc}}}\\in \\mathbb{R}^{h\\times 1} \\hspace{5mm} 1\\le i\\le m\\\\\n",
        "\\end{align} $\n",
        "\n",
        "Luego inicializaremos el primer estado oculto del **Decoder** $\\mathbf{h_0^{dec}}$ y el estado de celda $\\mathbf{c_0^{dec}}$ con una proyección lineal del estado oculto y celda final del **encoder**.<br>\n",
        "(Si no es obvio, piense por qué consideramos $[\\overleftarrow{\\mathbf{h_1^{enc}}},\\overrightarrow{\\mathbf{h_m^{enc}}}]$ el estado final del encoder.)\n",
        "\n",
        "$\\begin{align}\n",
        "\\mathbf{h_0^{dec}} &= \\mathbf{W_h} [\\overleftarrow{\\mathbf{h_1^{enc}}}; \\overrightarrow{\\mathbf{h_m^{enc}}}] \\hspace{10mm} &&con \\hspace{5mm} \\mathbf{h_0^{dec}}\\in \\mathbb{R}^{h\\times 1},\\hspace{5mm} \\mathbf{W_h}\\in \\mathbb{R}^{h\\times 2h}\\\\\n",
        "\\mathbf{c_0^{dec}} &= \\mathbf{W_c} [\\overleftarrow{\\mathbf{c_1^{enc}}}; \\overrightarrow{\\mathbf{c_m^{enc}}}] \\hspace{10mm} &&con \\hspace{5mm} \\mathbf{c_0^{dec}}\\in \\mathbb{R}^{h\\times 1},\\hspace{5mm} \\mathbf{W_c}\\in \\mathbb{R}^{h\\times 2h} \\\\\n",
        "\\end{align} $\n",
        "\n",
        "Con el **Decoder** inicializado, debemos alimentarlo con la oración *target* (destino). En el $t^{mo}$ paso, buscamos el embedding para la $t^{mo}$ *subword* $\\mathbf{y_t}\\in \\mathbb{R}^{e\\times 1}$. Luego concatenamos $\\mathbf{y_t}$ con un vector de salida combinado $o_{t-1}\\in \\mathbb{R}^{h\\times 1}$ del instante anterior (Explicado más adelante), para producir $\\bar{\\mathbf{y_t}}\\in \\mathbb{R}^{(e+h)\\times 1}$. Notar que para el primer *subword* destino (*target*) (El token de inicio) $\\mathbf{o_0}$ es un vector de ceros. Luego alimentamos $\\bar{\\mathbf{y_t}}$ como entrada al decoder.\n",
        "\n",
        "$\\mathbf{h_t^{dec}, \\mathbf{c_t^{dec}}} = Decoder(\\bar{\\mathbf{y_t}}, \\mathbf{h_{t-1}^{dec}}, \\mathbf{c_{t-1}^{dec}}) \\hspace{10mm} donde\\hspace{5mm} \\mathbf{h_t^{dec}}\\in \\mathbb{R}^{h\\times 1}, \\mathbf{c_t^{dec}}\\in \\mathbb{R}^{h\\times 1}\n",
        "$\n",
        "\n",
        "Luego utlizaremos $\\mathbf{h_t^{dec}}$ para calcular la atención de forma multiplicativa sobre $\\mathbf{h_1^{enc} \\dots h_m^{enc}}$:\n",
        "\n",
        "$\\begin{align} \\mathbf{e_{t,i}}\n",
        "&=(\\mathbf{h_t^{dec}})^T \\mathbf{W_{attProj}}\\mathbf{h_i^{enc}}\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\mathbf{e_t}\\in \\mathbb{R}^{m\\times 1},\n",
        "\\hspace{5mm} \\mathbf{W_{attProj}}\\in \\mathbb{R}^{h\\times 2h}\n",
        "\\hspace{5mm} 1\\le i \\le m\\\\\n",
        "\\alpha_t&=softmax(\\mathbf{e_t})\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\alpha_t\\in \\mathbb{R}^{m\\times 1}\\\\\n",
        "\\mathbf{a_t}&=\\sum_{i=1}^{m}\\alpha_{t,i}\\  \\mathbf{h_i^{enc}}\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\mathbf{a_t}\\in \\mathbb{R}^{2h\\times 1}\n",
        "\\end{align}$\n",
        "\n",
        "Ahora concatenaremos la salida de atención $\\mathbf{a_t}$ con el estado oculto del decoder $\\mathbf{h_t^{dec}}$ y pasarlo por una capa lineal, $tanh$ y una capa *dropout* para atender la salida combinada $\\mathbf{o_t}$\n",
        "\n",
        "$\\begin{align}\n",
        "\\mathbf{u_t} &= [\\mathbf{a_t};\\mathbf{h_t^{dec}}]\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\mathbf{u_t}\\in \\mathbb{R}^{3h\\times 1}\n",
        "\\hspace{5mm}\\\\\n",
        "\\mathbf{v_t} &=\n",
        "\\mathbf{W_u}\\ \\mathbf{u_t}\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm}\\mathbf{v_t}\\in \\mathbb{R}^{h\\times 1},\n",
        "\\hspace{5mm}\\mathbf{W_u}\\in \\mathbb{R}^{h\\times 3h}\\\\\n",
        "\\mathbf{o_t} &=dropout(tanh(\\mathbf{v_t}))\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\mathbf{o_t}\\in \\mathbb{R}^{h\\times 1}\n",
        "\\end{align}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_Op4uCJCXIOG"
      },
      "source": [
        "Luego generamos la distribución de proababilidad $\\mathbf{P_t}$ sobre los *subwords* destino (*target*) al\n",
        " $t^{mo}$ paso.\n",
        "\n",
        "$\\mathbf{P_t} = softmax(\\mathbf{W_{vocab}\\ o_t})\n",
        "\\hspace{10mm} donde\\hspace{5mm}\n",
        "\\mathbf{P_t}\\in \\mathbb{R}^{\\mathbf{V_t}\\times 1},\n",
        "\\hspace{5mm}\\mathbf{W_{vocab}}\\in \\mathbb{R}^{\\mathbf{V_t}\\times h}$\n",
        "\n",
        "Aquí $\\mathbf{V_t}$ es el tamaño del vocabulario destino (*target*). Finalmente, para entrenar la red podemos calcular la entropía cruzada softmax entre $\\mathbf{P_t}$ y $\\mathbf{g_t}$, donde $\\mathbf{g_t}$ es el vector *one-hot* del *subword* destino al instante $t$\n",
        "\n",
        "$J_t (\\theta ) =CrossEntropy(\\mathbf{P_t},\\mathbf{g_t})$\n",
        "\n",
        "Donde $\\theta$ representa todos los parámetros del modelo y $J_t(\\theta ) es el costo en el instante $t$ del **decoder**. Ahora que hemos descripto el modelo, tratemos de implementarlo para la traducción Cherokee-Inglés!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jioHmN-EXIOI"
      },
      "source": [
        "### Configurando la máquina virtual\n",
        "Siga las instrucciones de https://docs.google.com/document/d/1BQOAjhBxWbywkB4rMFH9iinb6YHSjaWw1TOVIGfyYho para crear una copia de la máquina virtual. Esto debería tomarte unos 45 minutos. Sin embargo necesitarás la GPU para entrenar el modelo, recomendamos fuertemente que primero desarrolles el código localmente y asegurate que funciona antes de intentar entrenarlo en la máquina virtual. El tiempo de la GPU es limitado y costoso. Toma aproximadamente entre 30 minutos y 1 hora entrenar el sistema NMT. Finalmente, asegurate de apagar la máquina virtual cuando no la estés utilizando.\n",
        "\n",
        "Si la subscripción a Azure se queda sin dinero, tu máquina virtual será temporalmente bloqueada e inaccesible. Si esto sucede, por favor complete el siguiente formulario https://docs.google.com/forms/d/e/1FAIpQLSdpd5CgwSulB_wGCKQb_VmLscicTswiTVBxnw40xVzeuS4BKQ/viewform\n",
        "\n",
        "Para ejecutar el código en tu máquina local, ejecuta los siguientes comandos para crear el apropiado entorno\n",
        "`conda env create --file local_env.yml`\n",
        "Notar que este entorno virtual no se necesita en la máquina virtual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "t5nX7OmfXIOI"
      },
      "source": [
        "### Implementación y preguntas escritas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNql31-Bh3Fi"
      },
      "source": [
        "#### a. Implemente la función `pad_sents` en `utils.py` que producirá estas oraciones tabuladas (2 points)\n",
        "Para aplicar las operaciones sobre los tensores, tenemos que asegurarnos que todas las oraciones del mismo *batch* tengan la misma longitud.\n",
        "* Identifique la oración más larga del *batch* y tabule a las demás en dicha longitud.\n",
        "\n",
        "#### b. (3 points) Implemente la función `__init__` de `model_embeddings.py` \n",
        "Que inicializa los *embeddings* del *source* (oración de origen) y *target* (oración destino)<br>\n",
        "\n",
        "#### c. (4 points) Implemente la función `__init__` de `nmt_model.py` para inicializar los embeddings del modelo y las capas (LSTM, projection y dropout) del sistema NMT\n",
        "(Ademas Utiliza la clase `ModelEmbeddings` de `model_embeddings.py`)\n",
        "#### d. (8 points) Implementar la función `encode` de `nmt_model.py`.\n",
        "Esta función \n",
        "* Convierte las oraciones *source* ya tabuladas en un tensor $\\mathbf{X}$\n",
        "* Genera $\\mathbf{h_1^{enc}\\dots h_m^{enc}}$\n",
        "* Calcula el estado inicial $\\mathbf{h_0^{dec}}$ y celda inicial $\\mathbf{c_0^{dec}}$ del **decoder**.\n",
        "\n",
        "Puedes ejecutar una verificación no exhaustiva mediante:\n",
        "`python sanity_check.py 1d`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtrfVLtvh9qo"
      },
      "source": [
        "!python sanity_check.py 1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKbZzJMTwKob"
      },
      "source": [
        "#### e. (8 points) Implemente la función `decode` de `nmt_model.py`.\n",
        "Esta función construye $\\bar{\\mathbf{y_t}}$ y ejecuta la función `step` por cada índice de tiempo de la entrada.\n",
        "Puedes ejecutar un test no exhaustivo mediante: `python sanity_check.py 1e`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34RJOEl5wJ0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40975f31-8f11-45b8-ff19-c70e975876ac"
      },
      "source": [
        "!python sanity_check.py 1e"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "--------------------------------------------------------------------------------\n",
            "Running Sanity Check for Question 1e: Decode\n",
            "--------------------------------------------------------------------------------\n",
            "torch.Size([23, 5, 3])\n",
            "combined_outputs Sanity Checks Passed!\n",
            "--------------------------------------------------------------------------------\n",
            "All Sanity Checks Passed for Question 1e: Decode!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2z-_OIXwXLU"
      },
      "source": [
        "#### f. (10 points) Implemente la función `step` de `nmt_model.py`.\n",
        "Esta función aplica la celda LSTM del **Decoder** para un único índice de tiempo, calcula el encoding del *subword* destino $\\mathbf{h_t^{dec}}$, los puntajes de atención $\\mathbf{e_t}$, la distribución de atención $\\alpha_t$, la salidas de atención $\\mathbf{a_t}$ y finalmente la salida combinada $\\mathbf{o_t}$.\n",
        "Puedes ejecutar un test no exhaustivo mediante: `python sanity_check.py 1f`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBQcMfgrwU_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bad606-9f1a-4723-d8f3-898e10fc6c83"
      },
      "source": [
        "!python sanity_check.py 1f"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "--------------------------------------------------------------------------------\n",
            "Running Sanity Check for Question 1f: Step\n",
            "--------------------------------------------------------------------------------\n",
            "dec_state[0] Sanity Checks Passed!\n",
            "dec_state[1] Sanity Checks Passed!\n",
            "combined_output  Sanity Checks Passed!\n",
            "e_t Sanity Checks Passed!\n",
            "--------------------------------------------------------------------------------\n",
            "All Sanity Checks Passed for Question 1f: Step!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jLHq1ZYzXIOJ"
      },
      "source": [
        "#### g. (3 points) La función `generate_sent_masks()` de `nmt_model.py` produce un tensor llamado `enc_masks`. Con dimensiones (batch_size, max source sentence length) y contiene unos en las posiciones correspondientes a los símbolos *pad* de entrada y ceros para el resto de símbolos. Observar cómo se utilizan las máscaras durante el cálculo de la atención en la función `step`.\n",
        "Primero explique (aprox. 3 oraciones) qué efecto tienen las máscaras sobre el mecanismo completo de atención. Luego explique (en una o dos oraciones), por qué es necesario utilizar las máscaras de esta manera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbx045yd5nSg"
      },
      "source": [
        "<font color = \"green\">\n",
        "\n",
        "* Al procesar en **Batch** se toma un conjunto de **b** \n",
        "oraciones, todas ellas tabuladas a la **misma longitud**.\n",
        "\n",
        "* Se utiliza una **máscara** para que no tome en cuenta los instantes de **tiempo** que la RNN calcula el estado $\\mathbf{h_i}$ para la entrada **< pad>**\n",
        " \n",
        "* Está implementado poniendo a cero el coeficiente que pondera a determinado estado. ($\\alpha_{t,i} = 0$)\n",
        "\n",
        "$$a_t = \\sum_{i=1}^m \\alpha_{t,i} h_i^{enc}$$\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOnZOVQALL_"
      },
      "source": [
        "### Ejecución\n",
        "\n",
        "Es momento de ejecutar las cosas! Ejecute lo siguiente para generar el archivo de vocabulario necesario\n",
        "`sh run.sh vocab`\n",
        "Ó si estás en un entorno Windows, utilice el siguiente comando. Asegurese que ejecutas esto en un entorno que tiene a python como PATH.\n",
        "`run.bat vocab`\n",
        "\n",
        "Como se observó anteriormente, recomendamos que desarrolles el código en tu ordenador personal. Confirma que estás ejecutando el entorno conda correcto y luego ejecuta el siguiente comando para entrenar el modelo en tu máquina local.\n",
        "\n",
        "(linux) `sh run.sh train_local`\n",
        "(windows) `run.bat train_local`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0-8hQL6AXJZ",
        "outputId": "ded53e31-260e-42f2-a3f2-8cd7f9c85ebf"
      },
      "source": [
        "# !sh run.sh vocab\n",
        "!sh run.sh train_local"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "uniformly initialize parameters [-0.100000, +0.100000]\n",
            "use device: cpu\n",
            "begin Maximum Likelihood training\n",
            "epoch 1, iter 10, avg. loss 205.01, avg. ppl 2921.41 cum. examples 320, speed 67.66 words/sec, time elapsed 121.51 sec\n",
            "epoch 1, iter 20, avg. loss 176.73, avg. ppl 734.35 cum. examples 640, speed 66.92 words/sec, time elapsed 249.57 sec\n",
            "epoch 1, iter 30, avg. loss 164.54, avg. ppl 534.99 cum. examples 960, speed 62.01 words/sec, time elapsed 384.72 sec\n",
            "epoch 1, iter 40, avg. loss 170.54, avg. ppl 474.57 cum. examples 1280, speed 60.92 words/sec, time elapsed 530.09 sec\n",
            "epoch 1, iter 50, avg. loss 162.54, avg. ppl 376.99 cum. examples 1600, speed 69.44 words/sec, time elapsed 656.36 sec\n",
            "epoch 1, iter 60, avg. loss 153.87, avg. ppl 349.35 cum. examples 1920, speed 69.00 words/sec, time elapsed 778.22 sec\n",
            "epoch 1, iter 70, avg. loss 153.79, avg. ppl 305.00 cum. examples 2240, speed 62.40 words/sec, time elapsed 916.09 sec\n",
            "epoch 1, iter 80, avg. loss 151.62, avg. ppl 307.01 cum. examples 2560, speed 63.23 words/sec, time elapsed 1050.07 sec\n",
            "epoch 1, iter 90, avg. loss 144.56, avg. ppl 271.58 cum. examples 2880, speed 61.66 words/sec, time elapsed 1183.93 sec\n",
            "epoch 1, iter 100, avg. loss 143.52, avg. ppl 228.83 cum. examples 3200, speed 68.64 words/sec, time elapsed 1307.07 sec\n",
            "epoch 1, iter 110, avg. loss 139.71, avg. ppl 223.59 cum. examples 3520, speed 66.38 words/sec, time elapsed 1431.56 sec\n",
            "epoch 1, iter 120, avg. loss 129.91, avg. ppl 196.69 cum. examples 3840, speed 70.41 words/sec, time elapsed 1543.34 sec\n",
            "epoch 1, iter 130, avg. loss 136.88, avg. ppl 185.07 cum. examples 4160, speed 70.90 words/sec, time elapsed 1661.68 sec\n",
            "epoch 1, iter 140, avg. loss 133.05, avg. ppl 182.16 cum. examples 4480, speed 67.65 words/sec, time elapsed 1782.59 sec\n",
            "epoch 1, iter 150, avg. loss 134.60, avg. ppl 171.33 cum. examples 4800, speed 73.40 words/sec, time elapsed 1896.68 sec\n",
            "epoch 1, iter 160, avg. loss 129.77, avg. ppl 159.55 cum. examples 5120, speed 67.88 words/sec, time elapsed 2017.29 sec\n",
            "epoch 1, iter 170, avg. loss 132.70, avg. ppl 165.89 cum. examples 5440, speed 59.53 words/sec, time elapsed 2156.85 sec\n",
            "epoch 1, iter 180, avg. loss 137.89, avg. ppl 150.63 cum. examples 5760, speed 67.94 words/sec, time elapsed 2286.35 sec\n",
            "epoch 1, iter 190, avg. loss 136.29, avg. ppl 161.85 cum. examples 6080, speed 59.03 words/sec, time elapsed 2431.59 sec\n",
            "epoch 1, iter 200, avg. loss 128.81, avg. ppl 141.71 cum. examples 6400, speed 57.69 words/sec, time elapsed 2575.83 sec\n",
            "epoch 1, iter 210, avg. loss 123.61, avg. ppl 152.07 cum. examples 6720, speed 57.98 words/sec, time elapsed 2711.61 sec\n",
            "epoch 1, iter 220, avg. loss 133.27, avg. ppl 138.10 cum. examples 7040, speed 69.04 words/sec, time elapsed 2836.96 sec\n",
            "epoch 1, iter 230, avg. loss 122.58, avg. ppl 117.75 cum. examples 7360, speed 71.41 words/sec, time elapsed 2952.16 sec\n",
            "epoch 1, iter 240, avg. loss 126.68, avg. ppl 115.85 cum. examples 7680, speed 64.29 words/sec, time elapsed 3084.84 sec\n",
            "epoch 1, iter 250, avg. loss 127.42, avg. ppl 116.81 cum. examples 8000, speed 69.69 words/sec, time elapsed 3207.75 sec\n",
            "epoch 1, iter 260, avg. loss 127.05, avg. ppl 103.99 cum. examples 8320, speed 71.36 words/sec, time elapsed 3330.43 sec\n",
            "epoch 1, iter 270, avg. loss 114.96, avg. ppl 104.42 cum. examples 8640, speed 72.20 words/sec, time elapsed 3440.04 sec\n",
            "epoch 1, iter 280, avg. loss 121.38, avg. ppl 106.35 cum. examples 8960, speed 67.31 words/sec, time elapsed 3563.68 sec\n",
            "epoch 1, iter 290, avg. loss 130.33, avg. ppl 126.53 cum. examples 9280, speed 62.76 words/sec, time elapsed 3700.97 sec\n",
            "epoch 1, iter 300, avg. loss 121.34, avg. ppl 103.17 cum. examples 9600, speed 69.92 words/sec, time elapsed 3820.76 sec\n",
            "epoch 1, iter 310, avg. loss 121.44, avg. ppl 96.24 cum. examples 9920, speed 70.07 words/sec, time elapsed 3942.19 sec\n",
            "epoch 1, iter 320, avg. loss 118.75, avg. ppl 93.39 cum. examples 10240, speed 63.88 words/sec, time elapsed 4073.31 sec\n",
            "epoch 1, iter 330, avg. loss 122.52, avg. ppl 102.40 cum. examples 10560, speed 72.49 words/sec, time elapsed 4190.15 sec\n",
            "epoch 1, iter 340, avg. loss 105.72, avg. ppl 87.79 cum. examples 10880, speed 69.00 words/sec, time elapsed 4299.72 sec\n",
            "epoch 1, iter 350, avg. loss 113.92, avg. ppl 90.15 cum. examples 11200, speed 73.86 words/sec, time elapsed 4409.36 sec\n",
            "epoch 1, iter 360, avg. loss 113.50, avg. ppl 85.79 cum. examples 11520, speed 61.36 words/sec, time elapsed 4542.31 sec\n",
            "epoch 1, iter 370, avg. loss 113.76, avg. ppl 89.28 cum. examples 11840, speed 65.24 words/sec, time elapsed 4666.53 sec\n",
            "epoch 1, iter 380, avg. loss 114.88, avg. ppl 94.97 cum. examples 12160, speed 63.93 words/sec, time elapsed 4792.81 sec\n",
            "epoch 1, iter 390, avg. loss 117.50, avg. ppl 81.46 cum. examples 12480, speed 60.55 words/sec, time elapsed 4933.93 sec\n",
            "epoch 1, iter 400, avg. loss 119.12, avg. ppl 82.99 cum. examples 12800, speed 63.71 words/sec, time elapsed 5069.34 sec\n",
            "epoch 1, iter 410, avg. loss 110.53, avg. ppl 73.29 cum. examples 13120, speed 60.91 words/sec, time elapsed 5204.56 sec\n",
            "epoch 1, iter 420, avg. loss 112.89, avg. ppl 70.11 cum. examples 13440, speed 72.74 words/sec, time elapsed 5321.42 sec\n",
            "epoch 1, iter 430, avg. loss 114.66, avg. ppl 85.23 cum. examples 13760, speed 68.82 words/sec, time elapsed 5441.35 sec\n",
            "epoch 1, iter 440, avg. loss 113.05, avg. ppl 76.44 cum. examples 14080, speed 62.11 words/sec, time elapsed 5575.67 sec\n",
            "epoch 1, iter 450, avg. loss 107.85, avg. ppl 72.11 cum. examples 14400, speed 59.79 words/sec, time elapsed 5710.59 sec\n",
            "epoch 1, iter 460, avg. loss 115.02, avg. ppl 80.90 cum. examples 14720, speed 62.15 words/sec, time elapsed 5845.40 sec\n",
            "epoch 2, iter 470, avg. loss 110.74, avg. ppl 61.88 cum. examples 15015, speed 61.73 words/sec, time elapsed 5973.69 sec\n",
            "epoch 2, iter 480, avg. loss 105.04, avg. ppl 52.72 cum. examples 15335, speed 67.64 words/sec, time elapsed 6099.01 sec\n",
            "epoch 2, iter 490, avg. loss 103.87, avg. ppl 55.92 cum. examples 15655, speed 61.24 words/sec, time elapsed 6233.90 sec\n",
            "epoch 2, iter 500, avg. loss 100.26, avg. ppl 53.29 cum. examples 15975, speed 57.94 words/sec, time elapsed 6373.17 sec\n",
            "epoch 2, iter 510, avg. loss 102.25, avg. ppl 51.47 cum. examples 16295, speed 59.14 words/sec, time elapsed 6513.57 sec\n",
            "epoch 2, iter 520, avg. loss 100.02, avg. ppl 53.83 cum. examples 16615, speed 66.70 words/sec, time elapsed 6633.96 sec\n",
            "epoch 2, iter 530, avg. loss 104.65, avg. ppl 49.05 cum. examples 16935, speed 62.07 words/sec, time elapsed 6772.54 sec\n",
            "epoch 2, iter 540, avg. loss 103.51, avg. ppl 51.39 cum. examples 17255, speed 61.88 words/sec, time elapsed 6908.42 sec\n",
            "epoch 2, iter 550, avg. loss 101.66, avg. ppl 47.54 cum. examples 17575, speed 59.80 words/sec, time elapsed 7049.30 sec\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 360, in <module>\n",
            "    main()\n",
            "  File \"run.py\", line 352, in main\n",
            "    train(args)\n",
            "  File \"run.py\", line 185, in train\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 255, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 149, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXk565X7A2J4"
      },
      "source": [
        "Deberías ver una disminución significativa del coste durante las iteraciones iniciales. Una vez que garantices que tu código no se rompe (ej. ejecutarlo hasta la iteración 10 o 20) enciende la máquina virtual desde el portal de Azure.\n",
        " Luego lee la sección *Managing Code Deployment to a VM` de nuestra guía práctica a VMs https://docs.google.com/document/d/1jtANWXbIYXMZO_2X7jupauPxcEbz-TVJkdatg4gzOdk/edit\n",
        "Por instrucciones de cómo subir tu código a la VM.\n",
        "\n",
        "Luego instala los paquetes necesarios a tu máquina virtual\n",
        "`pip install -r gpu_requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA2Q22j_BHx3",
        "outputId": "6d7d7bf9-b0f9-4e81-ac7c-41120a94f9ec"
      },
      "source": [
        "!pip install -r gpu_requirements.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 2)) (0.6.2)\n",
            "Collecting tqdm==4.29.1\n",
            "  Downloading tqdm-4.29.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 4)) (0.1.96)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 6)) (1.9.0+cu102)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->-r gpu_requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (1.19.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (2.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (0.4.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (0.8.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r gpu_requirements.txt (line 6)) (3.7.4.3)\n",
            "Installing collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.29.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.29.1 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.29.1 which is incompatible.\u001b[0m\n",
            "Successfully installed tqdm-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjcXSWXkBKvx"
      },
      "source": [
        "Finalmente, ir a la sección de la guía y sigue las instrucciones para crear una nueva sesión `tmux`. Concretamente, ejecutar el siguiente comando para crear una sesión `tmux` llamada `nmt`\n",
        "\n",
        "`tmux new -s nmt`\n",
        "\n",
        "Una vez configurado tu VM y estás en una sesión `tmux`, ejecute:\n",
        "(linux) sh run.sh train\n",
        "(Windows) run.bat train\n",
        "\n",
        "Una vez sepas que tu código funciona correctamente, puedes desconectarte de sesión y cerrar tu conexión ssh al servidor.\n",
        "Para hacerlo ejecuta:\n",
        "`tmux detach`\n",
        "\n",
        "Puedes volver a tu modelo de entrenamiento reconectandote al servidor y añadiendo la sesión `tmux` con:\n",
        "`tmux a -t nmt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAIExZRYeh9_"
      },
      "source": [
        "!sh run.sh train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "R5Tst7yCXIOJ"
      },
      "source": [
        "#### h. (3 points) Una vez tu modelo haya finalizado de entrenar (debería tomar menos de 1 hora en la máquina virtual) ejecute el siguiente comando para testear el modelo.\n",
        "(linux) sh run.sh test\n",
        "(windows) run.bat test\n",
        "Por favor, reporte el puntaje BLEU. Debería ser mayor a 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpWyt2OTCz6h",
        "outputId": "95b66925-6366-484d-fc28-54c09d1aa342"
      },
      "source": [
        "!sh run.sh test"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "load test source sentences from [./chr_en_data/test.chr]\n",
            "load test target sentences from [./chr_en_data/test.en]\n",
            "load model from model.bin\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 360, in <module>\n",
            "    main()\n",
            "  File \"run.py\", line 354, in main\n",
            "    decode(args)\n",
            "  File \"run.py\", line 291, in decode\n",
            "    model = NMT.load(args['MODEL_PATH'])\n",
            "  File \"/content/Tps/a4/nmt_model.py\", line 480, in load\n",
            "    params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 594, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 211, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'model.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC2V7hMLDAxc"
      },
      "source": [
        "<font color = \"green\">\n",
        "BLEU =\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoTluyBHC04s"
      },
      "source": [
        "#### i. (4 points) (written)\n",
        "En clase hemos aprendido sobre la atención de producto interno, atención multiplicativa y atención aditiva. Como recordatorio, la atención de producto interno es $\\mathbf{e_{t,i}}=\\mathbf{s_t^T\\ h_i}$, La atención multiplicativa es $\\mathbf{e_{t,i}}=\\mathbf{s_t^T\\ W\\ h_i}$ y la atención aditiva es $\\mathbf{e_{t,i}}=\\mathbf{v^T}\\ tanh(\\mathbf{W_1\\ h_i}+\\mathbf{W_2\\ s_t})$\n",
        "\n",
        "* i. (2 points) Explique una ventaja y una desventaja del producto interno como atención comparado con la atención multiplicativa.\n",
        "* ii. (2 points) Explicar una ventaja y una desventaja de la atención aditiva respecto a la atención multiplicativa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ILF7urHiXIOK"
      },
      "source": [
        "## 2. Analizando el sistema NMT (30 points)\n",
        "\n",
        "En la parte 1 hemos modelado nuestro problema NMT a un *subword-level*. Esto es, dada una oración en el idioma de origen, encontramos los componentes a nivel *subword* desde la matriz de *embeddings*.\n",
        "Alternativamente podríamos haber modelado el problema NMT a nivel palabra, obervando palabras completas desde la matriz de *embeddings*.\n",
        "### a. Por qué modelar el sistema **NMT** bajo *subword-level* vs *word-level*? (2 points)\n",
        "**Pista**: El Cherokee es un idioma polysintético"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9KdjH13oXIOK"
      },
      "source": [
        "<font color =\"green\">\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "P_HrBEdwXIOK"
      },
      "source": [
        "### b. Por qué las codificaciones (*Embeddings*) a nivel *letra* y *Subword* suelen ser más chicos que la codificación a nivel *palabra*? (2 points)\n",
        "Dar una razón en 1-2 oraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9rv3M8XDXIOL"
      },
      "source": [
        "<font color =\"green\">\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Atm1qyVJXIOL"
      },
      "source": [
        "### c. Cómo ayuda el entrenamiento multilenguaje en mejorar el rendimiento sobre lenguajes de bajos recursos? (2 points)\n",
        "Un reto de entrenar modelos **NMT** exitosos es la carencia de datos del lenguaje, particularmente para lenguajes de recursos escasos como el Cherokee. Una forma de responder a este reto es mediante el entrenamiento multi-lenguaje, donde entrenamos nuestro modelo **NMT** sobre varios idiomas (incluyendo Cherokee). <br>\n",
        "Puedes leer más en https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6LQJCFJxXIOL"
      },
      "source": [
        "<font color =\"green\">\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jq98EKqfXIOL"
      },
      "source": [
        "### d. (6 points)\n",
        "Aquí presentamos 3 ejemplos de errores encontrados en las salidas de nuestro modelo **NMT** (que es el mismo entrenado por tí). Los errores están subrayados.\n",
        "Por cada ejemplo de la oración de origen, referenciar la traducción a inglés y la traducción obtenida por el modelo.\n",
        "* Incluir las posibles razones que expliquen el error cometido por el modelo. (Tanto como su construcción lingüística o una limitación específica del modelo)\n",
        "* Describir una posible manera para corregir el error observado. Hay más de una manera de corregir un error. Por ejemplo, se podría ajustar el tamaño de la capa oculta o cambiando el mecanismo de atención.\n",
        "\n",
        "A continuación están las traducciones a analizar. Sólo analizar el error subrayado de cada oración. Se asegura que no tienes que saber Cherokee para responder estas preguntas. Pero si quieres saber un poco más de este lenguaje, sientete libre de consultar https://www.cherokeedictionary.net/ para buscar palabras.\n",
        "\n",
        "#### i. (2 points)\n",
        "**Origen de la traducción** (Source) = *Yona utsesdo anitsilvsgi digvtanv uwoduisdei* <br>\n",
        "**Traducción de referencia** *Fern had a crown of daisies in her hair*<br>\n",
        "**Traducción del modelo**: *Fern had <u>her hair</u> with her hair*\n",
        "#### ii. (2 points)\n",
        "**Origen de la traducción** (Source) = *Ulihelisdi nigalisda.* <br>\n",
        "**Traducción de referencia** *She is very excited.*<br>\n",
        "**Traducción del modelo**: *<u>it's</u> joy*\n",
        "#### iii. (2 points)\n",
        "**Origen de la traducción** (Source) = *Tsesdi hana yitsadawoesdi usdi atsadi!* <br>\n",
        "**Traducción de referencia** *Don't swim there, Littlefish!*<br>\n",
        "**Traducción del modelo**: *Don't know how <u>a small fish!</u>*\n",
        "\n",
        "### e. (4 points)\n",
        "Exploremos las salidas del modelo que hemos entrenado! Las traducciones del set de testeo producidas en el punto 1-i deberían encontrarse en\n",
        " `outputs/test_outputs.txt`.\n",
        "1. (2 points) Encontrar el límite donde las traducciones predichas son correctas para secuencias largas (4 o 5 palabras). Verificar el archivo destino *target* (Inglés).\n",
        "Contiene el archivo de entrenamiento ese texto (casi) literal? Si lo tiene o no, qué aprendió el sistema a hacer?\n",
        "2. (2 points) Encontrar el límite donde la traducción predicha comienza correcta durante una secuencia de 4 o 5 palabras, pero luego diverge (donde el final de la oración parece no tener sentido alguno). Qué dice esto sobre el comportamiento del decodificador?\n",
        "\n",
        "### f. (14 points)\n",
        "El puntaje **BLEU** es la métrica de evaluación automática más utilizada para sistemas **NMT**. Se suele calcular sobre el set de testeo completo, pero aquí consideraremos el puntaje para un solo ejemplo (*).\n",
        "Suponga que tenemos una oración de origen $\\mathbf{s}$, un conjunto de $k$ traducciones de referencia $\\mathbf{r_1 \\dots r_k}$ y una traducción candidata $\\mathbf{c}$.\n",
        "Para calcular el puntaje **BLEU** de $\\mathbf{c}$, primero debemos calcular la precisión n-grama modificada $p_n$ de $\\mathbf{c}$, para cada $n=1,2,3,4$ donde n es el n-grama.\n",
        "\n",
        "$p_n=\\frac{\\sum_{ngram\\in c} min(\n",
        "max_{i=1...k}(Count_{\\mathbf{r_i}} (n-gram), Count_{\\mathbf{c}} (n-gram))\n",
        ")}\n",
        "{\\sum_{ngram\\in c} Count_{\\mathbf{c}}(ngram)}$\n",
        "\n",
        "Aquí por cada n-grama que aparece en la traducción candidata $\\mathbf{c}$, contamos la máxima cantidad de veces que aparece en cualquiera traducción de referencia, limitado a la cantidad de veces que aparece en $\\mathbf{c}$ (este es el numerador). Dividimos este número por la cantidad de n-gramas en $\\mathbf{c}$ (denominador).\n",
        "\n",
        "(\\*) Esta definición de puntaje **BLEU** nivel oración coincide con la función `sentence_bleu()` en el paquete `nltk`. Observar que la función del NLTK es sensible a las mayúsculas. En esta pregunta, todo el texto está en minúsculas. https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu\n",
        "\n",
        "Luego, calculamos la *Brevity pealty* **BP**. <br>\n",
        "Sea $len(\\mathbf{c})$ la longitud de $\\mathbf{c}$ y sea $len(\\mathbf{r})$ la longitud de la traducción de referencia que es más cercana a $len(\\mathbf{c})$ (En el caso de dos referencias equidistantes, elegir $len(\\mathbf{r})$ como la más corta.)\n",
        "\n",
        "$ BP = \\begin{cases}\n",
        "1 \\hspace{10 mm} &si \\hspace{5mm} len(\\mathbf{c})\\ge len(\\mathbf{r}) \\\\\n",
        "exp(1-\\frac{len(\\mathbf{r})}{len (\\mathbf{c})})  &e.o.c.\n",
        "\\end{cases} $\n",
        "\n",
        "Finalmente, el puntaje **BLEU** para el candidato $\\mathbf{c}$ respecto a $\\mathbf{r_1 \\dots r_n}$ es:\n",
        "\n",
        "$BLEU = BP·exp(\\sum_{n=1}^4 \\lambda_n\\ log\\ p_{n})$\n",
        "\n",
        "Donde $\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4$ son pesos cuya suma es 1. El $log$ es el logaritmo natural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FXy_t2nzXIOM"
      },
      "source": [
        "#### i. Considere el ejemplo en español. (5 points)\n",
        "Oración de **Origen**: **El amor todo lo puede**<br>\n",
        "Traducción de referencia $\\mathbf{r_1}$:     *Love can always find a way* <br>\n",
        "Traducción de referencia $\\mathbf{r_2}$:     *Love makes anything possible*<br>\n",
        "Modelo NMT $\\mathbf{c_1}$: *The love can always do* <br>\n",
        "Modelo NMT $\\mathbf{c_2}$: *love can make anything possible*\n",
        "\n",
        "* Calcule el puntaje **BLEU** para $\\mathbf{c_1}$ y $\\mathbf{c_2}$.\n",
        "Sea $\\lambda_i = 0.5$ para $i\\in \\{1,2\\}$ y $\\lambda_i = 0$ para $i\\in \\{3,4\\}$ (Esto significa ignorar los 3-gramas y 4-gramas. no calcular $p_3$ y $p_4$)\n",
        "Cuando se calculan los puntajes BLEU, mostrar tu trabajo (valores $p_1, p_2, len(\\mathbf{c}),len(\\mathbf{r}), BP$). Notar que los puntajes **BLEU** se pueden expresar entre 0 y 1, o entre 0 y 100. El código utiliza la escala 0-100, mientras que las preguntas utilizan la escala 0-1.\n",
        "\n",
        "Cuál de las dos traducciones se considera mejor según **BLEU**? Estás de acuerdo en que es la mejor traducción?\n",
        "\n",
        "ii. (5 points) Nuestro disco duro se ha corrompido y hemos perdido la traducción de referencia $\\mathbf{r_2}$. Por favor, recalcule el puntaje **BLEU** para $\\mathbf{c_1}$ y $\\mathbf{c_2}$, esta vez solamente respecto a $\\mathbf{r_1}$. Cuál de las dos traducciones recive un mayor puntaje? Estás de acuerdo en que es la mejor traducción?\n",
        "iii. (2 points) Por la disponibilidad de datos, los sistemas NMT suelen evaluarse respecto a una sola traducción de referencia. Explique por qué podría ser problemático.\n",
        "iv. (2 points) Enumerar dos ventajas y dos desventajas de **BLEU** comparado con la evaluación humana como métrica para *Machine Translation*"
      ]
    }
  ]
}