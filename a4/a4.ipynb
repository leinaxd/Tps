{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "a4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leinaxd/Tps/blob/master/a4/a4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "x__t1uIZd139"
      },
      "source": [
        "# GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAfnePXhF3J2",
        "cellView": "form"
      },
      "source": [
        "#@markdown SSH\n",
        "\n",
        "#@markdown Ingrese directorio con contraseña privada\n",
        "directorio = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!ssh-keygen -h dsd\n",
        "# !ssh-keygen -t ed25519 -C \"leinaxd@gmail.com\"\n",
        "# %cd /root/.ssh/\n",
        "# !ls -la\n",
        "# !cat id_ed25519.pub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PehpfW10d14A",
        "cellView": "form",
        "outputId": "d0b20d80-c391-4110-c0eb-28b1ee18f3d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@markdown Cargar\n",
        "%cd ~\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/leinaxd/Tps/\n",
        "\n",
        "%cd Tps/a4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/content\n",
            "Cloning into 'Tps'...\n",
            "remote: Enumerating objects: 700, done.\u001b[K\n",
            "remote: Counting objects: 100% (700/700), done.\u001b[K\n",
            "remote: Compressing objects: 100% (524/524), done.\u001b[K\n",
            "remote: Total 700 (delta 391), reused 412 (delta 163), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (700/700), 78.37 MiB | 23.67 MiB/s, done.\n",
            "Resolving deltas: 100% (391/391), done.\n",
            "Checking out files: 100% (221/221), done.\n",
            "/content/Tps/a4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqNCm1kNt547",
        "cellView": "form"
      },
      "source": [
        "AUTH = \"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\" #@param {type:\"string\"}\n",
        "!git config --global user.email \"leinaxd@gmail.com\"\n",
        "!git config --global user.name \"leinaxd\"\n",
        "!git remote set-url origin https://$AUTH@github.com/leinaxd/Tps.git"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "E2DN-kWZd14C",
        "cellView": "form",
        "outputId": "70f24e00-f9ce-43fe-e199-545e2ade3aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "commit = \"a4 1g: nmt trained\" #@param {type:\"string\"}\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"$commit\"\n",
        "!git push\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master 41fcec3] a4 1g: nmt trained\n",
            " 5 files changed, 2 insertions(+), 2 deletions(-)\n",
            "Counting objects: 9, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 12.59 KiB | 12.59 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/leinaxd/Tps.git\n",
            "   9918213..41fcec3  master -> master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "cellView": "form",
        "id": "Mc8E2_i6d14D"
      },
      "source": [
        "#@markdown pull / update\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Gn5O6OqEhB"
      },
      "source": [
        "# Instalar librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKAYyMXjqHOl",
        "outputId": "95755fc1-2f18-4ebf-e631-933d43bcacd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install sacrebleu"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.4 portalocker-2.3.2 sacrebleu-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Z3vHnNuLd14E"
      },
      "source": [
        "# CS224N Assignment 4: Neural Machine Translation with RNNs (45 + 30 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gNPr6wj8d14E"
      },
      "source": [
        "Esta tarea se divide en dos partes: *Neural Machine Translation with RNNs* y *Analyzing NMT systems*. El primero es básicamente codificación y el segundo son preguntas escritas. Si te atoras en la primera sección, puedes trabajar en la segunda ya que las secciones son independientes.\n",
        "La notación e implementación de los *sistemas NMT* es tramposa. Recomendamos la lectura de Zhang, 2020. https:arxic.org/abs/2010.04791 para entender mejor la tarea de traducción *Cherokee-Inglés* que sirvió de inspiración para este proyecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEpOpuMDfBcR"
      },
      "source": [
        "## 1. Neural Machine Translation with RNNs (45 points)\n",
        "\n",
        "En *Machine Translation* el objetivo es convertir una oración desde un lenguaje *source*(Cherokee) hacia un lenguaje *target* (Inglés).<br>\n",
        "En este proyecto implementaremos un modelo neuronal **seq2seq** con **Atención**. En esta sección, describiremos el **procedimiento de entrenamiento** para el sistema **NMT** propuesto, el cual utiliza un **Encoder** **LSTM** **Bidireccional** y un **Decoder** **LSTM** **Unidireccional**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ajj9oU_goY0"
      },
      "source": [
        "### Descripción del modelo (Procedimiento de aprendizaje)\n",
        "![modelo](https://github.com/leinaxd/Tps/raw/master/a4/imgs/arquitecturaNMT.jpg)\n",
        "Figura 1. Modelo seq2seq con Atención multiplicativa, mostrada en la tercera fase del decoder. Los estados ocultos $h_i^{enc}$ y los estados de las celdas $c_i^{enc}$ se definen a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "IKnv473nXIOC"
      },
      "source": [
        "Dada una oración en un lenguaje de origen, buscaremos los embeddings a nivel *subword* a partir de la matriz de *embeddings* obteniendo $x_1...x_m \\hspace{10mm} (x_i\\in \\mathbb{R}^{e\\times 1})$, donde $m$ es la longitud de la oración de entrada y $e$ la dimensión del embedding.\n",
        "Alimentamos estos embeddings al encoder bidireccional, obteniendo estados ocultos y estados de celda tanto para el *forwards* como *backwards* del **LSTM**. Las versiones *Forwards* y *Backwards* se concatenan para obtener el estado oculto $\\mathbf{h_i^{enc}}$ y estado de celda $\\mathbf{c_i^{enc}}$:\n",
        "\n",
        "$\\begin{align}\n",
        "\\mathbf{h_i^{enc}} &= [\\overleftarrow{\\mathbf{h_i^{enc}}}; \\overrightarrow{\\mathbf{h_i^{enc}}}] \\hspace{10mm} &&con \\hspace{5mm} \\mathbf{h_i^{enc}}\\in \\mathbb{R}^{2h\\times 1},\\hspace{5mm} \\overleftarrow{\\mathbf{h_i^{enc}}},\\overrightarrow{\\mathbf{h_i^{enc}}}\\in \\mathbb{R}^{h\\times 1} \\hspace{5mm} 1\\le i\\le m\\\\\n",
        "\\mathbf{c_i^{enc}} &= [\\overleftarrow{\\mathbf{c_i^{enc}}}; \\overrightarrow{\\mathbf{c_i^{enc}}}] \\hspace{10mm} &&con \\hspace{5mm} \\mathbf{c_i^{enc}}\\in \\mathbb{R}^{2h\\times 1},\\hspace{5mm} \\overleftarrow{\\mathbf{c_i^{enc}}},\\overrightarrow{\\mathbf{c_i^{enc}}}\\in \\mathbb{R}^{h\\times 1} \\hspace{5mm} 1\\le i\\le m\\\\\n",
        "\\end{align} $\n",
        "\n",
        "Luego inicializaremos el primer estado oculto del **Decoder** $\\mathbf{h_0^{dec}}$ y el estado de celda $\\mathbf{c_0^{dec}}$ con una proyección lineal del estado oculto y celda final del **encoder**.<br>\n",
        "(Si no es obvio, piense por qué consideramos $[\\overleftarrow{\\mathbf{h_1^{enc}}},\\overrightarrow{\\mathbf{h_m^{enc}}}]$ el estado final del encoder.)\n",
        "\n",
        "$\\begin{align}\n",
        "\\mathbf{h_0^{dec}} &= \\mathbf{W_h} [\\overleftarrow{\\mathbf{h_1^{enc}}}; \\overrightarrow{\\mathbf{h_m^{enc}}}] \\hspace{10mm} &&con \\hspace{5mm} \\mathbf{h_0^{dec}}\\in \\mathbb{R}^{h\\times 1},\\hspace{5mm} \\mathbf{W_h}\\in \\mathbb{R}^{h\\times 2h}\\\\\n",
        "\\mathbf{c_0^{dec}} &= \\mathbf{W_c} [\\overleftarrow{\\mathbf{c_1^{enc}}}; \\overrightarrow{\\mathbf{c_m^{enc}}}] \\hspace{10mm} &&con \\hspace{5mm} \\mathbf{c_0^{dec}}\\in \\mathbb{R}^{h\\times 1},\\hspace{5mm} \\mathbf{W_c}\\in \\mathbb{R}^{h\\times 2h} \\\\\n",
        "\\end{align} $\n",
        "\n",
        "Con el **Decoder** inicializado, debemos alimentarlo con la oración *target* (destino). En el $t^{mo}$ paso, buscamos el embedding para la $t^{mo}$ *subword* $\\mathbf{y_t}\\in \\mathbb{R}^{e\\times 1}$. Luego concatenamos $\\mathbf{y_t}$ con un vector de salida combinado $o_{t-1}\\in \\mathbb{R}^{h\\times 1}$ del instante anterior (Explicado más adelante), para producir $\\bar{\\mathbf{y_t}}\\in \\mathbb{R}^{(e+h)\\times 1}$. Notar que para el primer *subword* destino (*target*) (El token de inicio) $\\mathbf{o_0}$ es un vector de ceros. Luego alimentamos $\\bar{\\mathbf{y_t}}$ como entrada al decoder.\n",
        "\n",
        "$\\mathbf{h_t^{dec}, \\mathbf{c_t^{dec}}} = Decoder(\\bar{\\mathbf{y_t}}, \\mathbf{h_{t-1}^{dec}}, \\mathbf{c_{t-1}^{dec}}) \\hspace{10mm} donde\\hspace{5mm} \\mathbf{h_t^{dec}}\\in \\mathbb{R}^{h\\times 1}, \\mathbf{c_t^{dec}}\\in \\mathbb{R}^{h\\times 1}\n",
        "$\n",
        "\n",
        "Luego utlizaremos $\\mathbf{h_t^{dec}}$ para calcular la atención de forma multiplicativa sobre $\\mathbf{h_1^{enc} \\dots h_m^{enc}}$:\n",
        "\n",
        "$\\begin{align} \\mathbf{e_{t,i}}\n",
        "&=(\\mathbf{h_t^{dec}})^T \\mathbf{W_{attProj}}\\mathbf{h_i^{enc}}\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\mathbf{e_t}\\in \\mathbb{R}^{m\\times 1},\n",
        "\\hspace{5mm} \\mathbf{W_{attProj}}\\in \\mathbb{R}^{h\\times 2h}\n",
        "\\hspace{5mm} 1\\le i \\le m\\\\\n",
        "\\alpha_t&=softmax(\\mathbf{e_t})\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\alpha_t\\in \\mathbb{R}^{m\\times 1}\\\\\n",
        "\\mathbf{a_t}&=\\sum_{i=1}^{m}\\alpha_{t,i}\\  \\mathbf{h_i^{enc}}\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\mathbf{a_t}\\in \\mathbb{R}^{2h\\times 1}\n",
        "\\end{align}$\n",
        "\n",
        "Ahora concatenaremos la salida de atención $\\mathbf{a_t}$ con el estado oculto del decoder $\\mathbf{h_t^{dec}}$ y pasarlo por una capa lineal, $tanh$ y una capa *dropout* para atender la salida combinada $\\mathbf{o_t}$\n",
        "\n",
        "$\\begin{align}\n",
        "\\mathbf{u_t} &= [\\mathbf{a_t};\\mathbf{h_t^{dec}}]\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\mathbf{u_t}\\in \\mathbb{R}^{3h\\times 1}\n",
        "\\hspace{5mm}\\\\\n",
        "\\mathbf{v_t} &=\n",
        "\\mathbf{W_u}\\ \\mathbf{u_t}\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm}\\mathbf{v_t}\\in \\mathbb{R}^{h\\times 1},\n",
        "\\hspace{5mm}\\mathbf{W_u}\\in \\mathbb{R}^{h\\times 3h}\\\\\n",
        "\\mathbf{o_t} &=dropout(tanh(\\mathbf{v_t}))\n",
        "\\hspace{10mm} &&donde\n",
        "\\hspace{5mm} \\mathbf{o_t}\\in \\mathbb{R}^{h\\times 1}\n",
        "\\end{align}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_Op4uCJCXIOG"
      },
      "source": [
        "Luego generamos la distribución de proababilidad $\\mathbf{P_t}$ sobre los *subwords* destino (*target*) al\n",
        " $t^{mo}$ paso.\n",
        "\n",
        "$\\mathbf{P_t} = softmax(\\mathbf{W_{vocab}\\ o_t})\n",
        "\\hspace{10mm} donde\\hspace{5mm}\n",
        "\\mathbf{P_t}\\in \\mathbb{R}^{\\mathbf{V_t}\\times 1},\n",
        "\\hspace{5mm}\\mathbf{W_{vocab}}\\in \\mathbb{R}^{\\mathbf{V_t}\\times h}$\n",
        "\n",
        "Aquí $\\mathbf{V_t}$ es el tamaño del vocabulario destino (*target*). Finalmente, para entrenar la red podemos calcular la entropía cruzada softmax entre $\\mathbf{P_t}$ y $\\mathbf{g_t}$, donde $\\mathbf{g_t}$ es el vector *one-hot* del *subword* destino al instante $t$\n",
        "\n",
        "$J_t (\\theta ) =CrossEntropy(\\mathbf{P_t},\\mathbf{g_t})$\n",
        "\n",
        "Donde $\\theta$ representa todos los parámetros del modelo y $J_t(\\theta ) es el costo en el instante $t$ del **decoder**. Ahora que hemos descripto el modelo, tratemos de implementarlo para la traducción Cherokee-Inglés!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jioHmN-EXIOI"
      },
      "source": [
        "### Configurando la máquina virtual\n",
        "Siga las instrucciones de https://docs.google.com/document/d/1BQOAjhBxWbywkB4rMFH9iinb6YHSjaWw1TOVIGfyYho para crear una copia de la máquina virtual. Esto debería tomarte unos 45 minutos. Sin embargo necesitarás la GPU para entrenar el modelo, recomendamos fuertemente que primero desarrolles el código localmente y asegurate que funciona antes de intentar entrenarlo en la máquina virtual. El tiempo de la GPU es limitado y costoso. Toma aproximadamente entre 30 minutos y 1 hora entrenar el sistema NMT. Finalmente, asegurate de apagar la máquina virtual cuando no la estés utilizando.\n",
        "\n",
        "Si la subscripción a Azure se queda sin dinero, tu máquina virtual será temporalmente bloqueada e inaccesible. Si esto sucede, por favor complete el siguiente formulario https://docs.google.com/forms/d/e/1FAIpQLSdpd5CgwSulB_wGCKQb_VmLscicTswiTVBxnw40xVzeuS4BKQ/viewform\n",
        "\n",
        "Para ejecutar el código en tu máquina local, ejecuta los siguientes comandos para crear el apropiado entorno\n",
        "`conda env create --file local_env.yml`\n",
        "Notar que este entorno virtual no se necesita en la máquina virtual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "t5nX7OmfXIOI"
      },
      "source": [
        "### Implementación y preguntas escritas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNql31-Bh3Fi"
      },
      "source": [
        "#### a. Implemente la función `pad_sents` en `utils.py` que producirá estas oraciones tabuladas (2 points)\n",
        "Para aplicar las operaciones sobre los tensores, tenemos que asegurarnos que todas las oraciones del mismo *batch* tengan la misma longitud.\n",
        "* Identifique la oración más larga del *batch* y tabule a las demás en dicha longitud.\n",
        "\n",
        "#### b. (3 points) Implemente la función `__init__` de `model_embeddings.py` \n",
        "Que inicializa los *embeddings* del *source* (oración de origen) y *target* (oración destino)<br>\n",
        "\n",
        "#### c. (4 points) Implemente la función `__init__` de `nmt_model.py` para inicializar los embeddings del modelo y las capas (LSTM, projection y dropout) del sistema NMT\n",
        "(Ademas Utiliza la clase `ModelEmbeddings` de `model_embeddings.py`)\n",
        "#### d. (8 points) Implementar la función `encode` de `nmt_model.py`.\n",
        "Esta función \n",
        "* Convierte las oraciones *source* ya tabuladas en un tensor $\\mathbf{X}$\n",
        "* Genera $\\mathbf{h_1^{enc}\\dots h_m^{enc}}$\n",
        "* Calcula el estado inicial $\\mathbf{h_0^{dec}}$ y celda inicial $\\mathbf{c_0^{dec}}$ del **decoder**.\n",
        "\n",
        "Puedes ejecutar una verificación no exhaustiva mediante:\n",
        "`python sanity_check.py 1d`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtrfVLtvh9qo",
        "outputId": "3f6dac97-89e2-4a64-9bf7-b13a05f0427f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python sanity_check.py 1d"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Running Sanity Check for Question 1d: Encode\n",
            "--------------------------------------------------------------------------------\n",
            "enc_hiddens Sanity Checks Passed!\n",
            "dec_init_state[0] Sanity Checks Passed!\n",
            "dec_init_state[1] Sanity Checks Passed!\n",
            "--------------------------------------------------------------------------------\n",
            "All Sanity Checks Passed for Question 1d: Encode!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKbZzJMTwKob"
      },
      "source": [
        "#### e. (8 points) Implemente la función `decode` de `nmt_model.py`.\n",
        "Esta función construye $\\bar{\\mathbf{y_t}}$ y ejecuta la función `step` por cada índice de tiempo de la entrada.\n",
        "Puedes ejecutar un test no exhaustivo mediante: `python sanity_check.py 1e`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34RJOEl5wJ0c",
        "outputId": "4d836750-c628-446c-d81b-8c55306c638f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python sanity_check.py 1e"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "--------------------------------------------------------------------------------\n",
            "Running Sanity Check for Question 1e: Decode\n",
            "--------------------------------------------------------------------------------\n",
            "torch.Size([23, 5, 3])\n",
            "combined_outputs Sanity Checks Passed!\n",
            "--------------------------------------------------------------------------------\n",
            "All Sanity Checks Passed for Question 1e: Decode!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2z-_OIXwXLU"
      },
      "source": [
        "#### f. (10 points) Implemente la función `step` de `nmt_model.py`.\n",
        "Esta función aplica la celda LSTM del **Decoder** para un único índice de tiempo, calcula el encoding del *subword* destino $\\mathbf{h_t^{dec}}$, los puntajes de atención $\\mathbf{e_t}$, la distribución de atención $\\alpha_t$, la salidas de atención $\\mathbf{a_t}$ y finalmente la salida combinada $\\mathbf{o_t}$.\n",
        "Puedes ejecutar un test no exhaustivo mediante: `python sanity_check.py 1f`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBQcMfgrwU_r",
        "outputId": "b6937fc7-d95d-4bd0-b5f5-2a5896070ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python sanity_check.py 1f"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "--------------------------------------------------------------------------------\n",
            "Running Sanity Check for Question 1f: Step\n",
            "--------------------------------------------------------------------------------\n",
            "dec_state[0] Sanity Checks Passed!\n",
            "dec_state[1] Sanity Checks Passed!\n",
            "combined_output  Sanity Checks Passed!\n",
            "e_t Sanity Checks Passed!\n",
            "--------------------------------------------------------------------------------\n",
            "All Sanity Checks Passed for Question 1f: Step!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jLHq1ZYzXIOJ"
      },
      "source": [
        "#### g. (3 points) La función `generate_sent_masks()` de `nmt_model.py` produce un tensor llamado `enc_masks`. Con dimensiones (batch_size, max source sentence length) y contiene unos en las posiciones correspondientes a los símbolos *pad* de entrada y ceros para el resto de símbolos. Observar cómo se utilizan las máscaras durante el cálculo de la atención en la función `step`.\n",
        "Primero explique (aprox. 3 oraciones) qué efecto tienen las máscaras sobre el mecanismo completo de atención. Luego explique (en una o dos oraciones), por qué es necesario utilizar las máscaras de esta manera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbx045yd5nSg"
      },
      "source": [
        "<font color = \"green\">\n",
        "\n",
        "* Al procesar en **Batch** se toma un conjunto de **b** \n",
        "oraciones, todas ellas tabuladas a la **misma longitud**.\n",
        "\n",
        "* Se utiliza una **máscara** para que no tome en cuenta los instantes de **tiempo** que la RNN calcula el estado $\\mathbf{h_i}$ para la entrada **< pad>**\n",
        " \n",
        "* Está implementado poniendo a cero el coeficiente que pondera a determinado estado. ($\\alpha_{t,i} = 0$)\n",
        "\n",
        "$$a_t = \\sum_{i=1}^m \\alpha_{t,i} h_i^{enc}$$\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOnZOVQALL_"
      },
      "source": [
        "### Ejecución\n",
        "\n",
        "Es momento de ejecutar las cosas! Ejecute lo siguiente para generar el archivo de vocabulario necesario\n",
        "`sh run.sh vocab`\n",
        "Ó si estás en un entorno Windows, utilice el siguiente comando. Asegurese que ejecutas esto en un entorno que tiene a python como PATH.\n",
        "`run.bat vocab`\n",
        "\n",
        "Como se observó anteriormente, recomendamos que desarrolles el código en tu ordenador personal. Confirma que estás ejecutando el entorno conda correcto y luego ejecuta el siguiente comando para entrenar el modelo en tu máquina local.\n",
        "\n",
        "(linux) `sh run.sh train_local`\n",
        "(windows) `run.bat train_local`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0-8hQL6AXJZ"
      },
      "source": [
        "# !sh run.sh vocab\n",
        "!sh run.sh train_local"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXk565X7A2J4"
      },
      "source": [
        "Deberías ver una disminución significativa del coste durante las iteraciones iniciales. Una vez que garantices que tu código no se rompe (ej. ejecutarlo hasta la iteración 10 o 20) enciende la máquina virtual desde el portal de Azure.\n",
        " Luego lee la sección *Managing Code Deployment to a VM` de nuestra guía práctica a VMs https://docs.google.com/document/d/1jtANWXbIYXMZO_2X7jupauPxcEbz-TVJkdatg4gzOdk/edit\n",
        "Por instrucciones de cómo subir tu código a la VM.\n",
        "\n",
        "Luego instala los paquetes necesarios a tu máquina virtual\n",
        "`pip install -r gpu_requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA2Q22j_BHx3",
        "outputId": "5d2bb665-817c-4af9-d2f3-3755845b7889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -r gpu_requirements.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 2)) (0.6.2)\n",
            "Collecting tqdm==4.29.1\n",
            "  Downloading tqdm-4.29.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 4)) (0.1.96)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r gpu_requirements.txt (line 6)) (1.9.0+cu102)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->-r gpu_requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (1.19.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (2.3.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r gpu_requirements.txt (line 5)) (0.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r gpu_requirements.txt (line 6)) (3.7.4.3)\n",
            "Installing collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.29.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.29.1 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.29.1 which is incompatible.\u001b[0m\n",
            "Successfully installed tqdm-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjcXSWXkBKvx"
      },
      "source": [
        "Finalmente, ir a la sección de la guía y sigue las instrucciones para crear una nueva sesión `tmux`. Concretamente, ejecutar el siguiente comando para crear una sesión `tmux` llamada `nmt`\n",
        "\n",
        "`tmux new -s nmt`\n",
        "\n",
        "Una vez configurado tu VM y estás en una sesión `tmux`, ejecute:\n",
        "(linux) sh run.sh train\n",
        "(Windows) run.bat train\n",
        "\n",
        "Una vez sepas que tu código funciona correctamente, puedes desconectarte de sesión y cerrar tu conexión ssh al servidor.\n",
        "Para hacerlo ejecuta:\n",
        "`tmux detach`\n",
        "\n",
        "Puedes volver a tu modelo de entrenamiento reconectandote al servidor y añadiendo la sesión `tmux` con:\n",
        "`tmux a -t nmt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAIExZRYeh9_",
        "outputId": "9aaa15ab-37e6-4103-ea55-f293c33088be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sh run.sh train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "uniformly initialize parameters [-0.100000, +0.100000]\n",
            "use device: cuda:0\n",
            "begin Maximum Likelihood training\n",
            "epoch 1, iter 10, avg. loss 209.71, avg. ppl 3507.98 cum. examples 320, speed 1395.33 words/sec, time elapsed 5.89 sec\n",
            "epoch 1, iter 20, avg. loss 178.38, avg. ppl 780.96 cum. examples 640, speed 1372.37 words/sec, time elapsed 12.14 sec\n",
            "epoch 1, iter 30, avg. loss 165.85, avg. ppl 562.57 cum. examples 960, speed 1322.42 words/sec, time elapsed 18.47 sec\n",
            "epoch 1, iter 40, avg. loss 171.09, avg. ppl 484.10 cum. examples 1280, speed 1303.92 words/sec, time elapsed 25.27 sec\n",
            "epoch 1, iter 50, avg. loss 162.76, avg. ppl 379.93 cum. examples 1600, speed 1439.92 words/sec, time elapsed 31.36 sec\n",
            "epoch 1, iter 60, avg. loss 154.27, avg. ppl 354.72 cum. examples 1920, speed 1428.34 words/sec, time elapsed 37.24 sec\n",
            "epoch 1, iter 70, avg. loss 154.14, avg. ppl 309.02 cum. examples 2240, speed 1331.13 words/sec, time elapsed 43.71 sec\n",
            "epoch 1, iter 80, avg. loss 152.28, avg. ppl 314.73 cum. examples 2560, speed 1310.56 words/sec, time elapsed 50.17 sec\n",
            "epoch 1, iter 90, avg. loss 144.63, avg. ppl 272.41 cum. examples 2880, speed 1259.81 words/sec, time elapsed 56.72 sec\n",
            "epoch 1, iter 100, avg. loss 143.82, avg. ppl 231.47 cum. examples 3200, speed 1404.03 words/sec, time elapsed 62.74 sec\n",
            "epoch 1, iter 110, avg. loss 140.28, avg. ppl 228.60 cum. examples 3520, speed 1360.73 words/sec, time elapsed 68.82 sec\n",
            "epoch 1, iter 120, avg. loss 130.02, avg. ppl 197.55 cum. examples 3840, speed 1431.61 words/sec, time elapsed 74.31 sec\n",
            "epoch 1, iter 130, avg. loss 137.20, avg. ppl 187.31 cum. examples 4160, speed 1461.21 words/sec, time elapsed 80.06 sec\n",
            "epoch 1, iter 140, avg. loss 133.71, avg. ppl 186.96 cum. examples 4480, speed 1383.92 words/sec, time elapsed 85.97 sec\n",
            "epoch 1, iter 150, avg. loss 134.96, avg. ppl 173.71 cum. examples 4800, speed 1488.90 words/sec, time elapsed 91.59 sec\n",
            "epoch 1, iter 160, avg. loss 130.18, avg. ppl 162.08 cum. examples 5120, speed 1383.95 words/sec, time elapsed 97.51 sec\n",
            "epoch 1, iter 170, avg. loss 133.25, avg. ppl 169.44 cum. examples 5440, speed 1237.45 words/sec, time elapsed 104.22 sec\n",
            "epoch 1, iter 180, avg. loss 137.95, avg. ppl 150.94 cum. examples 5760, speed 1401.40 words/sec, time elapsed 110.50 sec\n",
            "epoch 1, iter 190, avg. loss 136.57, avg. ppl 163.52 cum. examples 6080, speed 1243.82 words/sec, time elapsed 117.39 sec\n",
            "epoch 1, iter 200, avg. loss 128.77, avg. ppl 141.45 cum. examples 6400, speed 1214.39 words/sec, time elapsed 124.24 sec\n",
            "epoch 1, iter 200, cum. loss 148.99, cum. ppl 289.24 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 200, dev. ppl 147.978482\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 1, iter 210, avg. loss 123.38, avg. ppl 150.62 cum. examples 320, speed 542.46 words/sec, time elapsed 138.76 sec\n",
            "epoch 1, iter 220, avg. loss 133.67, avg. ppl 140.15 cum. examples 640, speed 1398.25 words/sec, time elapsed 144.95 sec\n",
            "epoch 1, iter 230, avg. loss 122.70, avg. ppl 118.30 cum. examples 960, speed 1445.88 words/sec, time elapsed 150.64 sec\n",
            "epoch 1, iter 240, avg. loss 126.83, avg. ppl 116.52 cum. examples 1280, speed 1365.48 words/sec, time elapsed 156.88 sec\n",
            "epoch 1, iter 250, avg. loss 127.68, avg. ppl 117.95 cum. examples 1600, speed 1427.81 words/sec, time elapsed 162.88 sec\n",
            "epoch 1, iter 260, avg. loss 127.18, avg. ppl 104.49 cum. examples 1920, speed 1455.31 words/sec, time elapsed 168.90 sec\n",
            "epoch 1, iter 270, avg. loss 114.86, avg. ppl 103.99 cum. examples 2240, speed 1445.02 words/sec, time elapsed 174.37 sec\n",
            "epoch 1, iter 280, avg. loss 121.45, avg. ppl 106.64 cum. examples 2560, speed 1387.86 words/sec, time elapsed 180.37 sec\n",
            "epoch 1, iter 290, avg. loss 130.19, avg. ppl 125.85 cum. examples 2880, speed 1298.18 words/sec, time elapsed 187.01 sec\n",
            "epoch 1, iter 300, avg. loss 121.95, avg. ppl 105.60 cum. examples 3200, speed 1420.59 words/sec, time elapsed 192.90 sec\n",
            "epoch 1, iter 310, avg. loss 121.86, avg. ppl 97.77 cum. examples 3520, speed 1431.56 words/sec, time elapsed 198.85 sec\n",
            "epoch 1, iter 320, avg. loss 118.95, avg. ppl 94.09 cum. examples 3840, speed 1293.23 words/sec, time elapsed 205.32 sec\n",
            "epoch 1, iter 330, avg. loss 122.32, avg. ppl 101.63 cum. examples 4160, speed 1466.40 words/sec, time elapsed 211.10 sec\n",
            "epoch 1, iter 340, avg. loss 105.63, avg. ppl 87.45 cum. examples 4480, speed 1370.63 words/sec, time elapsed 216.62 sec\n",
            "epoch 1, iter 350, avg. loss 113.99, avg. ppl 90.42 cum. examples 4800, speed 1495.16 words/sec, time elapsed 222.03 sec\n",
            "epoch 1, iter 360, avg. loss 113.92, avg. ppl 87.24 cum. examples 5120, speed 1267.48 words/sec, time elapsed 228.47 sec\n",
            "epoch 1, iter 370, avg. loss 113.74, avg. ppl 89.24 cum. examples 5440, speed 1360.99 words/sec, time elapsed 234.42 sec\n",
            "epoch 1, iter 380, avg. loss 115.28, avg. ppl 96.49 cum. examples 5760, speed 1351.43 words/sec, time elapsed 240.40 sec\n",
            "epoch 1, iter 390, avg. loss 117.49, avg. ppl 81.43 cum. examples 6080, speed 1303.02 words/sec, time elapsed 246.96 sec\n",
            "epoch 1, iter 400, avg. loss 119.69, avg. ppl 84.76 cum. examples 6400, speed 1333.53 words/sec, time elapsed 253.42 sec\n",
            "epoch 1, iter 400, cum. loss 120.64, cum. ppl 103.68 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 400, dev. ppl 91.201153\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 1, iter 410, avg. loss 110.28, avg. ppl 72.58 cum. examples 320, speed 582.83 words/sec, time elapsed 267.56 sec\n",
            "epoch 1, iter 420, avg. loss 113.02, avg. ppl 70.45 cum. examples 640, speed 1502.12 words/sec, time elapsed 273.21 sec\n",
            "epoch 1, iter 430, avg. loss 115.41, avg. ppl 87.74 cum. examples 960, speed 1433.40 words/sec, time elapsed 278.97 sec\n",
            "epoch 1, iter 440, avg. loss 113.22, avg. ppl 76.96 cum. examples 1280, speed 1289.35 words/sec, time elapsed 285.44 sec\n",
            "epoch 1, iter 450, avg. loss 107.89, avg. ppl 72.23 cum. examples 1600, speed 1249.18 words/sec, time elapsed 291.90 sec\n",
            "epoch 1, iter 460, avg. loss 115.32, avg. ppl 81.82 cum. examples 1920, speed 1286.88 words/sec, time elapsed 298.41 sec\n",
            "epoch 2, iter 470, avg. loss 111.00, avg. ppl 62.50 cum. examples 2215, speed 1313.95 words/sec, time elapsed 304.44 sec\n",
            "epoch 2, iter 480, avg. loss 105.39, avg. ppl 53.43 cum. examples 2535, speed 1394.27 words/sec, time elapsed 310.52 sec\n",
            "epoch 2, iter 490, avg. loss 103.87, avg. ppl 55.93 cum. examples 2855, speed 1310.66 words/sec, time elapsed 316.82 sec\n",
            "epoch 2, iter 500, avg. loss 100.32, avg. ppl 53.41 cum. examples 3175, speed 1267.06 words/sec, time elapsed 323.19 sec\n",
            "epoch 2, iter 510, avg. loss 102.49, avg. ppl 51.93 cum. examples 3495, speed 1235.22 words/sec, time elapsed 329.91 sec\n",
            "epoch 2, iter 520, avg. loss 99.96, avg. ppl 53.71 cum. examples 3815, speed 1365.67 words/sec, time elapsed 335.79 sec\n",
            "epoch 2, iter 530, avg. loss 104.24, avg. ppl 48.31 cum. examples 4135, speed 1289.18 words/sec, time elapsed 342.46 sec\n",
            "epoch 2, iter 540, avg. loss 103.05, avg. ppl 50.51 cum. examples 4455, speed 1290.54 words/sec, time elapsed 348.98 sec\n",
            "epoch 2, iter 550, avg. loss 101.76, avg. ppl 47.74 cum. examples 4775, speed 1308.28 words/sec, time elapsed 355.42 sec\n",
            "epoch 2, iter 560, avg. loss 104.52, avg. ppl 50.73 cum. examples 5095, speed 1427.01 words/sec, time elapsed 361.39 sec\n",
            "epoch 2, iter 570, avg. loss 105.17, avg. ppl 56.71 cum. examples 5415, speed 1268.47 words/sec, time elapsed 367.96 sec\n",
            "epoch 2, iter 580, avg. loss 100.51, avg. ppl 51.89 cum. examples 5735, speed 1365.41 words/sec, time elapsed 373.92 sec\n",
            "epoch 2, iter 590, avg. loss 106.02, avg. ppl 44.72 cum. examples 6055, speed 1368.31 words/sec, time elapsed 380.45 sec\n",
            "epoch 2, iter 600, avg. loss 104.90, avg. ppl 48.55 cum. examples 6375, speed 1425.55 words/sec, time elapsed 386.51 sec\n",
            "epoch 2, iter 600, cum. loss 106.40, cum. ppl 58.30 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 600, dev. ppl 68.082800\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 2, iter 610, avg. loss 104.61, avg. ppl 51.44 cum. examples 320, speed 601.07 words/sec, time elapsed 400.64 sec\n",
            "epoch 2, iter 620, avg. loss 104.53, avg. ppl 45.61 cum. examples 640, speed 1450.58 words/sec, time elapsed 406.68 sec\n",
            "epoch 2, iter 630, avg. loss 92.15, avg. ppl 42.16 cum. examples 960, speed 1345.21 words/sec, time elapsed 412.54 sec\n",
            "epoch 2, iter 640, avg. loss 112.55, avg. ppl 53.95 cum. examples 1280, speed 1304.09 words/sec, time elapsed 419.46 sec\n",
            "epoch 2, iter 650, avg. loss 95.66, avg. ppl 47.43 cum. examples 1600, speed 1271.40 words/sec, time elapsed 425.70 sec\n",
            "epoch 2, iter 660, avg. loss 97.82, avg. ppl 46.25 cum. examples 1920, speed 1200.63 words/sec, time elapsed 432.50 sec\n",
            "epoch 2, iter 670, avg. loss 94.51, avg. ppl 45.30 cum. examples 2240, speed 1444.35 words/sec, time elapsed 437.99 sec\n",
            "epoch 2, iter 680, avg. loss 101.90, avg. ppl 47.56 cum. examples 2560, speed 1368.02 words/sec, time elapsed 444.17 sec\n",
            "epoch 2, iter 690, avg. loss 104.10, avg. ppl 49.95 cum. examples 2880, speed 1394.94 words/sec, time elapsed 450.27 sec\n",
            "epoch 2, iter 700, avg. loss 100.21, avg. ppl 40.68 cum. examples 3200, speed 1308.04 words/sec, time elapsed 456.89 sec\n",
            "epoch 2, iter 710, avg. loss 98.17, avg. ppl 42.09 cum. examples 3520, speed 1428.66 words/sec, time elapsed 462.77 sec\n",
            "epoch 2, iter 720, avg. loss 100.85, avg. ppl 46.17 cum. examples 3840, speed 1276.25 words/sec, time elapsed 469.37 sec\n",
            "epoch 2, iter 730, avg. loss 101.40, avg. ppl 47.35 cum. examples 4160, speed 1396.20 words/sec, time elapsed 475.39 sec\n",
            "epoch 2, iter 740, avg. loss 100.62, avg. ppl 43.61 cum. examples 4480, speed 1438.23 words/sec, time elapsed 481.32 sec\n",
            "epoch 2, iter 750, avg. loss 93.66, avg. ppl 38.35 cum. examples 4800, speed 1441.79 words/sec, time elapsed 487.02 sec\n",
            "epoch 2, iter 760, avg. loss 96.38, avg. ppl 39.73 cum. examples 5120, speed 1422.46 words/sec, time elapsed 492.91 sec\n",
            "epoch 2, iter 770, avg. loss 97.00, avg. ppl 41.65 cum. examples 5440, speed 1326.90 words/sec, time elapsed 499.18 sec\n",
            "epoch 2, iter 780, avg. loss 95.52, avg. ppl 39.18 cum. examples 5760, speed 1378.18 words/sec, time elapsed 505.23 sec\n",
            "epoch 2, iter 790, avg. loss 95.82, avg. ppl 42.09 cum. examples 6080, speed 1355.53 words/sec, time elapsed 511.28 sec\n",
            "epoch 2, iter 800, avg. loss 94.85, avg. ppl 40.73 cum. examples 6400, speed 1417.54 words/sec, time elapsed 517.05 sec\n",
            "epoch 2, iter 800, cum. loss 99.11, cum. ppl 44.42 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 800, dev. ppl 56.004044\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 2, iter 810, avg. loss 100.30, avg. ppl 43.09 cum. examples 320, speed 590.37 words/sec, time elapsed 531.50 sec\n",
            "epoch 2, iter 820, avg. loss 97.44, avg. ppl 38.98 cum. examples 640, speed 1450.80 words/sec, time elapsed 537.37 sec\n",
            "epoch 2, iter 830, avg. loss 94.16, avg. ppl 41.23 cum. examples 960, speed 1451.11 words/sec, time elapsed 542.95 sec\n",
            "epoch 2, iter 840, avg. loss 97.74, avg. ppl 40.20 cum. examples 1280, speed 1394.34 words/sec, time elapsed 549.02 sec\n",
            "epoch 2, iter 850, avg. loss 93.58, avg. ppl 38.64 cum. examples 1600, speed 1274.16 words/sec, time elapsed 555.45 sec\n",
            "epoch 2, iter 860, avg. loss 94.37, avg. ppl 39.65 cum. examples 1920, speed 1362.35 words/sec, time elapsed 561.48 sec\n",
            "epoch 2, iter 870, avg. loss 95.96, avg. ppl 40.34 cum. examples 2240, speed 1297.14 words/sec, time elapsed 567.88 sec\n",
            "epoch 2, iter 880, avg. loss 91.85, avg. ppl 37.34 cum. examples 2560, speed 1400.46 words/sec, time elapsed 573.68 sec\n",
            "epoch 2, iter 890, avg. loss 93.00, avg. ppl 38.24 cum. examples 2880, speed 1329.83 words/sec, time elapsed 579.82 sec\n",
            "epoch 2, iter 900, avg. loss 88.89, avg. ppl 34.25 cum. examples 3200, speed 1382.77 words/sec, time elapsed 585.64 sec\n",
            "epoch 2, iter 910, avg. loss 95.69, avg. ppl 38.55 cum. examples 3520, speed 1223.79 words/sec, time elapsed 592.49 sec\n",
            "epoch 2, iter 920, avg. loss 97.40, avg. ppl 38.89 cum. examples 3840, speed 1504.71 words/sec, time elapsed 598.15 sec\n",
            "epoch 2, iter 930, avg. loss 92.80, avg. ppl 34.51 cum. examples 4135, speed 1378.37 words/sec, time elapsed 603.76 sec\n",
            "epoch 3, iter 940, avg. loss 82.50, avg. ppl 24.28 cum. examples 4455, speed 1363.74 words/sec, time elapsed 609.83 sec\n",
            "epoch 3, iter 950, avg. loss 84.57, avg. ppl 24.80 cum. examples 4775, speed 1423.52 words/sec, time elapsed 615.75 sec\n",
            "epoch 3, iter 960, avg. loss 83.55, avg. ppl 23.08 cum. examples 5095, speed 1383.11 words/sec, time elapsed 621.91 sec\n",
            "epoch 3, iter 970, avg. loss 79.61, avg. ppl 23.78 cum. examples 5415, speed 1429.45 words/sec, time elapsed 627.53 sec\n",
            "epoch 3, iter 980, avg. loss 86.67, avg. ppl 22.78 cum. examples 5735, speed 1394.31 words/sec, time elapsed 633.90 sec\n",
            "epoch 3, iter 990, avg. loss 83.29, avg. ppl 23.40 cum. examples 6055, speed 1360.64 words/sec, time elapsed 640.11 sec\n",
            "epoch 3, iter 1000, avg. loss 82.37, avg. ppl 23.76 cum. examples 6375, speed 1494.03 words/sec, time elapsed 645.68 sec\n",
            "epoch 3, iter 1000, cum. loss 90.78, cum. ppl 32.53 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 1000, dev. ppl 48.565627\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 3, iter 1010, avg. loss 81.85, avg. ppl 21.18 cum. examples 320, speed 614.38 words/sec, time elapsed 659.64 sec\n",
            "epoch 3, iter 1020, avg. loss 84.85, avg. ppl 24.89 cum. examples 640, speed 1233.27 words/sec, time elapsed 666.49 sec\n",
            "epoch 3, iter 1030, avg. loss 77.06, avg. ppl 20.96 cum. examples 960, speed 1352.47 words/sec, time elapsed 672.48 sec\n",
            "epoch 3, iter 1040, avg. loss 81.58, avg. ppl 24.37 cum. examples 1280, speed 1365.38 words/sec, time elapsed 678.47 sec\n",
            "epoch 3, iter 1050, avg. loss 84.34, avg. ppl 24.71 cum. examples 1600, speed 1298.58 words/sec, time elapsed 684.95 sec\n",
            "epoch 3, iter 1060, avg. loss 81.58, avg. ppl 23.30 cum. examples 1920, speed 1398.71 words/sec, time elapsed 690.88 sec\n",
            "epoch 3, iter 1070, avg. loss 84.19, avg. ppl 21.92 cum. examples 2240, speed 1406.47 words/sec, time elapsed 697.08 sec\n",
            "epoch 3, iter 1080, avg. loss 80.20, avg. ppl 21.85 cum. examples 2560, speed 1317.30 words/sec, time elapsed 703.40 sec\n",
            "epoch 3, iter 1090, avg. loss 76.49, avg. ppl 20.54 cum. examples 2880, speed 1439.39 words/sec, time elapsed 709.03 sec\n",
            "epoch 3, iter 1100, avg. loss 85.84, avg. ppl 24.18 cum. examples 3200, speed 1330.21 words/sec, time elapsed 715.51 sec\n",
            "epoch 3, iter 1110, avg. loss 83.71, avg. ppl 23.60 cum. examples 3520, speed 1093.03 words/sec, time elapsed 723.26 sec\n",
            "epoch 3, iter 1120, avg. loss 83.34, avg. ppl 25.01 cum. examples 3840, speed 1326.07 words/sec, time elapsed 729.51 sec\n",
            "epoch 3, iter 1130, avg. loss 74.33, avg. ppl 18.68 cum. examples 4160, speed 1309.30 words/sec, time elapsed 735.72 sec\n",
            "epoch 3, iter 1140, avg. loss 80.05, avg. ppl 21.79 cum. examples 4480, speed 1367.05 words/sec, time elapsed 741.80 sec\n",
            "epoch 3, iter 1150, avg. loss 84.60, avg. ppl 23.05 cum. examples 4800, speed 1414.27 words/sec, time elapsed 747.90 sec\n",
            "epoch 3, iter 1160, avg. loss 82.87, avg. ppl 21.67 cum. examples 5120, speed 1387.30 words/sec, time elapsed 754.11 sec\n",
            "epoch 3, iter 1170, avg. loss 76.99, avg. ppl 20.93 cum. examples 5440, speed 1367.81 words/sec, time elapsed 760.03 sec\n",
            "epoch 3, iter 1180, avg. loss 81.27, avg. ppl 22.21 cum. examples 5760, speed 1364.36 words/sec, time elapsed 766.18 sec\n",
            "epoch 3, iter 1190, avg. loss 80.83, avg. ppl 23.22 cum. examples 6080, speed 1452.60 words/sec, time elapsed 771.84 sec\n",
            "epoch 3, iter 1200, avg. loss 76.12, avg. ppl 20.59 cum. examples 6400, speed 1435.84 words/sec, time elapsed 777.45 sec\n",
            "epoch 3, iter 1200, cum. loss 81.10, cum. ppl 22.38 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 1200, dev. ppl 44.559486\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 3, iter 1210, avg. loss 76.43, avg. ppl 21.05 cum. examples 320, speed 582.76 words/sec, time elapsed 791.23 sec\n",
            "epoch 3, iter 1220, avg. loss 79.39, avg. ppl 21.61 cum. examples 640, speed 1465.38 words/sec, time elapsed 796.87 sec\n",
            "epoch 3, iter 1230, avg. loss 84.56, avg. ppl 23.82 cum. examples 960, speed 1367.73 words/sec, time elapsed 803.11 sec\n",
            "epoch 3, iter 1240, avg. loss 81.67, avg. ppl 22.59 cum. examples 1280, speed 1408.37 words/sec, time elapsed 809.06 sec\n",
            "epoch 3, iter 1250, avg. loss 79.61, avg. ppl 18.16 cum. examples 1600, speed 1458.15 words/sec, time elapsed 815.09 sec\n",
            "epoch 3, iter 1260, avg. loss 82.29, avg. ppl 22.12 cum. examples 1920, speed 1340.95 words/sec, time elapsed 821.43 sec\n",
            "epoch 3, iter 1270, avg. loss 86.36, avg. ppl 23.26 cum. examples 2240, speed 1188.15 words/sec, time elapsed 828.82 sec\n",
            "epoch 3, iter 1280, avg. loss 75.92, avg. ppl 19.10 cum. examples 2560, speed 1380.89 words/sec, time elapsed 834.78 sec\n",
            "epoch 3, iter 1290, avg. loss 76.40, avg. ppl 21.09 cum. examples 2880, speed 1411.99 words/sec, time elapsed 840.46 sec\n",
            "epoch 3, iter 1300, avg. loss 74.42, avg. ppl 20.61 cum. examples 3200, speed 1310.27 words/sec, time elapsed 846.47 sec\n",
            "epoch 3, iter 1310, avg. loss 85.65, avg. ppl 23.57 cum. examples 3520, speed 1370.59 words/sec, time elapsed 852.80 sec\n",
            "epoch 3, iter 1320, avg. loss 81.01, avg. ppl 22.18 cum. examples 3840, speed 1318.45 words/sec, time elapsed 859.14 sec\n",
            "epoch 3, iter 1330, avg. loss 77.58, avg. ppl 20.73 cum. examples 4160, speed 1408.71 words/sec, time elapsed 864.95 sec\n",
            "epoch 3, iter 1340, avg. loss 80.65, avg. ppl 22.16 cum. examples 4480, speed 1321.57 words/sec, time elapsed 871.26 sec\n",
            "epoch 3, iter 1350, avg. loss 78.63, avg. ppl 20.29 cum. examples 4800, speed 1306.00 words/sec, time elapsed 877.66 sec\n",
            "epoch 3, iter 1360, avg. loss 81.59, avg. ppl 21.80 cum. examples 5120, speed 1288.22 words/sec, time elapsed 884.23 sec\n",
            "epoch 3, iter 1370, avg. loss 72.56, avg. ppl 18.62 cum. examples 5440, speed 1459.88 words/sec, time elapsed 889.67 sec\n",
            "epoch 3, iter 1380, avg. loss 85.60, avg. ppl 23.74 cum. examples 5760, speed 1177.42 words/sec, time elapsed 897.02 sec\n",
            "epoch 3, iter 1390, avg. loss 79.62, avg. ppl 22.73 cum. examples 6080, speed 1403.36 words/sec, time elapsed 902.83 sec\n",
            "epoch 4, iter 1400, avg. loss 73.11, avg. ppl 14.97 cum. examples 6375, speed 1264.93 words/sec, time elapsed 909.13 sec\n",
            "epoch 4, iter 1400, cum. loss 79.68, cum. ppl 21.13 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 1400, dev. ppl 40.153157\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 4, iter 1410, avg. loss 61.90, avg. ppl 11.37 cum. examples 320, speed 584.74 words/sec, time elapsed 923.07 sec\n",
            "epoch 4, iter 1420, avg. loss 60.40, avg. ppl 10.44 cum. examples 640, speed 1379.43 words/sec, time elapsed 929.04 sec\n",
            "epoch 4, iter 1430, avg. loss 68.05, avg. ppl 12.82 cum. examples 960, speed 1282.95 words/sec, time elapsed 935.70 sec\n",
            "epoch 4, iter 1440, avg. loss 70.63, avg. ppl 13.26 cum. examples 1280, speed 1310.13 words/sec, time elapsed 942.37 sec\n",
            "epoch 4, iter 1450, avg. loss 65.20, avg. ppl 12.02 cum. examples 1600, speed 1325.84 words/sec, time elapsed 948.70 sec\n",
            "epoch 4, iter 1460, avg. loss 67.06, avg. ppl 14.56 cum. examples 1920, speed 1341.80 words/sec, time elapsed 954.67 sec\n",
            "epoch 4, iter 1470, avg. loss 66.99, avg. ppl 12.82 cum. examples 2240, speed 1407.70 words/sec, time elapsed 960.64 sec\n",
            "epoch 4, iter 1480, avg. loss 63.55, avg. ppl 11.86 cum. examples 2560, speed 1349.06 words/sec, time elapsed 966.74 sec\n",
            "epoch 4, iter 1490, avg. loss 63.71, avg. ppl 12.04 cum. examples 2880, speed 1363.66 words/sec, time elapsed 972.74 sec\n",
            "epoch 4, iter 1500, avg. loss 68.16, avg. ppl 12.91 cum. examples 3200, speed 1237.16 words/sec, time elapsed 979.64 sec\n",
            "epoch 4, iter 1510, avg. loss 62.62, avg. ppl 11.97 cum. examples 3520, speed 1352.93 words/sec, time elapsed 985.61 sec\n",
            "epoch 4, iter 1520, avg. loss 68.14, avg. ppl 13.23 cum. examples 3840, speed 1348.30 words/sec, time elapsed 991.87 sec\n",
            "epoch 4, iter 1530, avg. loss 64.39, avg. ppl 12.12 cum. examples 4160, speed 1322.98 words/sec, time elapsed 998.11 sec\n",
            "epoch 4, iter 1540, avg. loss 66.73, avg. ppl 13.29 cum. examples 4480, speed 1360.67 words/sec, time elapsed 1004.18 sec\n",
            "epoch 4, iter 1550, avg. loss 69.73, avg. ppl 14.14 cum. examples 4800, speed 1170.40 words/sec, time elapsed 1011.38 sec\n",
            "epoch 4, iter 1560, avg. loss 60.02, avg. ppl 10.97 cum. examples 5120, speed 1388.98 words/sec, time elapsed 1017.15 sec\n",
            "epoch 4, iter 1570, avg. loss 65.69, avg. ppl 12.85 cum. examples 5440, speed 1431.84 words/sec, time elapsed 1022.90 sec\n",
            "epoch 4, iter 1580, avg. loss 62.64, avg. ppl 12.47 cum. examples 5760, speed 1260.68 words/sec, time elapsed 1029.20 sec\n",
            "epoch 4, iter 1590, avg. loss 65.14, avg. ppl 12.12 cum. examples 6080, speed 1344.85 words/sec, time elapsed 1035.41 sec\n",
            "epoch 4, iter 1600, avg. loss 66.38, avg. ppl 12.33 cum. examples 6400, speed 1377.43 words/sec, time elapsed 1041.55 sec\n",
            "epoch 4, iter 1600, cum. loss 65.36, cum. ppl 12.45 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 1600, dev. ppl 41.130097\n",
            "hit patience 1\n",
            "hit #1 trial\n",
            "load previously best model and decay learning rate to 0.000250\n",
            "restore parameters of the optimizers\n",
            "epoch 4, iter 1610, avg. loss 67.38, avg. ppl 12.97 cum. examples 320, speed 720.75 words/sec, time elapsed 1053.22 sec\n",
            "epoch 4, iter 1620, avg. loss 65.99, avg. ppl 11.68 cum. examples 640, speed 1438.31 words/sec, time elapsed 1059.20 sec\n",
            "epoch 4, iter 1630, avg. loss 64.32, avg. ppl 12.16 cum. examples 960, speed 1335.28 words/sec, time elapsed 1065.37 sec\n",
            "epoch 4, iter 1640, avg. loss 61.74, avg. ppl 11.22 cum. examples 1280, speed 1377.74 words/sec, time elapsed 1071.30 sec\n",
            "epoch 4, iter 1650, avg. loss 64.22, avg. ppl 11.35 cum. examples 1600, speed 1343.61 words/sec, time elapsed 1077.59 sec\n",
            "epoch 4, iter 1660, avg. loss 63.56, avg. ppl 11.10 cum. examples 1920, speed 1384.09 words/sec, time elapsed 1083.70 sec\n",
            "epoch 4, iter 1670, avg. loss 63.87, avg. ppl 10.51 cum. examples 2240, speed 1415.00 words/sec, time elapsed 1089.84 sec\n",
            "epoch 4, iter 1680, avg. loss 68.46, avg. ppl 12.13 cum. examples 2560, speed 1277.06 words/sec, time elapsed 1096.71 sec\n",
            "epoch 4, iter 1690, avg. loss 61.49, avg. ppl 10.74 cum. examples 2880, speed 1399.10 words/sec, time elapsed 1102.64 sec\n",
            "epoch 4, iter 1700, avg. loss 63.15, avg. ppl 11.63 cum. examples 3200, speed 1300.24 words/sec, time elapsed 1108.97 sec\n",
            "epoch 4, iter 1710, avg. loss 63.25, avg. ppl 10.63 cum. examples 3520, speed 1471.75 words/sec, time elapsed 1114.79 sec\n",
            "epoch 4, iter 1720, avg. loss 62.59, avg. ppl 11.23 cum. examples 3840, speed 1416.46 words/sec, time elapsed 1120.64 sec\n",
            "epoch 4, iter 1730, avg. loss 62.03, avg. ppl 11.06 cum. examples 4160, speed 1307.93 words/sec, time elapsed 1126.95 sec\n",
            "epoch 4, iter 1740, avg. loss 61.27, avg. ppl 10.34 cum. examples 4480, speed 1445.65 words/sec, time elapsed 1132.76 sec\n",
            "epoch 4, iter 1750, avg. loss 59.99, avg. ppl 10.50 cum. examples 4800, speed 1388.23 words/sec, time elapsed 1138.64 sec\n",
            "epoch 4, iter 1760, avg. loss 66.49, avg. ppl 12.69 cum. examples 5120, speed 1399.60 words/sec, time elapsed 1144.62 sec\n",
            "epoch 4, iter 1770, avg. loss 61.47, avg. ppl 11.52 cum. examples 5440, speed 1390.11 words/sec, time elapsed 1150.41 sec\n",
            "epoch 4, iter 1780, avg. loss 62.81, avg. ppl 10.67 cum. examples 5760, speed 1369.70 words/sec, time elapsed 1156.61 sec\n",
            "epoch 4, iter 1790, avg. loss 64.22, avg. ppl 11.60 cum. examples 6080, speed 1313.44 words/sec, time elapsed 1162.99 sec\n",
            "epoch 4, iter 1800, avg. loss 63.76, avg. ppl 11.17 cum. examples 6400, speed 1417.99 words/sec, time elapsed 1168.96 sec\n",
            "epoch 4, iter 1800, cum. loss 63.60, cum. ppl 11.32 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 1800, dev. ppl 39.243210\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 4, iter 1810, avg. loss 59.79, avg. ppl 9.98 cum. examples 320, speed 576.32 words/sec, time elapsed 1183.39 sec\n",
            "epoch 4, iter 1820, avg. loss 62.07, avg. ppl 11.02 cum. examples 640, speed 1351.81 words/sec, time elapsed 1189.51 sec\n",
            "epoch 4, iter 1830, avg. loss 64.69, avg. ppl 11.14 cum. examples 960, speed 1393.91 words/sec, time elapsed 1195.67 sec\n",
            "epoch 4, iter 1840, avg. loss 62.68, avg. ppl 10.96 cum. examples 1280, speed 1328.82 words/sec, time elapsed 1201.98 sec\n",
            "epoch 4, iter 1850, avg. loss 64.84, avg. ppl 11.55 cum. examples 1600, speed 1324.15 words/sec, time elapsed 1208.38 sec\n",
            "epoch 4, iter 1860, avg. loss 64.26, avg. ppl 11.17 cum. examples 1895, speed 1363.52 words/sec, time elapsed 1214.15 sec\n",
            "epoch 5, iter 1870, avg. loss 59.22, avg. ppl 9.04 cum. examples 2215, speed 1391.00 words/sec, time elapsed 1220.33 sec\n",
            "epoch 5, iter 1880, avg. loss 55.19, avg. ppl 8.25 cum. examples 2535, speed 1375.85 words/sec, time elapsed 1226.42 sec\n",
            "epoch 5, iter 1890, avg. loss 55.95, avg. ppl 8.24 cum. examples 2855, speed 1346.58 words/sec, time elapsed 1232.72 sec\n",
            "epoch 5, iter 1900, avg. loss 56.28, avg. ppl 8.39 cum. examples 3175, speed 1477.32 words/sec, time elapsed 1238.46 sec\n",
            "epoch 5, iter 1910, avg. loss 53.22, avg. ppl 8.07 cum. examples 3495, speed 1394.36 words/sec, time elapsed 1244.31 sec\n",
            "epoch 5, iter 1920, avg. loss 60.55, avg. ppl 9.57 cum. examples 3815, speed 1271.48 words/sec, time elapsed 1251.05 sec\n",
            "epoch 5, iter 1930, avg. loss 51.89, avg. ppl 7.84 cum. examples 4135, speed 1457.57 words/sec, time elapsed 1256.58 sec\n",
            "epoch 5, iter 1940, avg. loss 55.50, avg. ppl 8.43 cum. examples 4455, speed 1199.66 words/sec, time elapsed 1263.53 sec\n",
            "epoch 5, iter 1950, avg. loss 58.17, avg. ppl 8.77 cum. examples 4775, speed 1465.98 words/sec, time elapsed 1269.38 sec\n",
            "epoch 5, iter 1960, avg. loss 56.01, avg. ppl 8.62 cum. examples 5095, speed 1351.57 words/sec, time elapsed 1275.53 sec\n",
            "epoch 5, iter 1970, avg. loss 58.86, avg. ppl 8.63 cum. examples 5415, speed 1503.65 words/sec, time elapsed 1281.35 sec\n",
            "epoch 5, iter 1980, avg. loss 56.38, avg. ppl 9.00 cum. examples 5735, speed 1293.58 words/sec, time elapsed 1287.69 sec\n",
            "epoch 5, iter 1990, avg. loss 59.81, avg. ppl 9.28 cum. examples 6055, speed 1291.15 words/sec, time elapsed 1294.35 sec\n",
            "epoch 5, iter 2000, avg. loss 56.25, avg. ppl 8.72 cum. examples 6375, speed 1338.80 words/sec, time elapsed 1300.56 sec\n",
            "epoch 5, iter 2000, cum. loss 58.56, cum. ppl 9.26 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 2000, dev. ppl 40.205172\n",
            "hit patience 1\n",
            "hit #2 trial\n",
            "load previously best model and decay learning rate to 0.000125\n",
            "restore parameters of the optimizers\n",
            "epoch 5, iter 2010, avg. loss 54.40, avg. ppl 8.90 cum. examples 320, speed 742.10 words/sec, time elapsed 1311.29 sec\n",
            "epoch 5, iter 2020, avg. loss 62.72, avg. ppl 10.99 cum. examples 640, speed 1139.23 words/sec, time elapsed 1318.63 sec\n",
            "epoch 5, iter 2030, avg. loss 58.50, avg. ppl 9.71 cum. examples 960, speed 1186.16 words/sec, time elapsed 1325.58 sec\n",
            "epoch 5, iter 2040, avg. loss 57.91, avg. ppl 9.40 cum. examples 1280, speed 1348.91 words/sec, time elapsed 1331.71 sec\n",
            "epoch 5, iter 2050, avg. loss 54.43, avg. ppl 8.27 cum. examples 1600, speed 1456.80 words/sec, time elapsed 1337.37 sec\n",
            "epoch 5, iter 2060, avg. loss 55.69, avg. ppl 8.67 cum. examples 1920, speed 1416.34 words/sec, time elapsed 1343.19 sec\n",
            "epoch 5, iter 2070, avg. loss 53.91, avg. ppl 8.46 cum. examples 2240, speed 1359.62 words/sec, time elapsed 1349.14 sec\n",
            "epoch 5, iter 2080, avg. loss 57.72, avg. ppl 9.17 cum. examples 2560, speed 1372.15 words/sec, time elapsed 1355.21 sec\n",
            "epoch 5, iter 2090, avg. loss 58.41, avg. ppl 8.92 cum. examples 2880, speed 1419.48 words/sec, time elapsed 1361.23 sec\n",
            "epoch 5, iter 2100, avg. loss 56.79, avg. ppl 8.81 cum. examples 3200, speed 1312.43 words/sec, time elapsed 1367.59 sec\n",
            "epoch 5, iter 2110, avg. loss 56.87, avg. ppl 8.58 cum. examples 3520, speed 1402.24 words/sec, time elapsed 1373.63 sec\n",
            "epoch 5, iter 2120, avg. loss 57.32, avg. ppl 9.49 cum. examples 3840, speed 1247.92 words/sec, time elapsed 1380.16 sec\n",
            "epoch 5, iter 2130, avg. loss 56.87, avg. ppl 9.56 cum. examples 4160, speed 1339.65 words/sec, time elapsed 1386.18 sec\n",
            "epoch 5, iter 2140, avg. loss 56.71, avg. ppl 9.37 cum. examples 4480, speed 1224.72 words/sec, time elapsed 1392.80 sec\n",
            "epoch 5, iter 2150, avg. loss 55.36, avg. ppl 8.21 cum. examples 4800, speed 1424.34 words/sec, time elapsed 1398.71 sec\n",
            "epoch 5, iter 2160, avg. loss 55.70, avg. ppl 8.53 cum. examples 5120, speed 1333.93 words/sec, time elapsed 1404.94 sec\n",
            "epoch 5, iter 2170, avg. loss 61.64, avg. ppl 9.42 cum. examples 5440, speed 1050.51 words/sec, time elapsed 1413.31 sec\n",
            "epoch 5, iter 2180, avg. loss 54.81, avg. ppl 7.92 cum. examples 5760, speed 1396.98 words/sec, time elapsed 1419.38 sec\n",
            "epoch 5, iter 2190, avg. loss 56.51, avg. ppl 8.76 cum. examples 6080, speed 1311.19 words/sec, time elapsed 1425.74 sec\n",
            "epoch 5, iter 2200, avg. loss 63.29, avg. ppl 10.08 cum. examples 6400, speed 1384.95 words/sec, time elapsed 1432.07 sec\n",
            "epoch 5, iter 2200, cum. loss 57.28, cum. ppl 9.04 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 2200, dev. ppl 39.281032\n",
            "hit patience 1\n",
            "hit #3 trial\n",
            "load previously best model and decay learning rate to 0.000063\n",
            "restore parameters of the optimizers\n",
            "epoch 5, iter 2210, avg. loss 58.01, avg. ppl 9.59 cum. examples 320, speed 733.33 words/sec, time elapsed 1443.27 sec\n",
            "epoch 5, iter 2220, avg. loss 59.87, avg. ppl 9.77 cum. examples 640, speed 1369.13 words/sec, time elapsed 1449.41 sec\n",
            "epoch 5, iter 2230, avg. loss 53.67, avg. ppl 8.17 cum. examples 960, speed 1373.93 words/sec, time elapsed 1455.36 sec\n",
            "epoch 5, iter 2240, avg. loss 55.69, avg. ppl 8.49 cum. examples 1280, speed 1498.83 words/sec, time elapsed 1460.92 sec\n",
            "epoch 5, iter 2250, avg. loss 57.57, avg. ppl 8.83 cum. examples 1600, speed 1401.97 words/sec, time elapsed 1466.95 sec\n",
            "epoch 5, iter 2260, avg. loss 56.47, avg. ppl 9.25 cum. examples 1920, speed 1327.09 words/sec, time elapsed 1473.07 sec\n",
            "epoch 5, iter 2270, avg. loss 58.27, avg. ppl 8.90 cum. examples 2240, speed 1387.44 words/sec, time elapsed 1479.22 sec\n",
            "epoch 5, iter 2280, avg. loss 59.57, avg. ppl 9.00 cum. examples 2560, speed 1444.87 words/sec, time elapsed 1485.22 sec\n",
            "epoch 5, iter 2290, avg. loss 57.40, avg. ppl 8.54 cum. examples 2880, speed 1412.60 words/sec, time elapsed 1491.28 sec\n",
            "epoch 5, iter 2300, avg. loss 58.51, avg. ppl 9.19 cum. examples 3200, speed 1358.29 words/sec, time elapsed 1497.50 sec\n",
            "epoch 5, iter 2310, avg. loss 55.64, avg. ppl 8.59 cum. examples 3520, speed 1411.99 words/sec, time elapsed 1503.36 sec\n",
            "epoch 5, iter 2320, avg. loss 53.26, avg. ppl 8.30 cum. examples 3840, speed 1381.53 words/sec, time elapsed 1509.19 sec\n",
            "epoch 6, iter 2330, avg. loss 53.60, avg. ppl 8.50 cum. examples 4135, speed 1348.44 words/sec, time elapsed 1514.67 sec\n",
            "epoch 6, iter 2340, avg. loss 54.38, avg. ppl 8.52 cum. examples 4455, speed 1367.67 words/sec, time elapsed 1520.61 sec\n",
            "epoch 6, iter 2350, avg. loss 58.71, avg. ppl 8.84 cum. examples 4775, speed 1307.12 words/sec, time elapsed 1527.20 sec\n",
            "epoch 6, iter 2360, avg. loss 55.64, avg. ppl 8.60 cum. examples 5095, speed 1278.49 words/sec, time elapsed 1533.68 sec\n",
            "epoch 6, iter 2370, avg. loss 51.63, avg. ppl 8.14 cum. examples 5415, speed 1421.67 words/sec, time elapsed 1539.22 sec\n",
            "epoch 6, iter 2380, avg. loss 51.00, avg. ppl 7.59 cum. examples 5735, speed 1462.80 words/sec, time elapsed 1544.72 sec\n",
            "epoch 6, iter 2390, avg. loss 52.26, avg. ppl 7.95 cum. examples 6055, speed 1326.51 words/sec, time elapsed 1550.80 sec\n",
            "epoch 6, iter 2400, avg. loss 56.70, avg. ppl 8.11 cum. examples 6375, speed 1375.56 words/sec, time elapsed 1557.11 sec\n",
            "epoch 6, iter 2400, cum. loss 55.90, cum. ppl 8.63 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 2400, dev. ppl 39.176403\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 6, iter 2410, avg. loss 56.46, avg. ppl 8.00 cum. examples 320, speed 630.86 words/sec, time elapsed 1570.88 sec\n",
            "epoch 6, iter 2420, avg. loss 54.10, avg. ppl 8.16 cum. examples 640, speed 1339.61 words/sec, time elapsed 1577.04 sec\n",
            "epoch 6, iter 2430, avg. loss 56.23, avg. ppl 8.47 cum. examples 960, speed 1437.19 words/sec, time elapsed 1582.89 sec\n",
            "epoch 6, iter 2440, avg. loss 56.65, avg. ppl 8.65 cum. examples 1280, speed 1469.51 words/sec, time elapsed 1588.61 sec\n",
            "epoch 6, iter 2450, avg. loss 57.02, avg. ppl 8.46 cum. examples 1600, speed 1299.46 words/sec, time elapsed 1595.19 sec\n",
            "epoch 6, iter 2460, avg. loss 54.73, avg. ppl 7.79 cum. examples 1920, speed 1432.54 words/sec, time elapsed 1601.14 sec\n",
            "epoch 6, iter 2470, avg. loss 55.90, avg. ppl 8.16 cum. examples 2240, speed 1520.27 words/sec, time elapsed 1606.75 sec\n",
            "epoch 6, iter 2480, avg. loss 53.18, avg. ppl 8.03 cum. examples 2560, speed 1434.39 words/sec, time elapsed 1612.44 sec\n",
            "epoch 6, iter 2490, avg. loss 57.91, avg. ppl 8.86 cum. examples 2880, speed 1317.32 words/sec, time elapsed 1618.89 sec\n",
            "epoch 6, iter 2500, avg. loss 62.47, avg. ppl 8.71 cum. examples 3200, speed 1369.68 words/sec, time elapsed 1625.63 sec\n",
            "epoch 6, iter 2510, avg. loss 55.85, avg. ppl 8.44 cum. examples 3520, speed 1392.12 words/sec, time elapsed 1631.65 sec\n",
            "epoch 6, iter 2520, avg. loss 53.17, avg. ppl 7.73 cum. examples 3840, speed 1392.47 words/sec, time elapsed 1637.63 sec\n",
            "epoch 6, iter 2530, avg. loss 57.86, avg. ppl 8.73 cum. examples 4160, speed 1272.77 words/sec, time elapsed 1644.34 sec\n",
            "epoch 6, iter 2540, avg. loss 57.90, avg. ppl 8.36 cum. examples 4480, speed 1370.78 words/sec, time elapsed 1650.71 sec\n",
            "epoch 6, iter 2550, avg. loss 57.68, avg. ppl 9.12 cum. examples 4800, speed 1302.25 words/sec, time elapsed 1657.12 sec\n",
            "epoch 6, iter 2560, avg. loss 50.03, avg. ppl 7.62 cum. examples 5120, speed 1282.80 words/sec, time elapsed 1663.26 sec\n",
            "epoch 6, iter 2570, avg. loss 59.76, avg. ppl 8.96 cum. examples 5440, speed 1270.47 words/sec, time elapsed 1670.13 sec\n",
            "epoch 6, iter 2580, avg. loss 57.86, avg. ppl 8.77 cum. examples 5760, speed 1201.65 words/sec, time elapsed 1677.22 sec\n",
            "epoch 6, iter 2590, avg. loss 57.69, avg. ppl 9.27 cum. examples 6080, speed 1363.69 words/sec, time elapsed 1683.30 sec\n",
            "epoch 6, iter 2600, avg. loss 59.03, avg. ppl 8.65 cum. examples 6400, speed 1278.75 words/sec, time elapsed 1690.15 sec\n",
            "epoch 6, iter 2600, cum. loss 56.57, cum. ppl 8.44 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 2600, dev. ppl 39.362032\n",
            "hit patience 1\n",
            "hit #4 trial\n",
            "load previously best model and decay learning rate to 0.000031\n",
            "restore parameters of the optimizers\n",
            "epoch 6, iter 2610, avg. loss 55.20, avg. ppl 8.07 cum. examples 320, speed 729.80 words/sec, time elapsed 1701.74 sec\n",
            "epoch 6, iter 2620, avg. loss 54.63, avg. ppl 8.73 cum. examples 640, speed 1439.66 words/sec, time elapsed 1707.35 sec\n",
            "epoch 6, iter 2630, avg. loss 53.31, avg. ppl 8.11 cum. examples 960, speed 1352.53 words/sec, time elapsed 1713.37 sec\n",
            "epoch 6, iter 2640, avg. loss 53.46, avg. ppl 8.41 cum. examples 1280, speed 1340.43 words/sec, time elapsed 1719.36 sec\n",
            "epoch 6, iter 2650, avg. loss 57.34, avg. ppl 8.44 cum. examples 1600, speed 1373.90 words/sec, time elapsed 1725.62 sec\n",
            "epoch 6, iter 2660, avg. loss 52.61, avg. ppl 7.85 cum. examples 1920, speed 1373.61 words/sec, time elapsed 1731.57 sec\n",
            "epoch 6, iter 2670, avg. loss 53.96, avg. ppl 7.74 cum. examples 2240, speed 1451.82 words/sec, time elapsed 1737.39 sec\n",
            "epoch 6, iter 2680, avg. loss 52.71, avg. ppl 8.02 cum. examples 2560, speed 1308.44 words/sec, time elapsed 1743.58 sec\n",
            "epoch 6, iter 2690, avg. loss 56.20, avg. ppl 8.99 cum. examples 2880, speed 1294.75 words/sec, time elapsed 1749.91 sec\n",
            "epoch 6, iter 2700, avg. loss 51.03, avg. ppl 7.78 cum. examples 3200, speed 1403.55 words/sec, time elapsed 1755.58 sec\n",
            "epoch 6, iter 2710, avg. loss 52.91, avg. ppl 7.72 cum. examples 3520, speed 1336.65 words/sec, time elapsed 1761.78 sec\n",
            "epoch 6, iter 2720, avg. loss 51.07, avg. ppl 8.04 cum. examples 3840, speed 1306.64 words/sec, time elapsed 1767.77 sec\n",
            "epoch 6, iter 2730, avg. loss 53.52, avg. ppl 8.03 cum. examples 4160, speed 1360.85 words/sec, time elapsed 1773.82 sec\n",
            "epoch 6, iter 2740, avg. loss 55.43, avg. ppl 8.59 cum. examples 4480, speed 1389.16 words/sec, time elapsed 1779.75 sec\n",
            "epoch 6, iter 2750, avg. loss 58.24, avg. ppl 8.96 cum. examples 4800, speed 1209.76 words/sec, time elapsed 1786.78 sec\n",
            "epoch 6, iter 2760, avg. loss 53.43, avg. ppl 8.19 cum. examples 5120, speed 1436.72 words/sec, time elapsed 1792.44 sec\n",
            "epoch 6, iter 2770, avg. loss 58.14, avg. ppl 8.64 cum. examples 5440, speed 1408.56 words/sec, time elapsed 1798.57 sec\n",
            "epoch 6, iter 2780, avg. loss 56.60, avg. ppl 8.24 cum. examples 5760, speed 1226.66 words/sec, time elapsed 1805.57 sec\n",
            "epoch 6, iter 2790, avg. loss 56.26, avg. ppl 8.19 cum. examples 6055, speed 1318.36 words/sec, time elapsed 1811.55 sec\n",
            "epoch 7, iter 2800, avg. loss 52.94, avg. ppl 7.62 cum. examples 6375, speed 1351.60 words/sec, time elapsed 1817.72 sec\n",
            "epoch 7, iter 2800, cum. loss 54.44, cum. ppl 8.21 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 2800, dev. ppl 39.341169\n",
            "hit patience 1\n",
            "hit #5 trial\n",
            "early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "R5Tst7yCXIOJ"
      },
      "source": [
        "#### h. (3 points) Una vez tu modelo haya finalizado de entrenar (debería tomar menos de 1 hora en la máquina virtual) ejecute el siguiente comando para testear el modelo.\n",
        "(linux) sh run.sh test\n",
        "(windows) run.bat test\n",
        "Por favor, reporte el puntaje BLEU. Debería ser mayor a 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpWyt2OTCz6h",
        "outputId": "1d934ecc-ab4e-4a0e-e541-b55c6aff5c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sh run.sh test"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "load test source sentences from [./chr_en_data/test.chr]\n",
            "load test target sentences from [./chr_en_data/test.en]\n",
            "load model from model.bin\n",
            "Decoding:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "Decoding: 100% 1000/1000 [01:56<00:00,  8.61it/s]\n",
            "Corpus BLEU: 12.125998270142949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC2V7hMLDAxc"
      },
      "source": [
        "<font color = \"green\">\n",
        "BLEU = 12.125998\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoTluyBHC04s"
      },
      "source": [
        "#### i. (4 points) (written)\n",
        "En clase hemos aprendido sobre la atención de producto interno, atención multiplicativa y atención aditiva. Como recordatorio, la atención de producto interno es $\\mathbf{e_{t,i}}=\\mathbf{s_t^T\\ h_i}$, La atención multiplicativa es $\\mathbf{e_{t,i}}=\\mathbf{s_t^T\\ W\\ h_i}$ y la atención aditiva es $\\mathbf{e_{t,i}}=\\mathbf{v^T}\\ tanh(\\mathbf{W_1\\ h_i}+\\mathbf{W_2\\ s_t})$\n",
        "\n",
        "* i. (2 points) Explique una ventaja y una desventaja del producto interno como atención comparado con la atención multiplicativa.\n",
        "* ii. (2 points) Explicar una ventaja y una desventaja de la atención aditiva respecto a la atención multiplicativa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ILF7urHiXIOK"
      },
      "source": [
        "## 2. Analizando el sistema NMT (30 points)\n",
        "\n",
        "En la parte 1 hemos modelado nuestro problema NMT a un *subword-level*. Esto es, dada una oración en el idioma de origen, encontramos los componentes a nivel *subword* desde la matriz de *embeddings*.\n",
        "Alternativamente podríamos haber modelado el problema NMT a nivel palabra, obervando palabras completas desde la matriz de *embeddings*.\n",
        "### a. Por qué modelar el sistema **NMT** bajo *subword-level* vs *word-level*? (2 points)\n",
        "**Pista**: El Cherokee es un idioma polysintético"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9KdjH13oXIOK"
      },
      "source": [
        "<font color =\"green\">\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "P_HrBEdwXIOK"
      },
      "source": [
        "### b. Por qué las codificaciones (*Embeddings*) a nivel *letra* y *Subword* suelen ser más chicos que la codificación a nivel *palabra*? (2 points)\n",
        "Dar una razón en 1-2 oraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9rv3M8XDXIOL"
      },
      "source": [
        "<font color =\"green\">\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Atm1qyVJXIOL"
      },
      "source": [
        "### c. Cómo ayuda el entrenamiento multilenguaje en mejorar el rendimiento sobre lenguajes de bajos recursos? (2 points)\n",
        "Un reto de entrenar modelos **NMT** exitosos es la carencia de datos del lenguaje, particularmente para lenguajes de recursos escasos como el Cherokee. Una forma de responder a este reto es mediante el entrenamiento multi-lenguaje, donde entrenamos nuestro modelo **NMT** sobre varios idiomas (incluyendo Cherokee). <br>\n",
        "Puedes leer más en https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6LQJCFJxXIOL"
      },
      "source": [
        "<font color =\"green\">\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jq98EKqfXIOL"
      },
      "source": [
        "### d. (6 points)\n",
        "Aquí presentamos 3 ejemplos de errores encontrados en las salidas de nuestro modelo **NMT** (que es el mismo entrenado por tí). Los errores están subrayados.\n",
        "Por cada ejemplo de la oración de origen, referenciar la traducción a inglés y la traducción obtenida por el modelo.\n",
        "* Incluir las posibles razones que expliquen el error cometido por el modelo. (Tanto como su construcción lingüística o una limitación específica del modelo)\n",
        "* Describir una posible manera para corregir el error observado. Hay más de una manera de corregir un error. Por ejemplo, se podría ajustar el tamaño de la capa oculta o cambiando el mecanismo de atención.\n",
        "\n",
        "A continuación están las traducciones a analizar. Sólo analizar el error subrayado de cada oración. Se asegura que no tienes que saber Cherokee para responder estas preguntas. Pero si quieres saber un poco más de este lenguaje, sientete libre de consultar https://www.cherokeedictionary.net/ para buscar palabras.\n",
        "\n",
        "#### i. (2 points)\n",
        "**Origen de la traducción** (Source) = *Yona utsesdo anitsilvsgi digvtanv uwoduisdei* <br>\n",
        "**Traducción de referencia** *Fern had a crown of daisies in her hair*<br>\n",
        "**Traducción del modelo**: *Fern had <u>her hair</u> with her hair*\n",
        "#### ii. (2 points)\n",
        "**Origen de la traducción** (Source) = *Ulihelisdi nigalisda.* <br>\n",
        "**Traducción de referencia** *She is very excited.*<br>\n",
        "**Traducción del modelo**: *<u>it's</u> joy*\n",
        "#### iii. (2 points)\n",
        "**Origen de la traducción** (Source) = *Tsesdi hana yitsadawoesdi usdi atsadi!* <br>\n",
        "**Traducción de referencia** *Don't swim there, Littlefish!*<br>\n",
        "**Traducción del modelo**: *Don't know how <u>a small fish!</u>*\n",
        "\n",
        "### e. (4 points)\n",
        "Exploremos las salidas del modelo que hemos entrenado! Las traducciones del set de testeo producidas en el punto 1-i deberían encontrarse en\n",
        " `outputs/test_outputs.txt`.\n",
        "1. (2 points) Encontrar el límite donde las traducciones predichas son correctas para secuencias largas (4 o 5 palabras). Verificar el archivo destino *target* (Inglés).\n",
        "Contiene el archivo de entrenamiento ese texto (casi) literal? Si lo tiene o no, qué aprendió el sistema a hacer?\n",
        "2. (2 points) Encontrar el límite donde la traducción predicha comienza correcta durante una secuencia de 4 o 5 palabras, pero luego diverge (donde el final de la oración parece no tener sentido alguno). Qué dice esto sobre el comportamiento del decodificador?\n",
        "\n",
        "### f. (14 points)\n",
        "El puntaje **BLEU** es la métrica de evaluación automática más utilizada para sistemas **NMT**. Se suele calcular sobre el set de testeo completo, pero aquí consideraremos el puntaje para un solo ejemplo (*).\n",
        "Suponga que tenemos una oración de origen $\\mathbf{s}$, un conjunto de $k$ traducciones de referencia $\\mathbf{r_1 \\dots r_k}$ y una traducción candidata $\\mathbf{c}$.\n",
        "Para calcular el puntaje **BLEU** de $\\mathbf{c}$, primero debemos calcular la precisión n-grama modificada $p_n$ de $\\mathbf{c}$, para cada $n=1,2,3,4$ donde n es el n-grama.\n",
        "\n",
        "$p_n=\\frac{\\sum_{ngram\\in c} min(\n",
        "max_{i=1...k}(Count_{\\mathbf{r_i}} (n-gram), Count_{\\mathbf{c}} (n-gram))\n",
        ")}\n",
        "{\\sum_{ngram\\in c} Count_{\\mathbf{c}}(ngram)}$\n",
        "\n",
        "Aquí por cada n-grama que aparece en la traducción candidata $\\mathbf{c}$, contamos la máxima cantidad de veces que aparece en cualquiera traducción de referencia, limitado a la cantidad de veces que aparece en $\\mathbf{c}$ (este es el numerador). Dividimos este número por la cantidad de n-gramas en $\\mathbf{c}$ (denominador).\n",
        "\n",
        "(\\*) Esta definición de puntaje **BLEU** nivel oración coincide con la función `sentence_bleu()` en el paquete `nltk`. Observar que la función del NLTK es sensible a las mayúsculas. En esta pregunta, todo el texto está en minúsculas. https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu\n",
        "\n",
        "Luego, calculamos la *Brevity pealty* **BP**. <br>\n",
        "Sea $len(\\mathbf{c})$ la longitud de $\\mathbf{c}$ y sea $len(\\mathbf{r})$ la longitud de la traducción de referencia que es más cercana a $len(\\mathbf{c})$ (En el caso de dos referencias equidistantes, elegir $len(\\mathbf{r})$ como la más corta.)\n",
        "\n",
        "$ BP = \\begin{cases}\n",
        "1 \\hspace{10 mm} &si \\hspace{5mm} len(\\mathbf{c})\\ge len(\\mathbf{r}) \\\\\n",
        "exp(1-\\frac{len(\\mathbf{r})}{len (\\mathbf{c})})  &e.o.c.\n",
        "\\end{cases} $\n",
        "\n",
        "Finalmente, el puntaje **BLEU** para el candidato $\\mathbf{c}$ respecto a $\\mathbf{r_1 \\dots r_n}$ es:\n",
        "\n",
        "$BLEU = BP·exp(\\sum_{n=1}^4 \\lambda_n\\ log\\ p_{n})$\n",
        "\n",
        "Donde $\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4$ son pesos cuya suma es 1. El $log$ es el logaritmo natural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FXy_t2nzXIOM"
      },
      "source": [
        "#### i. Considere el ejemplo en español. (5 points)\n",
        "Oración de **Origen**: **El amor todo lo puede**<br>\n",
        "Traducción de referencia $\\mathbf{r_1}$:     *Love can always find a way* <br>\n",
        "Traducción de referencia $\\mathbf{r_2}$:     *Love makes anything possible*<br>\n",
        "Modelo NMT $\\mathbf{c_1}$: *The love can always do* <br>\n",
        "Modelo NMT $\\mathbf{c_2}$: *love can make anything possible*\n",
        "\n",
        "* Calcule el puntaje **BLEU** para $\\mathbf{c_1}$ y $\\mathbf{c_2}$.\n",
        "Sea $\\lambda_i = 0.5$ para $i\\in \\{1,2\\}$ y $\\lambda_i = 0$ para $i\\in \\{3,4\\}$ (Esto significa ignorar los 3-gramas y 4-gramas. no calcular $p_3$ y $p_4$)\n",
        "Cuando se calculan los puntajes BLEU, mostrar tu trabajo (valores $p_1, p_2, len(\\mathbf{c}),len(\\mathbf{r}), BP$). Notar que los puntajes **BLEU** se pueden expresar entre 0 y 1, o entre 0 y 100. El código utiliza la escala 0-100, mientras que las preguntas utilizan la escala 0-1.\n",
        "\n",
        "Cuál de las dos traducciones se considera mejor según **BLEU**? Estás de acuerdo en que es la mejor traducción?\n",
        "\n",
        "ii. (5 points) Nuestro disco duro se ha corrompido y hemos perdido la traducción de referencia $\\mathbf{r_2}$. Por favor, recalcule el puntaje **BLEU** para $\\mathbf{c_1}$ y $\\mathbf{c_2}$, esta vez solamente respecto a $\\mathbf{r_1}$. Cuál de las dos traducciones recive un mayor puntaje? Estás de acuerdo en que es la mejor traducción?\n",
        "iii. (2 points) Por la disponibilidad de datos, los sistemas NMT suelen evaluarse respecto a una sola traducción de referencia. Explique por qué podría ser problemático.\n",
        "iv. (2 points) Enumerar dos ventajas y dos desventajas de **BLEU** comparado con la evaluación humana como métrica para *Machine Translation*"
      ]
    }
  ]
}