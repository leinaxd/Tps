{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "a5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leinaxd/Tps/blob/master/a5/a5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4zY3aaibN4J3"
      },
      "source": [
        "# GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dDgiES8tN4J8"
      },
      "source": [
        "#@markdown Cargar\n",
        "%cd ~\n",
        "%cd /content/\n",
        "uname = \"leinaxd\"\n",
        "!git config --global user.email '$uname@gmail.com'\n",
        "!git config --global user.name '$uname'\n",
        "\n",
        "from getpass import getpass\n",
        "password = getpass('Password:')\n",
        "!git clone https://$uname:$password@github.com/leinaxd/Tps/\n",
        "\n",
        "%cd Tps/a5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gkaf6DXMN4J-"
      },
      "source": [
        "commit = \"a5: Init\" #@param {type:\"string\"}\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"$commit\"\n",
        "!git push"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZiDwwu-7N4KJ"
      },
      "source": [
        "#@markdown pull / update\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "HGACFxhkN4KK"
      },
      "source": [
        "# CS224N Assignment 5: Self-Attention, Transformers and Pretraining (21 + 35 + 5 Points)\n",
        "\n",
        "---\n",
        "Nota. Hay diferentes cuestiones a tener a la hora de realizar este proyecto\n",
        "\n",
        "* Hay preguntas de deducciones matemáticas\n",
        "* La cantidad de código pytorch, y complejidad del código en este proyecto es mucho menor que en el proyecto 4. Sin embargo, se le dará menor seguimiento y menor andamiaje para escribir el código.\n",
        "* Este proyecto incluye un paso de pre-entrenamiento que toma aproximadamente 2 horas a realizar en Azure, y tendrás que escribirlo dos veces.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "-79T2Iy-N4KL"
      },
      "source": [
        "Este proyecto se trata de una investigación sobre los bloques y los efectos del entrenamiento de *self-attention* y *Transformers*.\n",
        "Esto cubre las propiedades matemáticas mediante preguntas escritas.\n",
        "Además, obtendrás experiencia sobre la construcción de sistemas prácticos mediante el repropósito del *codebase*.\n",
        "\n",
        "El trabajo se divide en una parte matemática escrita y una parte de codificación, con su propio conjunto de preguntas escritas.\n",
        "\n",
        "Aquí un pequeño resumen\n",
        "\n",
        "1. **Exploración matematica**: Qué clase de operaciones puede implementar sencillamente *self-attention*? Por qué deberíamos utilizar cosas mas bellas como *multi-head self attention*?\n",
        "Esta sección utilizará cierta investigación matemática para iluminar algunas de las motivaciones de las redes de *self-attention* y *Transofmers*.\n",
        "2. **Extendiendo el codebase investigado**: En esta parte del trabajo, obtendrás cierta experiencia e intuición sobre las investigaciones más filosas en NLP.\n",
        "Enseñar a los modelos NLP los hechos del mundo mediante *Pretraining* y acceso al conocimiento mediante un ajuste fino.\n",
        "Entrenarás un modelo *Transformer* para intentar responder preguntas sencillas de la forma *\"Donde ha nacido la persona [x]?\"* Sin proveer ningún texto de entrada del cual extraer la respuesta.\n",
        "Encontrarás que los modelos son capaces de aprender muchos Hechos sobre donde ha nacido las personas mediante el preentrenamiento y acceder a esta información durante el ajuste fino para responder estas preguntas.\n",
        "\n",
        "Luego observarás detenidamente al sistema construído y razonarás sobre las implicaciones y responsabilidades sobre tal conocimiento implícito."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "CZj4CcdpN4KM"
      },
      "source": [
        "## 1. Attention Exploration (21 points)\n",
        "El *self-attention Multi-Headed* es el componende del núcleo en los *Transformers*. En esta pregunta, obtendrás cierta práctica trabajando con las ecuaciones de *self-attention*, que motivarán a preguntarse Por qué *Muliti-Headed self-Attention* es preferible frente a *single-headed self-attention*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "R51_g6SaN4KO"
      },
      "source": [
        "### a. Copiado en Atención (2 points)\n",
        "Recuerde que la atención, podría ser vista como una operación sobre una secuencia *Querry* $q\\in \\mathbb{R}^d$, un conjunto de vectores evaluados $\\{v_1\\dots v_n\\},\\hspace{5mm} v_i\\in \\mathbb{R}^d$ y un conjunto de vectores de dirección *Keys* $\\{k_1 \\dots k_n\\},\\hspace{5mm}k_i\\in \\mathbb{R}^d$ especificado según:\n",
        "\n",
        "$\\begin{align}\n",
        "c&= \\sum_{i=1}^n v_i \\alpha_i\\\\\n",
        "\\alpha_i &= \\frac{exp(k_i^T\\ q)}{\\sum_{j=1}^n exp(k_j^T\\ q)}\n",
        "\\end{align}$\n",
        "\n",
        "Donde $\\alpha_i$ es comunmente llamado \"Los pesos de Atención\" y las salidas $c\\in \\mathbb{R}^d$ su correspondiente promedio ponderado sobre los valores de los vectores. <br>\n",
        "Primero mostraremos que para la *Atención* es particularmente simple \"Copiar\" un valor del vector de salida $c$.\n",
        "* Describa (en una oración) qué propiedades de la entrada de la operación de atención resulta en que la salida $c$ es aproximadamente igual a $v_j$ para algún $j\\in \\{1\\dots n\\}$.\n",
        "Específicamente, qué debe ser cierto sobre la secuencia $q$, los valores $\\{v_1\\dots v_n\\}$ y/o las *keys* $\\{k_1\\dots k_n\\}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "dns_Wux9N4KP"
      },
      "source": [
        "### b. Un promedio de 2 (4 points)\n",
        "Considere el conjunto de vectores *keys* $\\{k_1 \\dots k_n\\}$ donde todos los vectores *key* son perpendiculares, esto es que $k_i \\perp k_j\\hspace{5mm} \\forall i\\neq j$. Sea $||k_i||=1\\hspace{5mm} \\forall i$. Sea $\\{v_1 \\dots v_n\\}$ el conjunto arbitrario de vectores evaluados. Sea $v_a,\\ v_b\\in \\{v_1\\dots v_n\\}$ dos vectores. Dada una expresión para el vector de secuencia $q$ tal que la salida $c$ es aproximadamente igual al promedio entre $v_a$ y $v_b$ tal que sea $\\frac{1}{2}(v_a+v_b)$.\n",
        "Observar que puedes referenciar el correspondiente vector clave *key* de $v_a$ y $v_b$ como $k_a$ y $k_b$ respectivamente.\n",
        "\n",
        "* Pista: Mientras que la función softmax nunca promediará exactamente 2 vectores, puedes aproximarte utilizando un múltiplo escalar grande en la expresión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OlvroBjqN4Ke"
      },
      "source": [
        "### c. Desventajas de la atención de una cabezal (5 points)\n",
        "En la parte anterior, hemos visto cómo era posible para la atención de cabezal único concentrarse equitativamente en dos valores. El mismo concepto podría extenderse facilmente para cualquier conjunto de valores. En esta pregunta veremos por qué no es una solucíón práctica.\n",
        "Considere el conjunto de vectores clave *keys* $\\{k_1\\dots k_n\\}$ que fueron muestreados aleatoriamente de $k_i~N(\\mu_i, \\Sigma_i)$ donde la media $\\mu_i$ es conocida pero la covarianza $\\Sigma_i$ no lo es. Además, asuma que las medias $\\mu_i$ son perpendiculares, es decir $\\mu_i^T\\ \\mu_j = 0\\hspace{5mm} si i\\neq j$ y son de norma unitaria, $||\\mu_i|| = 1 $.\n",
        "\n",
        "i. (2 points) Asuma que las matrices de Covarianza son $\\Sigma_i = \\alpha I$ para $\\alpha$ evanescentemente pequeño. Diseñe una secuencia $q$ en términos de $\\mu_i$ tal como antes $c\\approx \\frac{1}{2}(v_a+v_b)$ y provea un breve argumento de por qué funciona.\n",
        "\n",
        "ii. (3 points) Aunque la atención de cabezal simple es resistente a pequeñas perturbaciones en sus claves *keys* algunos tipos de grandes perturbaciones podrían mostrar un inconveniente mayor. Específicamente, en muchos casos, un vector clave $k_a$ pordría ser mayor o menor en norma que otros, mientras que continúa apuntando en la misma dirección que $\\pi_a$. Como ejemplo, considere la covarianza para el item a como $\\Sigma_a = \\alpha \\ I +\\frac{1}{2}(\\mu_a\\ \\mu_a^T)$ para $\\alpha$ evanescentemente pequeño (como se ilustra en la figura 1). Además, sea $\\Sigma_i = \\alpha \\ I\\hspace{5mm} \\forall i\\neq a$\n",
        "Cuando muestreas $\\{k_1 \\dots k_n\\}$ múltiples veces y utilizas el vector $q$ definido en la parte (i), Cualitativamente, cómo esperas que se vea el vector $c$ para diferentes muestras?\n",
        "\n",
        "![Figura 1](https://github.com/leinaxd/Tps/raw/master/a5/imgs/mu_vector.jpg)\n",
        "Figura 1. El vector $\\mu_a$ (mostrado en 2D como ejemplo) donde el rango de posibles valores de $k_a$ se observan en rojo. Como se mencionó anteriormente, $k_a$ apunta fuertemente en la misma dirección que $\\mu_a$ pero podría tener una mayor o menor magnitud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "CT4QWB3vN4Ke"
      },
      "source": [
        "### d. Beneficios de la atención multi-headed (3 points)\n",
        "Ahora veremos algo sobre el poder de la atención multicabeza. Consideraremos una versión simplificada idéntica a la *single-headed* presentada en este trabajo, excepto que tiene dos vectores de secuencia $q_1$ y $q_2$. Esto lleva a un par de vectores $c_1$ y $c_2$ cada uno la salida de una atención *single-headed* dados sus respectivos vectores de secuencia.\n",
        "La salida final de la atención *multi-headed* es su promedio $\\frac{1}{2}(c_1 + c_2)$. Como en la pregunta (1.c) considere el conjunto de vectores clave *key* como $\\{k_1\\dots k_n\\}$ que son muestreados aleatoriamente $k_i~N(\\mu_i, \\Sigma_i)$. Donde la media $\\mu_i$ es conocida pero la covarianza $\\Sigma_i$ no lo es.\n",
        "También como antes, asuma que las medias son mutuamente ortogonales: $\\mu_i^T \\mu_j = 0\\hspace{5mm}si\\ i\\neq j$ y de norma unitaria $||\\mu_i|| = 1$.\n",
        "\n",
        "i. (1 point) Asuma que las matrices de covarianza son $\\Sigma_i = \\alpha I$ para $\\alpha$ evanescentemente pequeño. Diseñe $q_1$ y $q_2$ tal que $c$ es aproximadamente igual a $\\frac{1}{2}(v_a + v_b)$.\n",
        "ii. (2 points) Asuma que las matrices de covarianza son $\\Sigma_a = \\alpha\\ I + \\frac{1}{2}(\\mu_a^T \\mu_a)$ para $\\alpha$ evanescentemente pequeño y $\\Sigma_i = \\alpha\\ I\\hspace{5mm} \\forall i\\neq a$.\n",
        "Tomar los vectores de secuencia $q_1$ y $q_2$ tal cual fueron diseñados en la parte (i).\n",
        "Cualitativamente, Qué salida $c$ esperas obtener a través de diferentes muestras de vectores clave? Explique brevemente por qué. Puedes ignorar los casos donde $q_i^T\\ k_a<0$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "iz9hd1zBN4Kf"
      },
      "source": [
        "### e. *self-attention* *Key-Query-Value* en redes neuronales\n",
        "Hasta ahora, hemos discutido la atención como una función sobre un conjunto de vectores clave, un conjunto de vectores con valores y un vector de secuencia. En los *Transoformers* realizamos *self-attention* que crudamente signfica que dibujamos las claves, valores y secuencias de los mismos datos. Con mayor precisión, sea $\\{x_1 \\dots x_n\\}$ una secuencia de vectores en $\\mathbb{R}^d$. Piense a cada $x_i$ como la representante de la palabra $i$ en la oración. Una forma de *self-attention* defina a las claves, valores y secuencias como sigue.\n",
        "Sean las matrices de parámetros $V,\\ K,\\ Q\\ \\in \\mathbb{R}^{d\\times d}$, luego:\n",
        "\n",
        "$\\begin{align}\n",
        "v_i &= V \\ x_i\\hspace{5mm} &&i \\in \\{1\\dots n\\}\\\\\n",
        "k_i &= K \\ x_i\\hspace{5mm} &&i \\in \\{1\\dots n\\}\\\\\n",
        "q_i &= Q \\ x_i\\hspace{5mm} &&i \\in \\{1\\dots n\\}\\\\\n",
        "\\end{align}$\n",
        "\n",
        "Luego obtenemos el vector de contexto para cada entrada $i$: Tenemos $c_i=\\sum_{j=1}^n \\alpha_{ij}\\ v_j$, donde $\\alpha_{ij}$ es definida por $\\alpha_{ij}=\\frac{exp(k_j^T\\ q_i)}{\\sum_{l=1}^n exp(k_l^T\\ q_i)}$ Observar que esto es un *single-headed* *self-attention*.\n",
        "En esta pregunta mostraremos cómo la atención *key-value-query* como esta le permite a la red utilizar diferentes características de los vectores de entrada $x_i$ en cómo define las claves, secuencias y valores. Intuitivamente esto le permite a las redes elegir diferentes características de $x_i$ para ser el \"contenido\" (vector de valores) contra aquello que utiliza para determinar \"donde mirar\" por contenido (claves y secuencias)\n",
        "\n",
        "i. (3 points) Primero considere si no tuviesemos la atención \"clave-secuencia-valor\". Para las claves, secuencias y valores utilizaremos directamente $x_i$, esto es $v_i=q_i=k_i=x_i$. Consideremos un conjunto específico de $x_i$, en particular, sean $u_a,\\ u_b,\\ u_c,\\ u_d$ vectores mutuamente ortogonales en $\\mathbb{R}^d$ cada uno con misma norma $||u_a|| = ||u_b|| = ||u_c|| = ||u_d|| = \\beta$, donde $\\beta$ es grande. Ahora sea nuestro $x_i$ tal que\n",
        "\n",
        "$\\begin{align}\n",
        "x_1 &=u_d + u_b\\\\\n",
        "x_2 &= u_a\\\\\n",
        "x_3 &= u_c + u_b\n",
        "\\end{align}$\n",
        "\n",
        "Si aplicamos *self-attention* con estos vectores, a qué vector se aproxima $c_2$? Será posible que $c_2$ se aproxime a $u_b$ añadiendo tanto a $u_d$ o $u_c$ a $x_2$? Explique por qué o por qué no.\n",
        "\n",
        "ii. (4 points) Ahora considere utilizar la atención *key-query-value* como se definió originalmente, utilizando las mismas definiciones para $x_1,\\ x_2$ y $x_3$ como en la parte (i), especifique las matrices $K,Q,V$ tal que $c_2\\approx u_b$ y $c_1 \\approx u_b-u_c$. Hay muchas soluciones a este problema, por lo que será fácil para tí. Si primero encuentras una $V$ tal que $v_1=u_b$ y $v_3=u_b-u_c$ luego trabajar en $Q$ y $K$.\n",
        "Algunas propiedades del producto externo podrían ser útiles.\n",
        "Para vectores ortogonales $u,v,w\\in \\mathbb{R}^d$ el producto externo $u\\ v^T$ es una matriz de $\\mathbb{R}^{d\\times d}$ y $(u\\ v^T)v = u(v^T\\ v)= u ||v||_2^2$ y $(u\\ v^T)w = u(v^T\\ w) = u·0$ (La última igualdad se debe a que $v$ y $w$ son ortogonales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8yQumfmEtvH"
      },
      "source": [
        "## Pretrained Transformers Models and Knowledge Access (35 points)\n",
        "\n",
        "En este punto, entrenarás un modelo de Transformer que realizará una tarea de acceso al conocimiento del mundo. Conocimiento que no es proporcionado al entrenar el modelo. (Al menos si quisieras generalizar por fuera del set de entrenamiento)\n",
        "\n",
        "Encontrarás que mas o menos fallará la tarea. A continación, aprenderás a pre-entrenar el modelo utilizando texto de Wikipedia, y calibrando el modelo sobre la misma tarea, se le permitirá al modelo acceder al conocimiento de la etapa de pre-entrenamiento.\n",
        "De esta forma el modelo rendirá considerablemente mejor, sobre los datos de desarrollo (development)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q62yONI8GGcG"
      },
      "source": [
        "El código provisto es un *fork* de Andrej Karphaty [minGPT](https://github.com/kaparthy/minGPT). Es más amigable que la mayoría de los códigos de investigadores y es suficientemente simple y transparente. La parte \"GPT\" de minGPT corresponde al modelo de Transofmer de openAI descripto en [Este Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HizoVnCCHJg5"
      },
      "source": [
        "Como en los proyectos anteriores, desarrollaras el código localmente y luego entrenarás en Azure. Se utilizará el mismo entorno conda que el proyecto anterior y el mismo proceso de entrenamiento. Específicamente, ejecutarás \"conda activate py37_pytorch\" en la máquina de Azure. \n",
        "\n",
        "Requerirá aproximadamente 5hs de entrenamiento, por lo que debes administrar bien el tiempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSDigcsYHqi5"
      },
      "source": [
        "El flujo de trabajo es el siguiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0gF6pLCHtrC"
      },
      "source": [
        "### a. (0 points) Probar la demostración\n",
        "\n",
        "En la carpeta `mingpt-demo/` hay un notebook que entrena y muestrea un modelo de lenguaje Transormer.\n",
        "\n",
        "Echarle un vistazo, para familiarizarse sobre cómo define y entrena el modelo. Mucho del código que escribas estará inspirado en ese archivo.\n",
        "\n",
        "**OBS**. Para esta tarea, no debes escribir código ni responder preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZonF8zpImpL"
      },
      "source": [
        "### b. (0 points) Lectura de **NameDataset**, nuestro dataset de pares de nombre y lugar de nacimiento\n",
        "\n",
        "La tarea trata de acceder al lugar de nacimiento de una notable persona, escrita en Wikipedia. Creemos que es una forma particularmente simple de un Question Answering.\n",
        "\n",
        "Q: *Where was \\<person\\> was?*\n",
        "\n",
        "A: \\<*place*\\>\n",
        "\n",
        "* A partir de ahora, trabajaremos en el directorio `src/`.\n",
        "\n",
        "* El código en `mingpt-demo/` no será evaluado para esta tarea.\n",
        "\n",
        "* En `dataset.py` encontrarás la clase **NameDataset** que lee un archivo TSV (tab-separed values) de pares (nombre/lugar) y produce ejemplos para alimentar nuestro modelo Transformer.\n",
        "\n",
        "Para tener una noción de los ejemplos que estaremos trabajando, si ejecutas el código a continuación, cargará tu **NameDataset** en el conjunto de entrenamiento `birth_places_train.tsv` e imprimirá algunos ejemplos.\n",
        "\n",
        "`python src/dataset.py namedata`\n",
        "\n",
        "**Obs**. Para este punto, no hay que escribir código o responder preguntas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTm09FwYLUp8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brOlwfJZLU21"
      },
      "source": [
        "### c. (0 points) Implementar finetunning (Sin entrenamiento)\n",
        "\n",
        "Echar un vistazo a `run.py` Tiene cierto código esqueleto especificando banderas que eventualmente utilizarás como comandos de línea.\n",
        "\n",
        "En particular, querrás *pre-entrenar*, *calibrar finamente* o evaluar el código con este código.\n",
        "\n",
        "Por ahora, nos centraremos en la función de  calibración fina, para el caso \"sin pre-entrenamiento\".\n",
        "\n",
        "Inspirandonos sobre el código de entrenamiento en `play_char.ipynb`, escribir el código para calibrar el modelo de transformer sobre el dataset nombre/lugar de nacimiento, mediante ejemplos de la clase **NameDataset**.\n",
        "\n",
        "Por ahora, implemente el caso \"sin pre-entrenamiento\" (crear el modelo de la nada y entrenarlo sobre la tarea de predicción de la parte (b)).\n",
        "\n",
        "Tendrás que modificar 2 secciones, marcadas [parte c] en el código. Una es la de inicializar el modelo y la otra ajustarlo finamente.\n",
        "\n",
        "**Obs**. Solo debes inicializar el modelo en el caso etiquetado \"vainilla\" (luego en la sección g exploraremos una variante)\n",
        "\n",
        "Utilice los hiperparámetros del entrenador especificados en el código `run.py`\n",
        "\n",
        "Además echarle un vistazo al código de evaluación que ha sido implementado por tí. \n",
        "* Muestrea predicciones del modelo entrenado\n",
        "* y llama a `evaluate_places()`\n",
        "* para obtener el porcentaje total de lugares correctamente predichos.\n",
        "Ejecutarás este código en la parte (d) al evaluar tus modelos entrenados.\n",
        "\n",
        "Este es un paso intermedio para después, incluyendo la Parte d, que consiste en comandos que puedes ejecutar para verificar tu implementación. \n",
        "\n",
        "No se piden respuestas escritas en esta tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC11m9kXOkLd"
      },
      "source": [
        "### d. (5 points) Hacer predicciones (sin entrenamiento)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG0xFl8COwFm"
      },
      "source": [
        "# 3 Consideraciones en el conocimiento pre-entrenado (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HYV07CqQGr5"
      },
      "source": [
        "### a. (1 point) Sintéticamente explique por qué el modelo preentrenado (vainilla) es capaz de obtener un Accuracy superior al 10%, cuando el modelo no preentrenado no pudo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXOvVgC0Qb31"
      },
      "source": [
        "### b. (2 points) Observar algunas predicciones correctas del modelo preentrenado y ajustado finamente del modelo \"Vainilla\" así también como algunos errores.\n",
        "\n",
        "Creemos que encontrarás imposible distinguir, a simple vista, cuando el modelo obtuvo la respuesta correcta o se inventó una respuesta.\n",
        "\n",
        "Considere las implicaciones de esto para un usuario final que utilice diferentes componentes preentrenados NLP.\n",
        "\n",
        "Traer dos razones por la que la indeterminación del comportamiento del modelo podría traer preocupaciones en tales aplicaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxOz485NRRQT"
      },
      "source": [
        "### c. (2 points) Si tu modelo no ve el nombre en la hora de preentrenamiento, y la persona no fue vista en el ajuste fino.\n",
        "\n",
        "No es posible que podamos saber en donde nació. Sin embargo, nuestro modelo producirá una salida para el lugar de nacimiento.\n",
        "\n",
        "Concretamente, Describir una estrategia que podría utilizar tu modelo para predecir el lugar de nacimiento para el nombre de la persona y una razón de por qué esto genera preocupación al uso en tales aplicaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhWUT6j9OwJz"
      },
      "source": [
        "# Referecias\n",
        "\n",
        "1. Radford, Narasimhan, Salimans y Sutskever. \"Improving language understanding with unsupervised learning\" Technical report, OpenAI (2018)\n",
        "\n",
        "2. Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li y Liu. \"Exploring the limits of transfer learning with unified text-to-text transformer\". Journal of machine learning research 21, 14 (2020), 1-67\n",
        "\n",
        "3. Tay, Bahri, Metzler, Juan, Zhao y Zheng. \"Synthesizer: Rethinking self-attention in transformers models\". arXiv preprint: 2005.00743 (2020). "
      ]
    }
  ]
}