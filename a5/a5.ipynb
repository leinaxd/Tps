{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GitHub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@markdown Cargar\n",
    "%cd ~\n",
    "%cd /content/\n",
    "uname = \"leinaxd\"\n",
    "!git config --global user.email '$uname@gmail.com'\n",
    "!git config --global user.name '$uname'\n",
    "\n",
    "from getpass import getpass\n",
    "password = getpass('Password:')\n",
    "!git clone https://$uname:$password@github.com/leinaxd/Tps/\n",
    "\n",
    "%cd Tps/a5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "commit = \"a5: Init\" #@param {type:\"string\"}\n",
    "\n",
    "!git add .\n",
    "!git commit -m \"$commit\"\n",
    "!git push"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@markdown pull / update\n",
    "!git pull"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CS224N Assignment 5: Self-Attention, Transformers and Pretraining (21 + 35 + 5 Points)\n",
    "\n",
    "---\n",
    "Nota. Hay diferentes cuestiones a tener a la hora de realizar este proyecto\n",
    "\n",
    "* Hay preguntas de deducciones matemáticas\n",
    "* La cantidad de código pytorch, y complejidad del código en este proyecto es mucho menor que en el proyecto 4. Sin embargo, se le dará menor seguimiento y menor andamiaje para escribir el código.\n",
    "* Este proyecto incluye un paso de pre-entrenamiento que toma aproximadamente 2 horas a realizar en Azure, y tendrás que escribirlo dos veces.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este proyecto se trata de una investigación sobre los bloques y los efectos del entrenamiento de *self-attention* y *Transformers*.\n",
    "Esto cubre las propiedades matemáticas mediante preguntas escritas.\n",
    "Además, obtendrás experiencia sobre la construcción de sistemas prácticos mediante el repropósito del *codebase*.\n",
    "\n",
    "El trabajo se divide en una parte matemática escrita y una parte de codificación, con su propio conjunto de preguntas escritas.\n",
    "\n",
    "Aquí un pequeño resumen\n",
    "\n",
    "1. **Exploración matematica**: Qué clase de operaciones puede implementar sencillamente *self-attention*? Por qué deberíamos utilizar cosas mas bellas como *multi-head self attention*?\n",
    "Esta sección utilizará cierta investigación matemática para iluminar algunas de las motivaciones de las redes de *self-attention* y *Transofmers*.\n",
    "2. **Extendiendo el codebase investigado**: En esta parte del trabajo, obtendrás cierta experiencia e intuición sobre las investigaciones más afiladas en NLP.\n",
    "Enseñar a los modelos NLP los hechos del mundo mediante *Pretraining* y acceso al conocimiento mediante un ajuste fino.\n",
    "Entrenarás un modelo *Transformer* para intentar responder preguntas sencillas de la forma *\"Donde ha nacido la persona [x]?\"* Sin proveer ningún texto de entrada del cual extraer la respuesta.\n",
    "Encontrarás que los modelos son capaces de aprender muchos Hechos sobre donde ha nacido las personas mediante el preentrenamiento y acceder a esta información durante el ajuste fino para responder estas preguntas.\n",
    "\n",
    "Luego observarás detenidamente al sistema construído y razonarás sobre las implicaciones y responsabilidades sobre tal conocimiento implícito."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Attention Exploration (21 points)\n",
    "El *self-attention Multi-Headed* es el componende del núcleo en los *Transformers*. En esta pregunta, obtendrás cierta práctica trabajando con las ecuaciones de *self-attention*, que motivarán a preguntarse Por qué *Muliti-Headed self-Attention* es preferible frente a *single-headed self-attention*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. Copiado en Atención (2 points)\n",
    "Recuerde que la atención, podría ser vista como una operación sobre una secuencia *Querry* $q\\in \\mathbb{R}^d$, un conjunto de vectores evaluados $\\{v_1\\dots v_n\\},\\hspace{5mm} v_i\\in \\mathbb{R}^d$ y un conjunto de vectores de dirección *Keys* $\\{k_1 \\dots k_n\\},\\hspace{5mm}k_i\\in \\mathbb{R}^d$ especificado según:\n",
    "\n",
    "$\\begin{align}\n",
    "c&= \\sum_{i=1}^n v_i \\alpha_i\\\\\n",
    "\\alpha_i &= \\frac{exp(k_i^T\\ q)}{\\sum_{j=1}^n exp(k_j^T\\ q)}\n",
    "\\end{align}$\n",
    "\n",
    "Donde $\\alpha_i$ es comunmente llamado \"Los pesos de Atención\" y las salidas $c\\in \\mathbb{R}^d$ su correspondiente promedio ponderado sobre los valores de los vectores. <br>\n",
    "Primero mostraremos que para la *Atención* es particularmente simple \"Copiar\" un valor del vector de salida $c$.\n",
    "* Describa (en una oración) qué propiedades de la entrada de la operación de atención resulta en que la salida $c$ es aproximadamente igual a $v_j$ para algún $j\\in \\{1\\dots n\\}$.\n",
    "Específicamente, qué debe ser cierto sobre la secuencia $q$, los valores $\\{v_1\\dots v_n\\}$ y/o las *keys* $\\{k_1\\dots k_n\\}$?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b. Un promedio de 2 (4 points)\n",
    "Considere el conjunto de vectores *keys* $\\{k_1 \\dots k_n\\}$ donde todos los vectores *key* son perpendiculares, esto es que $k_i \\perp k_j\\hspace{5mm} \\forall i\\neq j$. Sea $||k_i||=1\\hspace{5mm} \\forall i$. Sea $\\{v_1 \\dots v_n\\}$ el conjunto arbitrario de vectores evaluados. Sea $v_a,\\ v_b\\in \\{v_1\\dots v_n\\}$ dos vectores. Dada una expresión para el vector de secuencia $q$ tal que la salida $c$ es aproximadamente igual al promedio entre $v_a$ y $v_b$ tal que sea $\\frac{1}{2}(v_a+v_b)$.\n",
    "Observar que puedes referenciar el correspondiente vector clave *key* de $v_a$ y $v_b$ como $k_a$ y $k_b$ respectivamente.\n",
    "\n",
    "* Pista: Mientras que la función softmax nunca promediará exactamente 2 vectores, puedes aproximarte utilizando un múltiplo escalar grande en la expresión"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c. Desventajas de la atención de una cabezal (5 points)\n",
    "En la parte anterior, hemos visto cómo era posible para la atención de cabezal único concentrarse equitativamente en dos valores. El mismo concepto podría extenderse facilmente para cualquier conjunto de valores. En esta pregunta veremos por qué no es una solucíón práctica.\n",
    "Considere el conjunto de vectores clave *keys* $\\{k_1\\dots k_n\\}$ que fueron muestreados aleatoriamente de $k_i~N(\\mu_i, \\Sigma_i)$ donde la media $\\mu_i$ es conocida pero la covarianza $\\Sigma_i$ no lo es. Además, asuma que las medias $\\mu_i$ son perpendiculares, es decir $\\mu_i^T\\ \\mu_j = 0\\hspace{5mm} si i\\neq j$ y son de norma unitaria, $||\\mu_i|| = 1 $.\n",
    "\n",
    "i. (2 points) Asuma que las matrices de Covarianza son $\\Sigma_i = \\alpha I$ para $\\alpha$ evanescentemente pequeño. Diseñe una secuencia $q$ en términos de $\\mu_i$ tal como antes $c\\approx \\frac{1}{2}(v_a+v_b)$ y provea un breve argumento de por qué funciona.\n",
    "\n",
    "ii. (3 points) Aunque la atención de cabezal simple es resistente a pequeñas perturbaciones en sus claves *keys* algunos tipos de grandes perturbaciones podrían mostrar un inconveniente mayor. Específicamente, en muchos casos, un vector clave $k_a$ pordría ser mayor o menor en norma que otros, mientras que continúa apuntando en la misma dirección que $\\pi_a$. Como ejemplo, considere la covarianza para el item a como $\\Sigma_a = \\alpha \\ I +\\frac{1}{2}(\\mu_a\\ \\mu_a^T)$ para $\\alpha$ evanescentemente pequeño (como se ilustra en la figura 1). Además, sea $\\Sigma_i = \\alpha \\ I\\hspace{5mm} \\forall i\\neq a$\n",
    "Cuando muestreas $\\{k_1 \\dots k_n\\}$ múltiples veces y utilizas el vector $q$ definido en la parte (i), Cualitativamente, cómo esperas que se vea el vector $c$ para diferentes muestras?\n",
    "\n",
    "![Figura 1](https://github.com/leinaxd/Tps/raw/master/a5/imgs/mu_vector.jpg)\n",
    "Figura 1. El vector $\\mu_a$ (mostrado en 2D como ejemplo) donde el rango de posibles valores de $k_a$ se observan en rojo. Como se mencionó anteriormente, $k_a$ apunta fuertemente en la misma dirección que $\\mu_a$ pero podría tener una mayor o menor magnitud."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d. Beneficios de la atención multi-headed (3 points)\n",
    "Ahora veremos algo sobre el poder de la atención multicabeza. Consideraremos una versión simplificada idéntica a la *single-headed* presentada en este trabajo, excepto que tiene dos vectores de secuencia $q_1$ y $q_2$. Esto lleva a un par de vectores $c_1$ y $c_2$ cada uno la salida de una atención *single-headed* dados sus respectivos vectores de secuencia.\n",
    "La salida final de la atención *multi-headed* es su promedio $\\frac{1}{2}(c_1 + c_2)$. Como en la pregunta (1.c) considere el conjunto de vectores clave *key* como $\\{k_1\\dots k_n\\}$ que son muestreados aleatoriamente $k_i~N(\\mu_i, \\Sigma_i)$. Donde la media $\\mu_i$ es conocida pero la covarianza $\\Sigma_i$ no lo es.\n",
    "También como antes, asuma que las medias son mutuamente ortogonales: $\\mu_i^T \\mu_j = 0\\hspace{5mm}si\\ i\\neq j$ y de norma unitaria $||\\mu_i|| = 1$.\n",
    "\n",
    "i. (1 point) Asuma que las matrices de covarianza son $\\Sigma_i = \\alpha I$ para $\\alpha$ evanescentemente pequeño. Diseñe $q_1$ y $q_2$ tal que $c$ es aproximadamente igual a $\\frac{1}{2}(v_a + v_b)$.\n",
    "ii. (2 points) Asuma que las matrices de covarianza son $\\Sigma_a = \\alpha\\ I + \\frac{1}{2}(\\mu_a^T \\mu_a)$ para $\\alpha$ evanescentemente pequeño y $\\Sigma_i = \\alpha\\ I\\hspace{5mm} \\forall i\\neq a$.\n",
    "Tomar los vectores de secuencia $q_1$ y $q_2$ tal cual fueron diseñados en la parte (i).\n",
    "Cualitativamente, Qué salida $c$ esperas obtener a través de diferentes muestras de vectores clave? Explique brevemente por qué. Puedes ignorar los casos donde $q_i^T\\ k_a<0$.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### e. *self-attention* *Key-Query-Value* en redes neuronales\n",
    "Hasta ahora, hemos discutido la atención como una función sobre un conjunto de vectores clave, un conjunto de vectores con valores y un vector de secuencia. En los *Transoformers* realizamos *self-attention* que crudamente signfica que dibujamos las claves, valores y secuencias de los mismos datos. Con mayor precisión, sea $\\{x_1 \\dots x_n\\}$ una secuencia de vectores en $\\mathbb{R}^d$. Piense a cada $x_i$ como la representante de la palabra $i$ en la oración. Una forma de *self-attention* defina a las claves, valores y secuencias como sigue.\n",
    "Sean las matrices de parámetros $V,\\ K,\\ Q\\ \\in \\mathbb{R}^{d\\times d}$, luego:\n",
    "\n",
    "$\\begin{align}\n",
    "v_i &= V \\ x_i\\hspace{5mm} &&i \\in \\{1\\dots n\\}\\\\\n",
    "k_i &= K \\ x_i\\hspace{5mm} &&i \\in \\{1\\dots n\\}\\\\\n",
    "q_i &= Q \\ x_i\\hspace{5mm} &&i \\in \\{1\\dots n\\}\\\\\n",
    "\\end{align}$\n",
    "\n",
    "Luego obtenemos el vector de contexto para cada entrada $i$: Tenemos $c_i=\\sum_{j=1}^n \\alpha_{ij}\\ v_j$, donde $\\alpha_{ij}$ es definida por $\\alpha_{ij}=\\frac{exp(k_j^T\\ q_i)}{\\sum_{l=1}^n exp(k_l^T\\ q_i)}$ Observar que esto es un *single-headed* *self-attention*.\n",
    "En esta pregunta mostraremos cómo la atención *key-value-query* como esta le permite a la red utilizar diferentes características de los vectores de entrada $x_i$ en cómo define las claves, secuencias y valores. Intuitivamente esto le permite a las redes elegir diferentes características de $x_i$ para ser el \"contenido\" (vector de valores) contra aquello que utiliza para determinar \"donde mirar\" por contenido (claves y secuencias)\n",
    "\n",
    "i. (3 points) Primero considere si no tuviesemos la atención \"clave-secuencia-valor\". Para las claves, secuencias y valores utilizaremos directamente $x_i$, esto es $v_i=q_i=k_i=x_i$. Consideremos un conjunto específico de $x_i$, en particular, sean $u_a,\\ u_b,\\ u_c,\\ u_d$ vectores mutuamente ortogonales en $\\mathbb{R}^d$ cada uno con misma norma $||u_a|| = ||u_b|| = ||u_c|| = ||u_d|| = \\beta$, donde $\\beta$ es grande. Ahora sea nuestro $x_i$ tal que\n",
    "\n",
    "$\\begin{align}\n",
    "x_1 &=u_d + u_b\\\\\n",
    "x_2 &= u_a\\\\\n",
    "x_3 &= u_c + u_b\n",
    "\\end{align}$\n",
    "\n",
    "Si aplicamos *self-attention* con estos vectores, a qué vector se aproxima $c_2$? Será posible que $c_2$ se aproxime a $u_b$ añadiendo tanto a $u_d$ o $u_c$ a $x_2$? Explique por qué o por qué no.\n",
    "\n",
    "ii. (4 points) Ahora considere utilizar la atención *key-query-value* como se definió originalmente, utilizando las mismas definiciones para $x_1,\\ x_2$ y $x_3$ como en la parte (i), especifique las matrices $K,Q,V$ tal que $c_2\\approx u_b$ y $c_1 \\approx u_b-u_c$. Hay muchas soluciones a este problema, por lo que será fácil para tí. Si primero encuentras una $V$ tal que $v_1=u_b$ y $v_3=u_b-u_c$ luego trabajar en $Q$ y $K$.\n",
    "Algunas propiedades del producto externo podrían ser útiles.\n",
    "Para vectores ortogonales $u,v,w\\in \\mathbb{R}^d$ el producto externo $u\\ v^T$ es una matriz de $\\mathbb{R}^{d\\times d}$ y $(u\\ v^T)v = u(v^T\\ v)= u ||v||_2^2$ y $(u\\ v^T)w = u(v^T\\ w) = u·0$ (La última igualdad se debe a que $v$ y $w$ son ortogonales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}