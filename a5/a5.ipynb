{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GitHub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@markdown Cargar\n",
    "%cd ~\n",
    "%cd /content/\n",
    "uname = \"leinaxd\"\n",
    "!git config --global user.email '$uname@gmail.com'\n",
    "!git config --global user.name '$uname'\n",
    "\n",
    "from getpass import getpass\n",
    "password = getpass('Password:')\n",
    "!git clone https://$uname:$password@github.com/leinaxd/Tps/\n",
    "\n",
    "%cd Tps/a5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "commit = \"a5: Init\" #@param {type:\"string\"}\n",
    "\n",
    "!git add .\n",
    "!git commit -m \"$commit\"\n",
    "!git push"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@markdown pull / update\n",
    "!git pull"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CS224N Assignment 5: Self-Attention, Transformers and Pretraining (21 + 35 + 5 Points)\n",
    "\n",
    "---\n",
    "Nota. Hay diferentes cuestiones a tener a la hora de realizar este proyecto\n",
    "\n",
    "* Hay preguntas de deducciones matemáticas\n",
    "* La cantidad de código pytorch, y complejidad del código en este proyecto es mucho menor que en el proyecto 4. Sin embargo, se le dará menor seguimiento y menor andamiaje para escribir el código.\n",
    "* Este proyecto incluye un paso de pre-entrenamiento que toma aproximadamente 2 horas a realizar en Azure, y tendrás que escribirlo dos veces.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este proyecto se trata de una investigación sobre los bloques y los efectos del entrenamiento de *self-attention* y *Transformers*.\n",
    "Esto cubre las propiedades matemáticas mediante preguntas escritas.\n",
    "Además, obtendrás experiencia sobre la construcción de sistemas prácticos mediante el repropósito del *codebase*.\n",
    "\n",
    "El trabajo se divide en una parte matemática escrita y una parte de codificación, con su propio conjunto de preguntas escritas.\n",
    "\n",
    "Aquí un pequeño resumen\n",
    "\n",
    "1. **Exploración matematica**: Qué clase de operaciones puede implementar sencillamente *self-attention*? Por qué deberíamos utilizar cosas mas bellas como *multi-head self attention*?\n",
    "Esta sección utilizará cierta investigación matemática para iluminar algunas de las motivaciones de las redes de *self-attention* y *Transofmers*.\n",
    "2. **Extendiendo el codebase investigado**: En esta parte del trabajo, obtendrás cierta experiencia e intuición sobre las investigaciones más afiladas en NLP.\n",
    "Enseñar a los modelos NLP los hechos del mundo mediante *Pretraining* y acceso al conocimiento mediante un ajuste fino.\n",
    "Entrenarás un modelo *Transformer* para intentar responder preguntas sencillas de la forma *\"Donde ha nacido la persona [x]?\"* Sin proveer ningún texto de entrada del cual extraer la respuesta.\n",
    "Encontrarás que los modelos son capaces de aprender muchos Hechos sobre donde ha nacido las personas mediante el preentrenamiento y acceder a esta información durante el ajuste fino para responder estas preguntas.\n",
    "\n",
    "Luego observarás detenidamente al sistema construído y razonarás sobre las implicaciones y responsabilidades sobre tal conocimiento implícito."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Attention Exploration (21 points)\n",
    "El *self-attention Multi-Headed* es el componende del núcleo en los *Transformers*. En esta pregunta, obtendrás cierta práctica trabajando con las ecuaciones de *self-attention*, que motivarán a preguntarse Por qué *Muliti-Headed self-Attention* es preferible frente a *single-headed self-attention*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. Copiado en Atención (2 points)\n",
    "Recuerde que la atención, podría ser vista como una operación sobre una secuencia *Querry* $q\\in \\mathbb{R}^d$, un conjunto de vectores evaluados $\\{v_1\\dots v_n\\},\\hspace{5mm} v_i\\in \\mathbb{R}^d$ y un conjunto de vectores de dirección *Keys* $\\{k_1 \\dotsk_n\\},\\hspace{5mm}k_i\\in \\mathbb{R}^d$ especificado según:\n",
    "\n",
    "$\\begin{align}\n",
    "c&= \\sum_{i=1}^n v_i \\alpha_i\\\\\n",
    "\\alpha_i &= \\frac{exp(k_i^T\\ q)}{\\sum_{j=1}^n exp(k_j^T\\ q)}\n",
    "\\end{align}$\n",
    "\n",
    "Donde $\\alpha_i$ es comunmente llamado \"Los pesos de Atención\" y las salidas $c\\in \\mathbb{R}^d$ su correspondiente promedio ponderado sobre los valores de los vectores. <br>\n",
    "Primero mostraremos que para la *Atención* es particularmente simple \"Copiar\" un valor del vector de salida $c$.\n",
    "* Describa (en una oración) qué propiedades de la entrada de la operación de atención resulta en que la salida $c$ es aproximadamente igual a $v_j$ para algún $j\\in \\{1\\dots n\\}$.\n",
    "Específicamente, qué debe ser cierto sobre la secuencia $q$, los valores $\\{v_1\\dots v_n\\}$ y/o las *keys* $\\{k_1\\dots k_n\\}$?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}