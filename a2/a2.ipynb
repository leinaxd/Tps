{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS224N Assignment 2: word2vec (44 Points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1 Escrito (26 points)\n",
    "Refresquemos rápidamente el algoritmo word2vec. La intuición clave detrás de word2vec es que ‘Se conoce a una palabra por la compañia que tiene’.\n",
    "Concretamente, suponga que tenemos una palabra central $c$ y una ventana de contexto alrededor de $c$.\n",
    "\n",
    "Nos referiremos a las palabras contenidas en la ventana de contexto como las 'palabras de afuera'\n",
    "\n",
    "Por ejemplo, en la figura se observa que la palabra central $c$ es 'banking’. ya que el tamaño de la ventana de contexto es 2, las palabras de afuera son ‘turning’, ‘into’, ‘crises’, and ‘as’.\n",
    "El objetivo del algoritmo word2vec skip-gram es el de aprender precisamente la distribución de probabilidad $P(O|C)$\n",
    "\n",
    "Dada una palabra específica $o$ y una palabra específica $c$, queremos calcular $P(O=o|C=c)$, que es la probabilidad de que una palabra $o$ sea una palabra de afuera para $c$, ej. la probabilidad que $o$ caiga dentro de una ventana en el contexto de $c$\n",
    "\n",
    "![word2vec](./imgs/word2vec_prob.png)\n",
    "\n",
    "En word2vec, la distribución de probabilidad condicional es dada tomando el producto intero y aplicando la función softmax.\n",
    "\n",
    "$$ P(O = o | C = c) = \\frac{exp(u_o^T v_c)}{\\sum_{w\\in vocab}exp(u_w^T v_c)} $$ (1)\n",
    "\n",
    "Aquí, $u_o$ es el vector de 'afuera' representando la palabra 'o' de afuera, y $v_c$ es el vector del 'centro' representando la palabra del centro $c$.\n",
    "Para almacenar estos parámetros, definimos dos matrices, $U$ y $V$.\n",
    "Las columnas de $U$ son todos los vectores $u_w$ de 'afuera'.\n",
    "Las columnas de $V$ son todos los vectores $v_w$ del 'centro'.\n",
    "Ambos $U$ y $V$ contienen un vector para cada $w\\in Vocabulary$\n",
    "\n",
    "Recordar de las clases que, para un par de palabras $c$ y $o$ la función de costo está dada por:\n",
    "\n",
    "$$J_{naive-softmax}(v_c, o, U) = -log\\ P(O=o|C=c) $$ (2)\n",
    "\n",
    "Se puede ver este coste como la entropía cruzada entre la verdadera distribución de $y$ y la distribución estimada $\\hat y$.\n",
    "\n",
    "Recordar que la entropía cruzada entre una distribución de probabilidad verdadera $p$ y otra distribución $q$ para una variable discreta $x$ era:\n",
    "$$ H(p,q) = -\\sum_{i}p_i\\ log(q_i) $$ <br>\n",
    "Aquí, tanto $y$ como $\\hat y$ son vectores de igual longitud al número de palabras del vocabulario. Más allá, la componente $k^{esima}$ de estos vectores indican la probabilidad condicional de que la $k^{esima}$ palabra sea una palabra de 'afuera' dada la palabra 'c'.\n",
    "La verdadera distribución empírica de $y$ es un *one-hot vector* con un 1 para la verdadera palabra 'outside' y 0 en otro caso. La distribución estimada $\\hat y$ es la distribución de probabilidad $P(O|C = c)$ Dado nuestro modelo de la ecuación (1)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1a (3 points)\n",
    "Mostrar que la función de costo Naive-Softmax coincide con la función de coste de Cross-Entropy entre $y$ y $\\hat{y}$\n",
    "\n",
    "$$J=-\\sum_{w\\in Vocab} y_w\\ log\\ (\\hat y_w) = -log(\\hat y_w)$$\n",
    "\n",
    "La respuesta debe ser una línea"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "Respuesta\n",
    "\n",
    "</span>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1b (5 points)\n",
    "Calcular la derivada parcial de la función $J_{naive-softmax}(v_c, o, U)$ respecto a $v_c$.\n",
    "Escribir la respuesta en términos de $y$, $\\hat y$ y $U$.\n",
    "<br><br><br>\n",
    "\n",
    "·Se espera que las respuestas finales cumplan la **convención de dimensión**. Esto significa que la derivada parcial de cualquier función $f(x)$ respecto a $x$ debe tener la misma dimensión que $x$.\n",
    "\n",
    "·Presentar la respuestas en su **forma vectorizada**. (No deberías referirte a elementos específicos de $y$, $\\hat y$ o $U$ como $y_1, y_2, ...)\n",
    "\n",
    "Esto nos permite minimizar eficientemente una función sin preocuparnos por redimensionar las matrices al utilizar el gradiente descendiente.\n",
    "Utilizando esta convención, nos garantizamos que la regla de actualización\n",
    "$θ := θ − α \\frac{\\partial J(θ)}{\\partial \\theta}$ esté bien definida."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "Respuesta\n",
    "\n",
    "</span>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1c (5 points)\n",
    "Calcular la derivada parcial de $J_{naive-softmax}(v_c, o, U)$ respecto a cada word vector de salida $u_w$. <br>\n",
    "Habrá dos casos: cuando $w=o$ sea el verdadero vector de salida\n",
    "y cuando $w\\neq o$ para el resto de palabras\n",
    "\n",
    "·Escriba su respuesta en términos de $y$, $\\hat y$ y $v_c$. En esta parte podrás utilizar elementos específicos como $y_1$, $y_2$, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "Respuesta\n",
    "\n",
    "</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1d (1 point)\n",
    "Calcular la derivadad parcial de $J_{naive-softmax}(v_c,o,U)$ respecto a $U$\n",
    "\n",
    "·Escribir su respuesta en términos de $\\frac{\\partial J(v_c, o, U)}{\\partial u_1}$, $\\frac{\\partial J(v_c, o, U)}{\\partial u_2}$, ..., $\\frac{\\partial J(v_c, o, U)}{\\partial u_{|Vocab|}}$\n",
    "\n",
    "·La solución debe tener una o dos líneas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "Respuesta\n",
    "\n",
    "</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1e (3 Points)\n",
    "La función sigmoidea es<br>\n",
    "$$σ(x) = \\frac{1}{1 + e^{−x}} = \\frac{e^x}{e^x + 1} $$<br>\n",
    "Obtener la derivada de $σ(x)$ respecto a $x$, donde $x$ es un escalar.<br>\n",
    "Pista: Querrás reescribir tu respuesta en términos de $σ(x)$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "Respuesta\n",
    "\n",
    "</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1f (4 points)\n",
    "Ahora consideraremos la función de coste de **Negative Sampling** que es una alternativa a la función de coste Naive softmax.<br>\n",
    "Suponer que se toman del vocabulario $K$ muestras negativas (palabras).\n",
    "Por sencillez de notación, nos referiremos a ellas por $w_1, w_2, ..., w_K$\n",
    "Y a sus vectores de salida como $u_1, ..., u_K$.\n",
    "\n",
    "Para esta pregunta, suponer que las $K$ muestras son diferentes, en otras palabras, $w_i\\neq w_j$ para $i\\neq j, i,j\\in\\{1,...,K\\}$\n",
    "\n",
    "·Observar que para $o\\neq\\{w_1, ..., w_K\\}$. Para una palabra central $c$ y una palabra externa $o$, la función de costo de Negative-Sampling es\n",
    "\n",
    "$$ J_{neg-sample}(v_c, o, U) = -log(σ(u_o^T v_c))−\\sum_{k=1}^K log(σ(−u_k^T v_c)) $$\n",
    "\n",
    "* Repetir los ejercicios 1b y 1c, calculando las derivadas parciales de $J_{neg-sample}$ respecto a $v_c$, respecto a $u_o$ y respecto la muestra negativa $u_k$.<br>\n",
    "* Escribir las respuestas en términos de vectores de $u_o$, $v_c$ y $u_k$ donde $k\\in [1,K]$\n",
    "\n",
    "* Al finalizar, escribir en una oración, por qué esta función de coste es mucho más eficiente de calcular que la función Naive-Softmax.\n",
    "\n",
    "·Obs. Debes ser capaz de utilizar la solución del 1e para calcular los gradientes requeridos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "Respuesta\n",
    "\n",
    "</span>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1g (2 point)\n",
    "\n",
    "Repetiremos el ejercicio anterior, pero sin suponer que las $K$ muestras eran diferentes. Suponer que las $K$ muestras se tomaron del vocabulario.<br>\n",
    "·Por sencillez de notación, nos referiremos como $w_1, w_2,...,w_K$ y a sus vectores de salida como $u_1, ..., u_K$\n",
    "\n",
    "En esta pregunta, no puedes suponer que las palabras son diferentes, en otras palabras, $w_i=w_j$ podría ser cierto cuando $i=j$ lo sea. Notar que $o\\notin\\{w_1,...,w_K\\}$. Para una palabra central $c$ una palabra de salida $o$, la función de coste Negative-Sampling está dada por:<br>\n",
    "\n",
    "$$ J_{neg-sample}(v_c, o, U) = -log(σ(u_o^T v_c))−\\sum_{k=1}^K log(σ(−u_k^T v_c)) $$\n",
    "\n",
    "* Calcular la derivada parcial de $J_{neg-sample}$ respecto a la muestra negativa $u_k$.<br>\n",
    "* Escribir las respuestas en términos de vecotres $v_c$ y $u_k$, donde $k\\in[1,K]$.\n",
    "\n",
    "Pista: Partir la sumatoria de la función de costo en dos partes, la suma sobre todas las palabras muestreadas igual a $u_k$ y la suma sobre todas las palabras muestreadas distintas de $u_k$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "Respuesta\n",
    "\n",
    "</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 1h (3 points)\n",
    "Suponga que la palabra central es $c=w_t$ y la ventana de contexto es\n",
    "$[w_{t−m}, ..., w_{t−1}, w_t, w_{t+1}, ..., w_{t+m}]$, donde $m$ es el tamaño de la ventana. Recordar que para la versión skip-gram de word2vec, el costo total para la ventana de contexto es\n",
    "$$ J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U) = \\sum_{−m≤j≤m\\atop j\\neq 0} J(v_c, w_{t+j}, U) $$\n",
    "\n",
    "Donde $J(v_c, w_{t+j}, U)$ representa un coste arbitrario en términos de la palabra central $c=w_t$ y la palabra de salida $w{t+j}$.\n",
    " $J(v_c, w_{t+j}, U)$ podría ser $J_{naive-softmax}(v_c, w_{t+j}, U)$ o $J_{neg-sample}(v_c, w_{t+j}, U), dependiendo de tu implementación.\n",
    "\n",
    "Escribir las 3 derivadas parciales:\n",
    "1. $\\frac{\\partial}{\\partial U}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)$\n",
    "3. $\\frac{\\partial}{\\partial v_c}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)$\n",
    "3. $\\frac{\\partial}{\\partial v_w}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)$ cuando $w\\neq c$\n",
    "\n",
    "·Escribir las respuestas en términos de $\\frac{\\partial}{\\partial U}J(v_c, w_{t+j}, U)$ y $\\frac{\\partial}{\\partial v_c}J(v_c, w_{t+j}, U)$.\n",
    "Esto es muy sencillo, cada solución debe ser una línea."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "Respuesta\n",
    "\n",
    "</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PARTE 2 Código (18 points)\n",
    "En esta parte implementarás el modelo word2vec y entrenarás tus propios word vectors mediante Stochastic Gradient Descent (SGD).\n",
    "Antes de comenzar, se deben ejecutar las siguientes instrucciones dentro del directorio del proyecto con el fin de crear el apropiado entorno virtual para conda.\n",
    " Esto garantiza que tienes todos las librerías necesarias para completar esta parte.\n",
    "·Obs. Debes terminar la sección matemática anterior antes de intentar implementar el código ya que se pedirá implementar estas funciones matemáticas.\n",
    "También querrás implementar y testear la siguientes secciones en orden ya que son acumulativas.\n",
    "\n",
    "` conda env create -f env.yml `<br>\n",
    "` conda activate a2 `\n",
    "\n",
    "Al terminar con el ejercicio, puedes desactivas este entorno con <br>\n",
    "` conda deactivate `<br>\n",
    "\n",
    "Para cada método, en los comentarios hemos incluído aproximadamente la cantidad de líneas que nuestra solución tiene para que tomes de referencia.\n",
    " Los bucles for en python toman más tiempo en finalizar cuando se utilizan sobre arrays grandes, por eso esperamos que utilices los métodos de numpy. Estaremos controlando la eficiencia de tu código, serás capaz de ver los resultados del autograder cuando cargues el código al Gradescope. Se recomienda publicarlo temprana y continuamente."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 2a  (12 points)\n",
    "Empezaremos implementando los métodos dentro de **word2vec.py**\n",
    "\n",
    "Puedes testear un método $m$ particular ejecutando\n",
    "` python word2vec.py m `<br>\n",
    "Por ejemplo, puedes testear la función sigmoidea ejecutando: <br>\n",
    "` python word2vec.py sigmoid `\n",
    "\n",
    "Ejercicios:\n",
    "1. Implementar el método **sigmoid**, que recive un vector y le aplica la función sigmoidea.\n",
    "2. Implementar la función de coste softmax y gradiente en el método **naiveSoftmaxLossAndGradient**\n",
    "3. Implement the negative sampling loss and gradient in the negSamplingLossAndGradient\n",
    "method.\n",
    "4. Implementar el modelo skip-gram en el método **skipgram**.\n",
    "Al finalizar, verificar la implementación entera ejecutando <br>\n",
    "` python word2vec.py `"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#code 2a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 2b  (4 points)\n",
    "Complete la implementación para el optimizador SGD en el método **sgd.py**. Verificar tu implementación ejecutando <br>\n",
    "` python sgd.py `"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#code 2b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTE 2c (2 points)\n",
    "Hora de Exposición! Es la hora de cargar datos reales y entrenar los word vectors con todo ya implementado! Utilizaremos el DataSet **Stanford Sentiment Treebank (SST)** para entrenar los word vectors, y luego aplicarlos en una simple tarea de análisis del sentimiento.<br>\n",
    "\n",
    "Para obtener el dataset, ejecutar:<br>\n",
    "` sh get datasets.sh `<br>\n",
    " No hay código adicional para escribir en esta parte, solo ejecutar: <br>\n",
    "` python run.py `\n",
    "\n",
    "Obs.: El proceso de entrenamiento podría llevar mucho tiempo dependiendo de la eficiencia de tu implementación y el poder de cálculo de tu computadora (a una implementación eficiente le lleva entre 1 y 2 horas). Planificar acorde!\n",
    "\n",
    "Luego de 40,000 iteraciones, el script terminará y aparecerá un gráfico para tus word vectors.\n",
    "Además, se guardará como **word_vectors.png** en tu directorio. Incluir el gráfico en tu documento escrito, explicando brevemente al menos 3 oraciones que veas en el gráfico."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#code 2c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}