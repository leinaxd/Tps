{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "pycharm-aa490f0e",
   "language": "python",
   "display_name": "PyCharm (Tps)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "a2.ipynb",
   "provenance": [],
   "include_colab_link": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/leinaxd/Tps/blob/master/a2/a2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "h6YRuAp6lMmC"
   },
   "source": [
    "# CS224N Assignment 2: word2vec (44 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KC_HTTbclMmH"
   },
   "source": [
    "## PARTE 1 Escrito (26 points)\n",
    "Refresquemos rápidamente el algoritmo word2vec. La intuición clave detrás de word2vec es que ‘Se conoce a una palabra por la compañia que tiene’.\n",
    "\n",
    "Nos referiremos a la palabra central como $c$ y a las palabras vecinas a ella en la ventana de contexto como $o$. Por ejemplo, en la figura se observa que la palabra central $c$ es 'banking’. ya que el tamaño de la ventana de contexto es 2, las palabras de afuera son ‘turning’, ‘into’, ‘crises’, and ‘as’.\n",
    "\n",
    "El objetivo del algoritmo word2vec skip-gram es el de aprender la distribución de probabilidad $P(O=o|C=c)$.  $P(O|C)$ es la probabilidad de que una palabra sea del contexto $o$ para $c$.\n",
    "\n",
    "![word2vec](https://github.com/leinaxd/Tps/blob/master/a2/imgs/word2vec_prob.png?raw=1)\n",
    "\n",
    "En word2vec, la distribución de probabilidad condicional esta definida con el producto intero y la función softmax.\n",
    "\n",
    "$$ P(O = o | C = c) = \\frac{exp(u_o^T v_c)}{\\sum_{w\\in vocab}exp(u_w^T v_c)} $$ (1)\n",
    "\n",
    "Aquí, $u_o\\in \\rm I\\!R^{K}$ es el vector de 'la ventana de contexto' representando la palabra 'o' de la ventana, y $v_c\\in \\rm I\\!R^{K}$ es el vector del 'centro' representando la palabra del centro $c$.\n",
    "Para almacenar estos parámetros, definimos dos matrices, $U\\in \\rm I\\!R^{K\\times |V|}$ y $V\\in \\rm I\\!R^{K\\times |V|}$.\n",
    "Las columnas de $U$ son todos los vectores $u_w$ del 'contexto'.\n",
    "Las columnas de $V$ son todos los vectores $v_w$ del 'centro'.\n",
    "Ambos $U$ y $V$ contienen un vector para cada $w\\in Vocabulary$\n",
    "\n",
    "Recordar de las clases que, para un par de palabras $c$ y $o$ la función de costo está dada por:\n",
    "\n",
    "$$J_{naive-softmax}(v_c, o, U) = -log\\ P(O=o|C=c) $$ (2)\n",
    "\n",
    "Además, la entropía cruzada entre una distribución de probabilidad verdadera $p$ y otra distribución $q$ para una variable discreta $x$ era:\n",
    "\n",
    "$$ H(p,q) = -\\sum_{x\\in X}p(x)\\ log(q(x)) $$ <br>\n",
    "\n",
    "Se puede ver este coste como la entropía cruzada entre la verdadera distribución de $y$ y la distribución estimada $\\hat y$.\n",
    "Aquí, tanto $y$ como $\\hat y$ son vectores de igual longitud al número de palabras del vocabulario. Además, la componente $k^{esima}$ de estos vectores indican la probabilidad condicional de que la $k^{esima}$ palabra sea una palabra de 'contexto' dada la palabra 'c'.\n",
    "La verdadera distribución empírica de $y$ es un *one-hot vector* con un 1 para la verdadera palabra 'de contexto' y 0 en otro caso. La distribución estimada $\\hat y$ es la distribución de probabilidad $P(O|C = c)$ Dado nuestro modelo de la ecuación (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "MTgh08NElMmK"
   },
   "source": [
    "## PARTE 1a (3 points)\n",
    "Mostrar que la función de costo Naive-Softmax coincide con la función de coste de Cross-Entropy entre $y$ y $\\hat{y}$\n",
    "\n",
    "$$J=-\\sum_{w\\in Vocab} y_w\\ log\\ (\\hat y_w) = -log(\\hat y_o)$$\n",
    "\n",
    "La respuesta debe ser una línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "bbEMc2vSlMmL"
   },
   "source": [
    "<font color=\"green\">Respuesta:<br>\n",
    "$\\begin{align}\n",
    "H(P,\\hat P) &= -\\sum_{w\\in vocab} P(O|C)\\ log(\\hat P(O|C))  &&\n",
    "\\small\\text{La prob. que una palabra sea del contexto de c}\\\\\n",
    "H(y,\\hat y)&= -\\sum_{i=1\\atop \\text{Coord. vector}}^K y_i\\ log(\\hat y_i)&&\n",
    "\\small\\text{Representar en One-Hot encoding no cambia la prob.}\\\\\n",
    "&=-1_{i = i_o}\\ log(\\hat y_o) - 0_{i\\neq i_o}\\ log(\\hat y_i)&&\n",
    "\\small\\text{One-Hot = 1 para la componente del vector correcta}\\\\\n",
    "&=-log(\\hat y_o)\n",
    "\\end{align}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "TqTqtyTOlMmM"
   },
   "source": [
    "## PARTE 1b (5 points)\n",
    "Calcular la derivada parcial de la función $J_{naive-softmax}(v_c, o, U)$ respecto a $v_c$.\n",
    "Escribir la respuesta en términos de $y$, $\\hat y$ y $U$.\n",
    "<br><br><br>\n",
    "\n",
    "·Se espera que las respuestas finales cumplan la **convención de dimensión**. Esto significa que la derivada parcial de cualquier función $f(x)$ respecto a $x$ debe tener la misma dimensión que $x$.\n",
    "\n",
    "·Presentar la respuestas en su **forma vectorizada**. (No deberías referirte a elementos específicos de $y$, $\\hat y$ o $U$ como $y_1$, $y_2$, ...)\n",
    "\n",
    "Esto nos permite minimizar eficientemente una función sin preocuparnos por redimensionar las matrices al utilizar el gradiente descendiente.\n",
    "Utilizando esta convención, nos garantizamos que la regla de actualización\n",
    "$θ := θ − α \\frac{\\partial J(θ)}{\\partial \\theta}$ esté bien definida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "sVbdehszlMmO"
   },
   "source": [
    "<font color=\"green\">Respuesta: <br>\n",
    "$\\begin{align}\n",
    "J_{naive-softmax}(v_c, o, U) &= -log\\ \\hat P(O=o|C=c)\\\\\n",
    "&=-log\\ \\frac{exp(u_o^T v_c)}{\\sum_{w\\in vocab}exp(u_w^T v_c)}\\\\\n",
    "&= -u_o^T v_c\\ +\\ log\\ \\sum_{w\\in vocab}exp(u_w^T v_c)\n",
    "\\end{align}$\n",
    "<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial v_c} J_{naive-softmax}(v_c, o, U)\n",
    "&= \\frac{\\partial}{\\partial v_c} -u_o^T v_c\\ + \\ \\frac{\\partial}{\\partial v_c}\\ log \\sum_{w\\in vocab}exp(u_w^T v_c)\\\\\n",
    "&=-u_o\\ +\\ \\frac{1}{\\sum_{w\\in vocab}exp(u_w^T v_c)}\\sum_{w\\in vocab}\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial v_c^1}exp(u_w^T v_c)\\\\\n",
    "    \\vdots\\\\\n",
    "    \\frac{\\partial}{\\partial v_c^K}exp(u_w^T v_c)\\\\\n",
    "\\end{bmatrix} \\\\\\\n",
    "&=-u_o\\ +\\ \\frac{1}{\\sum_{w\\in vocab}exp(u_w^T v_c)}\\sum_{w\\in vocab}\n",
    "\\begin{bmatrix}\n",
    "    exp(u_w^T v_c)\\ ·\\ u_w^1\\\\\n",
    "    \\vdots\\\\\n",
    "    exp(u_w^T v_c)\\ ·\\ u_w^K\\\\\n",
    "\\end{bmatrix} \\\\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\\begin{align}\n",
    "&=-u_o\\ +\\ \\frac{\n",
    "\\sum_{w\\in vocab}\n",
    "exp(u_w^T v_c)\\ ·\\ u_w\n",
    "}{\\sum_{w\\in vocab}exp(u_w^T v_c)}\\\\\\\n",
    "&=-u_o\\ +\\ \\sum_{w\\in vocab} u_w \\frac{\n",
    "exp(u_w^T v_c)\n",
    "}{\\sum_{w'\\in vocab}exp(u_{w'}^T v_c)}\\\\\\\n",
    "&=-u_o\\ +\\\n",
    "\\underbrace{U}_{\\in \\mathbb{R}^{K\\times |V|}} ·\\\n",
    "\\underbrace{softmax(U^T v_c)}_{\\in \\mathbb{R}^{|V|\\times 1}}\\\\\n",
    "&=-U·y + U · \\hat y\\\\\n",
    "&=U · (\\hat y - y)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Donde La función Softmax es:\n",
    "\n",
    "$\\begin{align}\n",
    "\\sigma (\n",
    "    \\begin{bmatrix}\n",
    "        z_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        z_K\n",
    "    \\end{bmatrix} )=\n",
    "    \\begin{bmatrix}\n",
    "        \\frac {z_1}{\\sum_{j=1}^K exp(z_j)} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac {z_K}{\\sum_{j=1}^K exp(z_j)}\n",
    "    \\end{bmatrix}\n",
    "\\end{align}$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$\\hat y = \\begin{bmatrix}\n",
    "            \\hat y_1 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\hat y_K\n",
    "         \\end{bmatrix} = softmax(U^T v_c) = softmax(U^T V x_{one-hot})$\n",
    "\n",
    "$y = Enc_{one-hot} (w_i)$ (Muestras supervisadas)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "DJzivCIjlMmQ"
   },
   "source": [
    "## PARTE 1c (5 points)\n",
    "Calcular la derivada parcial de $J_{naive-softmax}(v_c, o, U)$ respecto a cada word vector de salida $u_w$. <br>\n",
    "Habrá dos casos: cuando $w=o$ sea el verdadero vector de salida\n",
    "y cuando $w\\neq o$ para el resto de palabras\n",
    "\n",
    "·Escriba su respuesta en términos de $y$, $\\hat y$ y $v_c$. En esta parte podrás utilizar elementos específicos como $y_1$, $y_2$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Aj6RqMeZlMmR"
   },
   "source": [
    "<font color=\"green\">Respuesta:\n",
    "<br>\n",
    "$J_{naive-softmax}(v_c, o, U) = -u_o^T v_c\\ +\\ log\\ \\sum_{w\\in vocab}exp(u_w^T v_c)$\n",
    "<br>\n",
    "Para $w = o$:\n",
    "<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial u_o} J_{naive-softmax}(v_c, o, U)\n",
    "&= \\frac{\\partial}{\\partial u_o} -u_o^T v_c\\ + \\ \\frac{\\partial}{\\partial u_o}\\ log \\sum_{w\\in vocab}exp(u_w^T v_c)\\\\\n",
    "&= -v_c\\ + \\frac{1}{\\sum_{w\\in vocab}exp(u_w^T v_c)}\n",
    "\\ \\frac{\\partial}{\\partial u_o}\\ \\sum_{w\\in vocab} exp(u_w^T v_c)\\\\\n",
    "&= -v_c\\ + \\frac{1}{\\sum_{w\\in vocab}exp(u_w^T v_c)}\n",
    "\\ \\frac{\\partial}{\\partial u_o}\\ exp(u_o^T v_c)\\\\\n",
    "&= -v_c\\ + \\frac{1}{\\sum_{w\\in vocab}exp(u_w^T v_c)}\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial u_o^1}\\ exp(u_o^T v_c)\\\\\n",
    "    \\vdots\\\\\n",
    "    \\frac{\\partial}{\\partial u_o^K}\\ exp(u_o^T v_c)\\\\\n",
    "\\end{bmatrix}\\\\\\\n",
    "&= -v_c\\ + \\frac{1}{\\sum_{w\\in vocab}exp(u_w^T v_c)}\n",
    "\\begin{bmatrix}\n",
    "    exp(u_o^T v_c)\\ v_c^1\\\\\n",
    "    \\vdots\\\\\n",
    "    exp(u_o^T v_c)\\ v_c^K\\\\\n",
    "\\end{bmatrix}\\\\\\\n",
    "&= -v_c\\ + \\frac{ exp(u_o^T v_c)\\ v_c }{\\sum_{w\\in vocab}exp(u_w^T v_c)}\\\\\\\n",
    "&= (softmax^T(U^T v_c)·y_o-1)v_c\\\\\\\n",
    "&= (\\hat y^T·y-1)v_c\\\\\\\n",
    "\\end{align}$\n",
    "<br>\n",
    "Para $w \\neq o$:\n",
    "<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial u_w} J_{naive-softmax}(v_c, o, U)\n",
    "&= \\frac{\\partial}{\\partial u_w} -u_o^T v_c\\ + \\ \\frac{\\partial}{\\partial u_w}\\ log \\sum_{w'\\in vocab}exp(u_{w'}^T v_c)\\\\\n",
    "&= \\frac{1}{\\sum_{w'\\in vocab}exp(u_{w'}^T v_c)} \\frac{\\partial}{\\partial u_w}\\ \\sum_{w'\\in vocab}exp(u_{w'}^T v_c)\\\\\n",
    "&= \\frac{1}{\\sum_{w'\\in vocab}exp(u_{w'}^T v_c)} \\frac{\\partial}{\\partial u_w}\\ exp(u_w^T v_c)\\\\\n",
    "&= \\frac{exp(u_w^T v_c)\\ v_c}{\\sum_{w'\\in vocab}exp(u_{w'}^T v_c)}\\\\\n",
    "&= softmax^T(U^T\\ v_c)·y_w\\ v_c\\\\\n",
    "&= \\hat y^T·y_w\\ v_c\\\\\n",
    "\\end{align}$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KNiFACG6lMmT"
   },
   "source": [
    "## PARTE 1d (1 point)\n",
    "Calcular la derivadad parcial de $J_{naive-softmax}(v_c,o,U)$ respecto a $U$\n",
    "\n",
    "·Escribir su respuesta en términos de $\\frac{\\partial J(v_c, o, U)}{\\partial u_1}$, $\\frac{\\partial J(v_c, o, U)}{\\partial u_2}$, ..., $\\frac{\\partial J(v_c, o, U)}{\\partial u_{|Vocab|}}$\n",
    "\n",
    "·La solución debe tener una o dos líneas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "4MiEfqVDlMmU"
   },
   "source": [
    "<font color=\"green\">Respuesta:<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial U} J_{naive-softmax}\n",
    "&=\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial u_1} J &\n",
    "    \\frac{\\partial}{\\partial u_2} J &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial}{\\partial u_o} J &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial}{\\partial u_{|V|}}\n",
    "  \\end{bmatrix}\\\\\\\n",
    "&=\\begin{bmatrix}\n",
    "     \\frac{exp(u_1^T v_c)\\ v_c}{\\sum_{w'\\in vocab}exp(u_{w'}^T v_c)} &\n",
    "     \\cdots\n",
    "     (-v_c\\ + \\frac{ exp(u_o^T v_c)\\ v_c }{\\sum_{w\\in vocab}exp(u_w^T v_c)})&\n",
    "     \\cdots &\n",
    "     \\frac{exp(u_{|V|}^T v_c)\\ v_c}{\\sum_{w'\\in vocab}exp(u_{w'}^T v_c)}\n",
    "  \\end{bmatrix}\\\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "     0&\n",
    "     \\cdots\n",
    "     -v_c&\n",
    "     \\cdots &\n",
    "     0\n",
    "\\end{bmatrix}+v_c\n",
    "\\odot\n",
    "\\ \\underbrace{softmax^T(U^T ·v_c)}_{\\mathbb{R}^{1\\times |V|}}\n",
    "\\end{align}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fKfD0kKPlMmV"
   },
   "source": [
    "## PARTE 1e (3 Points)\n",
    "La función sigmoidea es<br>\n",
    "$$σ(x) = \\frac{1}{1 + e^{−x}} = \\frac{e^x}{e^x + 1} $$<br>\n",
    "Obtener la derivada de $σ(x)$ respecto a $x$, donde $x$ es un escalar.<br>\n",
    "Pista: Querrás reescribir tu respuesta en términos de $σ(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "t39IKDWPlMmV"
   },
   "source": [
    "<font color=\"green\">Respuesta:<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial x} \\sigma (x)\n",
    "&=\\frac{e^x}{e^x+1}-\\frac{e^x\\ e^x}{(e^x+1)^2}\\\\\n",
    "&=\\frac{e^{x}}{e^x+1}\\ ·\\ (1-\\frac{e^x}{e^x+1})\\\\\n",
    "&=\\sigma(x)\\ ·\\ (1-\\sigma(x))\n",
    "\\end{align}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "PPP9IUyDlMmW"
   },
   "source": [
    "## PARTE 1f (4 points)\n",
    "Ahora consideraremos la función de coste de **Negative Sampling** que es una alternativa a la función de coste Naive softmax.<br>\n",
    "Suponer que se toman del vocabulario $\\tilde K$ muestras negativas (palabras).\n",
    "Por sencillez de notación, nos referiremos a ellas por $\\tilde w_1, \\tilde w_2, ..., \\tilde w_K$\n",
    "Y a sus vectores del decodificador (contexto) como $\\tilde u_1, ..., \\tilde u_K$.\n",
    "\n",
    "Para esta pregunta, suponer que las $\\tilde K$ muestras son diferentes, en otras palabras, $\\tilde w_i\\neq \\tilde w_j$ para $i\\neq j \\wedge i,j\\in\\{1,...,K\\}$. Observar que $o\\neq\\{\\tilde w_1, ..., \\tilde w_K\\}$.\n",
    "\n",
    "Para una palabra central $c$ y una palabra externa $o$, la función de costo de Negative-Sampling es\n",
    "\n",
    "$$ J_{neg-sample}(v_c, o, U) = -log(σ(u_o^T v_c))−\\sum_{k=1}^{\\tilde K} log(σ(−\\tilde u_k^T v_c)) $$\n",
    "\n",
    "* Repetir los ejercicios 1b y 1c, calculando las derivadas parciales de $J_{neg-sample}$ respecto a $v_c$, respecto a $u_o$ y respecto la muestra negativa $\\tilde u_k$.<br>\n",
    "* Escribir las respuestas en términos de vectores de $u_o$, $v_c$ y $\\tilde u_k$ donde $k\\in [1,\\tilde K]$\n",
    "\n",
    "* Al finalizar, escribir en una oración, por qué esta función de coste es mucho más eficiente de calcular que la función Naive-Softmax.\n",
    "\n",
    "·Obs. Debes ser capaz de utilizar la solución del 1e para calcular los gradientes requeridos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MB-GVmXSlMmX"
   },
   "source": [
    "<font color=\"green\">Respuesta:<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial v_c} J_{neg.\\ sample}(v_c,o,U)\n",
    "&=- \\frac{\\partial}{\\partial v_c}\n",
    " log(\\sigma(u_o^T v_c))\n",
    " -\\frac{\\partial}{\\partial v_c}\n",
    " \\sum_{k=1}^{\\tilde K} log(\\sigma(-\\tilde u_k^T v_c))\\\\\n",
    "&=- \\frac{1}{\\sigma(u_o^T v_c)}\n",
    " \\frac{\\partial}{\\partial v_c}\n",
    " \\sigma(u_o^T v_c)\n",
    " -\\sum_{k=1}^{\\tilde K}\n",
    " \\frac{\\partial}{\\partial v_c}\n",
    " log(\\sigma(-\\tilde u_k^T v_c))\\\\\n",
    "&=- \\frac{\\sigma(u_o^T v_c) · (1 - \\sigma(u_o^T v_c))}\n",
    " {\\sigma(u_o^T v_c)}\n",
    " \\frac{\\partial}{\\partial v_c}\n",
    " u_o^T v_c\n",
    " -\\sum_{k=1}^{\\tilde K}\n",
    " \\frac{1}{\\sigma(-\\tilde u_k^T v_c)}\n",
    " \\frac{\\partial}{\\partial v_c}\n",
    " \\sigma(-\\tilde u_k^T v_c)\\\\\n",
    "&=- (1 - \\sigma(u_o^T v_c))\\ u_o\n",
    " -\\sum_{k=1}^{\\tilde K}\n",
    " \\frac{ \\sigma(-\\tilde u_k^T v_c) (1-\\sigma(-\\tilde u_k^T v_c))}{\\sigma(-\\tilde u_k^T v_c)}\n",
    " \\frac{\\partial}{\\partial v_c}\n",
    " -\\tilde u_k^T v_c\\\\\n",
    "&=- (1 - \\sigma(u_o^T v_c))\\ u_o\n",
    " +\\sum_{k=1}^{\\tilde K}\n",
    " (1-\\sigma(-\\tilde u_k^T v_c)) \\tilde u_k\\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial u_o} J_{neg.\\ sample}(v_c,o,U)\n",
    "&=- \\frac{\\partial}{\\partial u_o}\n",
    " log(\\sigma(u_o^T v_c))\n",
    " -\\frac{\\partial}{\\partial u_o}\n",
    " \\sum_{k=1}^{\\tilde K} log(\\sigma(-\\tilde u_k^T v_c))\\\\\n",
    "&=-\\frac{1}{\\sigma(u_o^T v_c)}\n",
    " \\frac{\\partial}{\\partial u_o} \\sigma(u_o^T v_c) -0\\\\\n",
    "&=-\\frac{\\sigma(u_o^T v_c)(1-\\sigma(u_o^T v_c))}{\\sigma(u_o^T v_c)}\n",
    " \\frac{\\partial}{\\partial u_o} (u_o^T v_c)\\\\\n",
    "&=-(1-\\sigma(u_o^T v_c))\\ v_c\\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\tilde u_k} J_{neg.\\ sample}(v_c,o,U)\n",
    "&=- \\frac{\\partial}{\\partial \\tilde u_k}\n",
    " log(\\sigma(u_o^T v_c))\n",
    " -\\frac{\\partial}{\\partial \\tilde u_k}\n",
    " \\sum_{k'=1}^{\\tilde K} log(\\sigma(-\\tilde u_{k'}^T v_c))\\\\\n",
    "&=- 0 - \\frac{\\partial}{\\partial \\tilde u_k}\n",
    " log(\\sigma(-\\tilde u_{k}^T v_c))\\\\\n",
    "&= - \\frac{1}{\\sigma(-\\tilde u_k^T v_c)}\n",
    " \\frac{\\partial}{\\partial \\tilde u_k} \\sigma(-\\tilde u_k^T v_c)\\\\\n",
    "&= -\\frac{\\sigma(-\\tilde u_k^T v_c)(1-\\sigma(-\\tilde u_k^T v_c))}{\\sigma(-\\tilde u_k^T v_c)}\n",
    " \\frac{\\partial}{\\partial \\tilde u_k} (-\\tilde u_k^T v_c)\\\\\n",
    "&= (1-\\sigma(-\\tilde u_k^T v_c))\\ v_c\\\\\n",
    "\\end{align}$\n",
    "</font>\n",
    "\n",
    "<font color=\"00B000\">Es más eficiente que Naive Softmax en la evaluación de los gradientes, ya que antes la sumatoria iteraba sobre todo el vocabulario y ahora con Negative Sampling, iteramos sobre K muestras</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bkNbf18zlMmY"
   },
   "source": [
    "## PARTE 1g (2 point)\n",
    "\n",
    "Repetiremos el ejercicio anterior, pero sin suponer que las $\\tilde K$ muestras eran diferentes.\n",
    "·Por sencillez de notación, nos referiremos como $\\tilde w_1, \\tilde w_2,...,\\tilde w_K$ y a sus vectores como $\\tilde u_1, ..., \\tilde u_K$\n",
    "\n",
    "En esta pregunta, no se puede suponer que las palabras sean diferentes, en otras palabras, $w_i=w_j$ podría ser cierto cuando $i=j$ lo sea. Notar que $o\\notin\\{w_1,...,w_K\\}$. Para una palabra central $c$ una palabra de salida $o$, la función de coste Negative-Sampling está dada por:<br>\n",
    "\n",
    "$$ J_{neg-sample}(v_c, o, U) = -log(σ(u_o^T v_c))−\\sum_{k=1}^K log(σ(−u_k^T v_c)) $$\n",
    "\n",
    "* Calcular la derivada parcial de $J_{neg-sample}$ respecto a la muestra negativa $\\tilde u_k$.<br>\n",
    "* Escribir las respuestas en términos de vecotres $v_c$ y $\\tilde u_k$, donde $k\\in[1,K]$.\n",
    "\n",
    "Pista: Partir la sumatoria de la función de costo en dos partes, la suma sobre todas las palabras muestreadas igual a $\\tilde u_k$ y la suma sobre todas las palabras muestreadas distintas de $\\tilde u_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "eKc1mrNqlMmZ"
   },
   "source": [
    "<font color=\"green\">Respuesta:<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\tilde u_k} J_{neg.\\ sample}(v_c,o,U)\n",
    "&=- \\frac{\\partial}{\\partial \\tilde u_k}\n",
    " log(\\sigma(u_o^T v_c))\n",
    " -\\frac{\\partial}{\\partial \\tilde u_k}\n",
    " \\sum_{k'=1}^{\\tilde K} log(\\sigma(-\\tilde u_{k'}^T v_c))\\\\\n",
    "&=-0 -\\frac{\\partial}{\\partial \\tilde u_k} \\sum_{k'=1}^{\\tilde K}\n",
    " (\\delta (u_k=u_{k'})+\\delta(u_k\\neq u_{k'}))\n",
    " log(\\sigma(-\\tilde u_{k'}^T v_c))\\\\\n",
    "&=-\\frac{\\partial}{\\partial \\tilde u_k} \\sum_{k'=1}^{\\tilde K}\n",
    " \\delta(u_k=u_{k'})\n",
    " log(\\sigma(-\\tilde u_{k'}^T v_c))\n",
    " -\\frac{\\partial}{\\partial \\tilde u_k} \\sum_{k'=1}^{\\tilde K}\n",
    " \\delta(u_k\\neq u_{k'})\n",
    " log(\\sigma(-\\tilde u_{k'}^T v_c))\\\\\n",
    "&=-\\sum_{k'=1}^{\\tilde K}\n",
    " \\delta(u_k=u_{k'})\n",
    " \\frac{\\partial}{\\partial \\tilde u_k}\n",
    " log(\\sigma(-\\tilde u_k^T v_c))-0\\\\\n",
    "&= -\\sum_{k'=1}^{\\tilde K}\n",
    " \\delta(u_k=u_{k'})\n",
    " \\frac{1}{\\sigma(-\\tilde u_k^T v_c)}\n",
    " \\frac{\\partial}{\\partial \\tilde u_k} \\sigma(-\\tilde u_k^T v_c)\\\\\n",
    "&= -\\sum_{k'=1}^{\\tilde K}\n",
    " \\delta(u_k=u_{k'})\n",
    " \\frac{\\sigma(-\\tilde u_k^T v_c)(1-\\sigma(-\\tilde u_k^T v_c))}{\\sigma(-\\tilde u_k^T v_c)}\n",
    " \\frac{\\partial}{\\partial \\tilde u_k} (-\\tilde u_k^T v_c)\\\\\n",
    "&= \\sum_{k'=1}^{\\tilde K}\n",
    " \\delta(u_k=u_{k'})\n",
    " (1-\\sigma(-\\tilde u_k^T v_c))\\ v_c\\\\\n",
    "&= (1-\\sigma(-\\tilde u_k^T v_c))\\ v_c\\ (\\sum_{k'=1}^{\\tilde K}\n",
    " \\delta(u_k=u_{k'}))\\\\\n",
    "\\end{align}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8JewiOPylMma"
   },
   "source": [
    "## PARTE 1h (3 points)\n",
    "Suponga que la palabra central es $c=w_t$ y la ventana de contexto es\n",
    "$[w_{t−m}, ..., w_{t−1}, w_t, w_{t+1}, ..., w_{t+m}]$, donde $m$ es el tamaño de la ventana. Recordar que para la versión skip-gram de word2vec, el costo total para esa ventana de contexto es\n",
    "$$ J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U) = \\sum_{−m≤j≤m\\atop j\\neq 0} J(v_c, w_{t+j}, U) $$\n",
    "\n",
    "Donde $J(v_c, w_{t+j}, U)$ representa un coste arbitrario en términos de la palabra central $c=w_t$ y la palabra de ventana $w{t+j}$.<br>\n",
    " $J(v_c, w_{t+j}, U)$ podría ser $J_{naive-softmax}(v_c, w_{t+j}, U)$ o $J_{neg-sample}(v_c, w_{t+j}, U)$, dependiendo de tu implementación.\n",
    "\n",
    "Escribir las 3 derivadas parciales:\n",
    "1. $\\frac{\\partial}{\\partial U}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)$\n",
    "3. $\\frac{\\partial}{\\partial v_c}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)$\n",
    "3. $\\frac{\\partial}{\\partial v_w}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)$ cuando $w\\neq c$\n",
    "\n",
    "·Escribir las respuestas en términos de $\\frac{\\partial}{\\partial U}J(v_c, w_{t+j}, U)$ y $\\frac{\\partial}{\\partial v_c}J(v_c, w_{t+j}, U)$.\n",
    "Esto es muy sencillo, cada solución debe ser una línea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "azKPDySUlMma"
   },
   "source": [
    "<font color=\"green\">Respuesta<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial U}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)\n",
    "&=\\frac{\\partial}{\\partial U}\n",
    "\\sum_{-m\\le j\\le m\\atop j\\neq 0} J(v_c, w_t+j, U)\\\\\n",
    "&=\\sum_{-m\\le j\\le m\\atop j\\neq 0}\n",
    "\\frac{\\partial}{\\partial U} J(v_c, w_t+j, U)\\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial v_c}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)\n",
    "&=\\frac{\\partial}{\\partial v_c}\n",
    "\\sum_{-m\\le j\\le m\\atop j\\neq 0} J(v_c, w_t+j, U)\\\\\n",
    "&=\\sum_{-m\\le j\\le m\\atop j\\neq 0}\n",
    "\\frac{\\partial}{\\partial v_c} J(v_c, w_t+j, U)\\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial}{\\partial v_w}J_{skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U)\n",
    "&=\\frac{\\partial}{\\partial v_w}\n",
    "\\sum_{-m\\le j\\le m\\atop j\\neq 0} J(v_c, w_t+j, U)\\\\\n",
    "&=\\sum_{-m\\le j\\le m\\atop j\\neq 0}\n",
    "\\frac{\\partial}{\\partial v_w} J(v_c, w_t+j, U)\\\\\n",
    "&=0\n",
    "\\end{align}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "E0E0OzBtlMmb"
   },
   "source": [
    "# PARTE 2 Código (18 points)\n",
    "En esta parte implementarás el modelo word2vec y entrenarás tus propios word vectors mediante Stochastic Gradient Descent (SGD).\n",
    "Antes de comenzar, se deben ejecutar las siguientes instrucciones dentro del directorio del proyecto con el fin de crear el apropiado entorno virtual para conda.\n",
    " Esto garantiza que tienes todos las librerías necesarias para completar esta parte.\n",
    "·Obs. Debes terminar la sección matemática anterior antes de intentar implementar el código ya que se pedirá implementar estas funciones matemáticas.\n",
    "También querrás implementar y testear la siguientes secciones en orden ya que son acumulativas.\n",
    "\n",
    "` conda env create -f env.yml `<br>\n",
    "` conda activate a2 `\n",
    "\n",
    "Al terminar con el ejercicio, puedes desactivas este entorno con <br>\n",
    "` conda deactivate `<br>\n",
    "\n",
    "Para cada método, en los comentarios hemos incluído aproximadamente la cantidad de líneas que nuestra solución tiene para que tomes de referencia.\n",
    " Los bucles for en python toman más tiempo en finalizar cuando se utilizan sobre arrays grandes, por eso esperamos que utilices los métodos de numpy. Estaremos controlando la eficiencia de tu código, serás capaz de ver los resultados del autograder cuando cargues el código al Gradescope. Se recomienda publicarlo temprana y continuamente."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VqAnEo8RlMmb"
   },
   "source": [
    "#Imports"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "z9mkapetlMmc"
   },
   "source": [
    "## PARTE 2a  (12 points)\n",
    "Empezaremos implementando los métodos dentro de **word2vec.py**\n",
    "\n",
    "Puedes testear un método $m$ particular ejecutando\n",
    "` python word2vec.py m `<br>\n",
    "Por ejemplo, puedes testear la función sigmoidea ejecutando: <br>\n",
    "` python word2vec.py sigmoid `\n",
    "\n",
    "Ejercicios:\n",
    "1. Implementar el método **sigmoid**, que recive un vector y le aplica la función sigmoidea.\n",
    "2. Implementar la función de coste softmax y gradiente en el método **naiveSoftmaxLossAndGradient**\n",
    "3. Implement the negative sampling loss and gradient in the negSamplingLossAndGradient\n",
    "method.\n",
    "4. Implementar el modelo skip-gram en el método **skipgram**.\n",
    "Al finalizar, verificar la implementación entera ejecutando <br>\n",
    "` python word2vec.py `"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dJUc_k6YlMmd"
   },
   "source": [
    "#code 2a\n",
    "!python word2vec.py sigmoid\n",
    "!python word2vec.py naiveSoftmaxLossAndGradient\n",
    "!python word2vec.py negSamplingLossAndGradient\n",
    "!python word2vec.py skipgram"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity check for sigmoid ===\n",
      "Tests for sigmoid passed!\n",
      "==== Gradient check for naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "==== Gradient check for negSamplingLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "======Skip-Gram with naiveSoftmaxLossAndGradient Test Cases======\n",
      "The first test passed!\n",
      "The second test passed!\n",
      "The third test passed!\n",
      "All 3 tests passed!\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "======Skip-Gram with negSamplingLossAndGradient======\n",
      "The first test passed!\n",
      "The second test passed!\n",
      "The third test passed!\n",
      "All 3 tests passed!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ppsYRXZtlMme"
   },
   "source": [
    "## PARTE 2b  (4 points)\n",
    "Complete la implementación para el optimizador SGD en el método **sgd.py**. Verificar tu implementación ejecutando <br>\n",
    "` python sgd.py `"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DNdq2RHVlMme"
   },
   "source": [
    "#code 2b\n",
    "!python sgd.py"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.414836786079764e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.524451035823933e-09\n",
      "----------------------------------------\n",
      "ALL TESTS PASSED\n",
      "----------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "T3uSvFvilMme",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PARTE 2c (2 points)\n",
    "Hora de Exposición! Es la hora de cargar datos reales y entrenar los word vectors con todo ya implementado! Utilizaremos el DataSet **Stanford Sentiment Treebank (SST)** para entrenar los word vectors, y luego aplicarlos en una simple tarea de análisis del sentimiento.<br>\n",
    "\n",
    "Para obtener el dataset, ejecutar:<br>\n",
    "` sh get datasets.sh `<br>\n",
    " No hay código adicional para escribir en esta parte, solo ejecutar: <br>\n",
    "` python run.py `\n",
    "\n",
    "Obs.: El proceso de entrenamiento podría llevar mucho tiempo dependiendo de la eficiencia de tu implementación y el poder de cálculo de tu computadora (a una implementación eficiente le lleva entre 1 y 2 horas). Planificar acorde!\n",
    "\n",
    "Luego de 40,000 iteraciones, el script terminará y aparecerá un gráfico para tus word vectors.\n",
    "Además, se guardará como **word_vectors.png** en tu directorio. Incluir el gráfico en tu documento escrito, explicando brevemente al menos 3 oraciones que veas en el gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "!get_datasets.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Estudio\\Stanford\\NLP with Deep Learning, Winter 2019\\Tps\\a2\\run.py\", line 6, in <module>\n",
      "    import matplotlib\n",
      "ModuleNotFoundError: No module named 'matplotlib'\n"
     ]
    }
   ],
   "source": [
    "#code 2c\n",
    "!python run.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}